
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <title>Daily Digest</title>
        <link rel="stylesheet" href="static/style.css">
    </head>
    <body>
        <!-- Header -->
        <header class="header">
            <div class="container">
                <nav class="nav">
                    <div class="logo">üì∞ Daily Digest</div>
                    <ul class="nav-links">
                        <li><a href="#astronomy">Astronomy</a></li>
                        <li><a href="#earth">Earth</a></li>
                        <li><a href="#tarot">Tarot</a></li>
                        <li><a href="#tech">Tech News</a></li>
                        <li><a href="#papers">Papers</a></li>
                    </ul>
                    <div class="date-badge">Thu, Oct 16, 2025</div>
                </nav>
            </div>
        </header>

        <!-- Main Content -->
        <main class="main-content">
            <div class="container">
                <!-- Hero Section -->
                <section class="hero">
                    <h1>Your Daily Tech & Science Digest</h1>
                    <p>Stay updated with the latest in astronomy, earth sciences, technology news, and cutting-edge research papers</p>
                </section>

                <!-- Content Grid -->
                <div class="content-grid">
                    <!-- APOD, Earth Observatory, and Tarot Row -->
                    <div class="three-column">
                        <!-- APOD Section -->
                        <section id="astronomy" class="card apod-card">
                            <div class="card-media" id="apod-media"></div>
                            <div class="card-content-wrapper">
                                <div class="card-header">
                                    <div class="card-icon">‚ú®</div>
                                    <div>
                                        <h2 class="card-title">Astronomy Picture</h2>
                                        <p class="card-subtitle">NASA's daily cosmic wonder</p>
                                    </div>
                                </div>
                                <div class="card-content" id="apod-content">
                                    
        <div style='display: flex; flex-direction: column; gap: 20px;'>
            <div style='display: none;'>
                <img src="https://apod.nasa.gov/apod/image/2510/WitchBroom_Meyers_1080.jpg" alt="APOD" class="apod-media-element">
            </div>
            <div>
                <h3>No Title Found</h3>
                <p><b> Explanation: </b> 
Ten thousand years ago, before the dawn of recorded human history, 
a new light would suddenly have appeared in the 
night sky and faded after a few weeks.  

Today we know this light was from a
<a href="http://chandra.harvard.edu/xray_sources/supernovas.html">supernova,
or exploding star</a>,
and record the expanding debris cloud as the 
<a href="https://apod.nasa.gov/apod/ap250602.html">Veil Nebula</a>, a 
<a href="http://en.wikipedia.org/wiki/List_of_supernova_remnants">supernova remnant</a>.  

This sharp telescopic view is centered on a
<a href="http://en.wikipedia.org/wiki/File:Cygnus_Loop_Labeled.png">western
segment</a> of the 
Veil Nebula cataloged as 
<a href="http://en.wikipedia.org/wiki/New_General_Catalog">NGC</a> 6960 but less formally known as the Witch's Broom Nebula.  

Blasted out in the cataclysmic explosion, an 
<a href="https://science.nasa.gov/solar-system/10-things-going-interstellar/" {="">interstellar</a> shock wave plows
through space sweeping up and exciting interstellar material.

Imaged with narrow band filters, the 
<a href="https://apod.nasa.gov/apod/ap170919.html">glowing filaments</a> are
like long ripples in a sheet seen almost edge on,
remarkably well separated into atomic hydrogen (red)
and oxygen (blue-green) gas.

The complete supernova remnant lies about 1400
<a href="https://spaceplace.nasa.gov/light-year/">light-years</a> 
away towards the
<a href="https://apod.nasa.gov/apod/ap101119.html">constellation Cygnus</a>.

This Witch's <a href="https://i.ytimg.com/vi/D3tZOUeTeRU/maxresdefault.jpg">Broom</a> actually spans about 35 light-years.

The bright star in the frame is 
<a href="http://stars.astro.illinois.edu/sow/52cyg.html">52 Cygni</a>,
visible with the unaided eye from 
a dark location but unrelated to the ancient supernova remnant.

<br><b> Tomorrow's picture: </b>the shadowy realm</p>
            </div>
        </div>
        
                                </div>
                            </div>
                        </section>

                        <!-- Earth Observatory -->
                        <section id="earth" class="card eo-card">
                            <div class="card-media" id="eo-media"></div>
                            <div class="card-content-wrapper">
                                <div class="card-header">
                                    <div class="card-icon">üåç</div>
                                    <div>
                                        <h2 class="card-title">Earth Observatory</h2>
                                        <p class="card-subtitle">Our planet from above</p>
                                    </div>
                                </div>
                                <div class="card-content" id="eo-content">
                                    
        <div style='display: flex; flex-direction: column; gap: 20px;'>
            <div style='display: none;'>
                <img src='https://eoimages.gsfc.nasa.gov/images/imagerecords/154000/154848/tarimbasinzm_oli2_20250911_th.jpg' alt='Earth Observatory' class='eo-media-element'>
            </div>
            <div>
                <h3>A Desert Intersection</h3>
                <p>A colorful ridge and winding glacial meltwater river meet amidst dune fields in western China. <a href='https://earthobservatory.nasa.gov/images/154848/a-desert-intersection' target='_blank'>[Read more]</a></p>
                <div style='background: #f0f8f0; padding: 15px; border-radius: 8px; border-left: 4px solid #2d5016; margin-top: 20px;'>
                    <p>
                        <strong>ü§ñ AI Summary:</strong> The Tarim Basin in western China features Mazartagh Ridge (Hongbaishan), a 145-km-long, 200-m-high geological formation acting as a sand barrier, trapping 62% of windblown sand and altering dune dynamics. The ridge's dark red northern rock consists of iron-rich sandstones/mudstones from arid environments, while its southern side reveals gypsum and calcareous sandstone layers formed in shallow aquatic settings. The seasonal Hotan River, sustained by glacial meltwater from the Kunlun Mountains, flows northward through the Takla Makan Desert, supporting vegetation (poplars, salt cedars) in saline soils. Its gravel bars contain white/green nephrite jade, historically harvested by hand, contributing to Silk Road trade. An eighth-century Tibetan Empire-era fort at Mazar Tagh, excavated in 1907, yielded military documents elucidating the region's ancient history.
                    </p>
                </div>
            </div>
        </div>
        
                                </div>
                            </div>
                        </section>

                        <!-- Tarot Section -->
                        <section id="tarot" class="card tarot-card">
                            <div class="card-media" id="tarot-media"></div>
                            <div class="card-content-wrapper">
                                <div class="card-header">
                                    <div class="card-icon">üîÆ</div>
                                    <div>
                                        <h2 class="card-title">Daily Tarot</h2>
                                        <p class="card-subtitle">Your mystical guidance</p>
                                    </div>
                                </div>
                                <div class="card-content" id="tarot-content">
                                    
        <div style='display: flex; flex-direction: column; gap: 20px;'>
            <div style='display: none;'>
                <div class='tarot-card-container tarot-media-element'>
                    <div class='tarot-card' onclick='this.style.transform = this.style.transform.includes("rotateY(180deg)") ? "rotateY(0deg)" : "rotateY(180deg)"'>
                        <div>
                            <div style='font-size: 2.5rem; color: #d4af37; text-align: center; line-height: 1.2;'>
                                üîÆ<br>
                                <span style='font-size: 0.8rem; letter-spacing: 2px; font-weight: normal;'>DAILY TAROT</span><br>
                                <span style='font-size: 0.6rem; opacity: 0.8;'>Click to Reveal</span>
                            </div>
                        </div>
                        <div>
                            <img src='https://raw.githubusercontent.com/Haus226/daily-email/refs/heads/main/tarot_cards/Pictorial_Key_to_the_Tarot_Cups_04.jpg' alt='Four of Cups' />
                        </div>
                    </div>
                </div>
            </div>
            <div>
                <h3>Four of Cups</h3>                
                <div style='background: linear-gradient(135deg, #ffeaa7, #fdcb6e); padding: 5px; border-radius: 12px; border-left: 4px solid #e17055; margin-bottom: 20px; box-shadow: 0 4px 15px rgba(0,0,0,0.1);'>
                    <div style='font-weight: bold; color: #2d3436; margin-bottom: 5px; font-size: 1.2rem; text-transform: uppercase; letter-spacing: 1px;'>‚ú® Core Meaning</div>
                    <p style='margin: 0; color: #2d3436; line-height: 1.5; font-size: 1.0rem;'>Weariness, disgust, aversion, imaginary vexations, as if the wine of this world had caused satiety only; another wine, as if a fairy gift, is now offered the wastrel, but he sees no consolation therein. This is also a card of blended pleasure.</p>
                </div>
                <div style='background: linear-gradient(135deg, #ddd6fe, #c4b5fd); padding: 5px; border-radius: 12px; border-left: 4px solid #8b5cf6; box-shadow: 0 4px 15px rgba(0,0,0,0.1);'>
                    <div style='font-weight: bold; color: #2d3436; margin-bottom: 5px; font-size: 1.4rem; text-transform: uppercase; letter-spacing: 1px;'>üîç Daily Guidance</div>
                    <p style='margin: 0; color: #2d3436; line-height: 1.6; font-size: 1.3rem; font-weight: 500;'>"Remember, even when you feel discontent, there's always another cup of joy waiting to be discovered. Keep your eyes open and your heart ready."</p>
                </div>
    
            </div>
        </div>
        
                                </div>
                            </div>
                        </section>
                        
                        <!-- Navigation Arrows -->
                        <button class="card-nav prev" onclick="prevCard()" aria-label="Previous card">‚Äπ</button>
                        <button class="card-nav next" onclick="nextCard()" aria-label="Next card">‚Ä∫</button>
                    </div>

                    <!-- Hacker News Section -->
                    <section id="tech" class="card hn-card featured-section">
                        <div class="card-header">
                            <div class="card-icon">üî•</div>
                            <div>
                                <h2 class="card-title">Hacker News Top 10</h2>
                                <p class="card-subtitle">What's trending in tech</p>
                            </div>
                        </div>
                        <div class="card-content">
                            <ol class='hn-list'><li><a href='https://www.apple.com/newsroom/2025/10/apple-unleashes-m5-the-next-big-leap-in-ai-performance-for-apple-silicon/' target='_blank'>Apple M5 chip</a></li><li><a href='https://www.anthropic.com/news/claude-haiku-4-5' target='_blank'>Claude Haiku 4.5</a></li><li><a href='https://www.youtube.com/watch?v=bwjMLyBU4RU&list=PLyR4neQXqQo5nPdEiMbaEJxWiy_UuyNN4&index=1' target='_blank'>Build a Superscalar 8-Bit CPU (YouTube Playlist) [video]</a></li><li><a href='https://exploring-better-ways.bellroy.com/free-applicatives-the-handle-pattern-and-remote-systems.html' target='_blank'>Free applicatives, the handle pattern, and remote systems</a></li><li><a href='https://zed.dev/blog/zed-for-windows-is-here' target='_blank'>Zed is now available on Windows</a></li><li><a href='https://github.com/IRS-Public/fact-graph' target='_blank'>IRS open sources its fact graph</a></li><li><a href='https://www.backblaze.com/blog/are-hard-drives-getting-better-lets-revisit-the-bathtub-curve/' target='_blank'>Are hard drives getting better?</a></li><li><a href='https://www.gilesthomas.com/2025/10/llm-from-scratch-22-finally-training-our-llm' target='_blank'>Writing an LLM from scratch, part 22 ‚Äì training our LLM</a></li><li><a href='https://www.scottrlarson.com/publications/publication-windows-move-towards-surveillance/' target='_blank'>Retiring Windows 10 and Microsoft's move towards a surveillance state</a></li><li><a href='https://arxiv.org/abs/2507.16126' target='_blank'>TaxCalcBench: Evaluating Frontier Models on the Tax Calculation Task</a></li></ol>
                        </div>
                    </section>

                    <!-- Hugging Face Papers -->
                    <section id="papers" class="card hf-card-container featured-section">
                        <div class="card-header">
                            <div class="card-icon">üìö</div>
                            <div>
                                <h2 class="card-title">Latest Research Papers</h2>
                                <p class="card-subtitle">Cutting-edge AI & ML research</p>
                            </div>
                        </div>
                        
                        <!-- Paper Filters -->
                        <div class="paper-filters" id="hf-filters">
                            <button class="filter-btn active" data-tag="ALL" onclick="filterPapers('ALL')">All Papers</button>
                            <button class="filter-btn" data-tag="DAILY" onclick="filterPapers('DAILY')">Daily</button>
                            <button class="filter-btn" data-tag="WEEKLY" onclick="filterPapers('WEEKLY')">Weekly</button>
                            <button class="filter-btn" data-tag="MONTHLY" onclick="filterPapers('MONTHLY')">Monthly</button>
                            <button class="filter-btn" data-tag="TRENDING" onclick="filterPapers('TRENDING')">Trending</button>
                        </div>

                        <!-- Papers Content -->
                        <div class="card-content">
                            
        <div id="hf-grid" class="papers-grid">
        
                <div class="paper-card hf-card flip-card" data-tags="DAILY" data-date="2025-10-15" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2510.13554" target="_blank" onclick="event.stopPropagation()">Attention Illuminates LLM Reasoning: The Preplan-and-Anchor Rhythm
  Enables Fine-Grained Policy Optimization</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-10-15</div>
                                        <div class="tags"><span class="tag">DAILY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2510.13554" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <div class="link-item" style="background: #f5f5f5; color: #999;">No GitHub</div>
                                    <div class="link-item upvote-item">üî• 28</div>
                                    <div class="link-item stars-item">‚≠ê 0</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">Attention mechanisms in LLMs are analyzed to reveal reasoning patterns, leading to novel RL strategies that improve performance by focusing on critical tokens.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                The reasoning pattern of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Large%20language%20models">Large language models</a> (LLMs) remains opaque, and
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Reinforcement%20learning">Reinforcement learning</a> (RL) typically applies uniform credit across an entire
generation, blurring the distinction between pivotal and routine steps. This
work positions attention as a privileged substrate that renders the internal
logic of LLMs legible, not merely as a byproduct of computation, but as a
mechanistic blueprint of reasoning itself. We first distinguish <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=attention%20heads">attention heads</a>
between locally and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=globally%20focused">globally focused</a> information processing and reveal that
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=locally%20focused">locally focused</a> heads produce a sawtooth pattern near the diagonal indicating
phrasal chunks, while <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=globally%20focused">globally focused</a> heads expose tokens that exert broad
downstream influence over future tokens. We formalize these with two metrics:
1) <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Windowed%20Average%20Attention%20Distance">Windowed Average Attention Distance</a>, which measures the extent of backward
attention within a clipped window; 2) <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Future%20Attention%20Influence">Future Attention Influence</a>, which
quantifies a token's global importance as the average attention it receives
from subsequent tokens. Taken together, these signals reveal a recurring
preplan-and-anchor mechanism, where the model first performs a long-range
contextual reference to generate an introductory token, which is immediately
followed by or coincides with a semantic anchor token that organizes subsequent
reasoning. Leveraging these insights, we introduce three novel RL strategies
that dynamically perform <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=targeted%20credit%20assignment">targeted credit assignment</a> to critical nodes (preplan
tokens, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=anchor%20tokens">anchor tokens</a>, and their temporal coupling) and show consistent
performance gains across various reasoning tasks. By aligning optimization with
the model's intrinsic reasoning rhythm, we aim to transform opaque optimization
into an actionable structure-aware process, hoping to offer a potential step
toward more transparent and effective optimization of LLM reasoning.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="DAILY TRENDING" data-date="2025-10-15" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2510.13678" target="_blank" onclick="event.stopPropagation()">FlashWorld: High-quality 3D Scene Generation within Seconds</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-10-15</div>
                                        <div class="tags"><span class="tag">DAILY</span><span class="tag">TRENDING</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2510.13678" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/imlixinyang/FlashWorld" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 21</div>
                                    <div class="link-item stars-item">‚≠ê 24</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">FlashWorld generates 3D scenes from single images or text prompts quickly and with high quality by combining MV-oriented and 3D-oriented generation methods.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                We propose FlashWorld, a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=generative%20model">generative model</a> that produces <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=3D%20scenes">3D scenes</a> from a
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=single%20image">single image</a> or <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=text%20prompt">text prompt</a> in seconds, 10~100times faster than previous
works while possessing superior rendering quality. Our approach shifts from the
conventional multi-view-oriented (MV-oriented) paradigm, which generates
multi-view images for subsequent 3D reconstruction, to a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=3D-oriented%20approach">3D-oriented approach</a>
where the model directly produces <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=3D%20Gaussian%20representations">3D Gaussian representations</a> during multi-view
generation. While ensuring 3D consistency, 3D-oriented method typically suffers
poor visual quality. FlashWorld includes a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=dual-mode%20pre-training">dual-mode pre-training</a> phase
followed by a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=cross-mode%20post-training">cross-mode post-training</a> phase, effectively integrating the
strengths of both paradigms. Specifically, leveraging the prior from a video
diffusion model, we first pre-train a dual-mode <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=multi-view%20diffusion%20model">multi-view diffusion model</a>,
which jointly supports MV-oriented and 3D-oriented generation modes. To bridge
the quality gap in 3D-oriented generation, we further propose a cross-mode
post-training distillation by matching distribution from consistent 3D-oriented
mode to high-quality MV-oriented mode. This not only enhances visual quality
while maintaining 3D consistency, but also reduces the required <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=denoising%20steps">denoising steps</a>
for inference. Also, we propose a strategy to leverage massive single-view
images and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=text%20prompt">text prompt</a>s during this process to enhance the model's
generalization to <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=out-of-distribution%20inputs">out-of-distribution inputs</a>. Extensive experiments demonstrate
the superiority and efficiency of our method.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="MONTHLY" data-date="2025-09-30" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2509.26507" target="_blank" onclick="event.stopPropagation()">The Dragon Hatchling: The Missing Link between the Transformer and
  Models of the Brain</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-09-30</div>
                                        <div class="tags"><span class="tag">MONTHLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2509.26507" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/pathwaycom/bdh" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 470</div>
                                    <div class="link-item stars-item">‚≠ê 3.06k</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">BDH, a biologically inspired Large Language Model, combines scale-free network architecture with Hebbian learning to achieve Transformer-like performance while maintaining interpretability.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                The relationship between computing systems and the brain has served as
motivation for pioneering theoreticians since John von Neumann and Alan Turing.
Uniform, scale-free biological networks, such as the brain, have powerful
properties, including generalizing over time, which is the main barrier for
Machine Learning on the path to Universal Reasoning Models.
  We introduce `Dragon Hatchling' (BDH), a new <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Large%20Language%20Model">Large Language Model</a>
architecture based on a scale-free biologically inspired network of \n
locally-interacting neuron particles. BDH couples strong theoretical
foundations and inherent <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=interpretability">interpretability</a> without sacrificing Transformer-like
performance.
  BDH is a practical, performant state-of-the-art attention-based state space
sequence learning architecture. In addition to being a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=graph%20model">graph model</a>, BDH admits
a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=GPU-friendly">GPU-friendly</a> formulation. It exhibits <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Transformer-like%20scaling%20laws">Transformer-like scaling laws</a>:
empirically BDH rivals GPT2 performance on language and translation tasks, at
the same number of parameters (10M to 1B), for the same training data.
  BDH can be represented as a brain model. The working memory of BDH during
inference entirely relies on <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=synaptic%20plasticity">synaptic plasticity</a> with <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Hebbian%20learning">Hebbian learning</a> using
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=spiking%20neurons">spiking neurons</a>. We confirm empirically that specific, individual synapses
strengthen connection whenever BDH hears or reasons about a specific concept
while processing language inputs. The neuron interaction network of BDH is a
graph of high modularity with heavy-tailed degree distribution. The BDH model
is biologically plausible, explaining one possible mechanism which human
neurons could use to achieve speech.
  BDH is designed for <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=interpretability">interpretability</a>. Activation vectors of BDH are sparse
and positive. We demonstrate <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=monosemanticity">monosemanticity</a> in BDH on language tasks.
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Interpretability">Interpretability</a> of state, which goes beyond <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=interpretability">interpretability</a> of neurons and
model parameters, is an inherent feature of the BDH architecture.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="TRENDING WEEKLY MONTHLY" data-date="2025-10-13" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2510.11690" target="_blank" onclick="event.stopPropagation()">Diffusion Transformers with Representation Autoencoders</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-10-13</div>
                                        <div class="tags"><span class="tag">MONTHLY</span><span class="tag">TRENDING</span><span class="tag">WEEKLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2510.11690" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/bytetriper/RAE" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 122</div>
                                    <div class="link-item stars-item">‚≠ê 960</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">Replacing VAEs with pretrained representation encoders in Diffusion Transformers enhances generative quality and convergence speed without auxiliary losses.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Latent%20generative%20modeling">Latent generative modeling</a>, where a pretrained autoencoder maps pixels into a
latent space for the diffusion process, has become the standard strategy for
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Diffusion%20Transformers%20(DiT)">Diffusion Transformers (DiT)</a>; however, the autoencoder component has barely
evolved. Most DiTs continue to rely on the original <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=VAE%20encoder">VAE encoder</a>, which
introduces several limitations: outdated backbones that compromise
architectural simplicity, low-dimensional latent spaces that restrict
information capacity, and weak representations that result from purely
reconstruction-based training and ultimately limit generative quality. In this
work, we explore replacing the VAE with <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=pretrained%20representation%20encoders">pretrained representation encoders</a>
(e.g., <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=DINO">DINO</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=SigLIP">SigLIP</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=MAE">MAE</a>) paired with trained decoders, forming what we term
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Representation%20Autoencoders%20(RAEs)">Representation Autoencoders (RAEs)</a>. These models provide both high-quality
reconstructions and semantically rich latent spaces, while allowing for a
scalable <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=transformer-based%20architecture">transformer-based architecture</a>. Since these latent spaces are
typically high-dimensional, a key challenge is enabling <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=diffusion%20transformers">diffusion transformers</a>
to operate effectively within them. We analyze the sources of this difficulty,
propose theoretically motivated solutions, and validate them empirically. Our
approach achieves faster convergence without auxiliary representation alignment
losses. Using a DiT variant equipped with a lightweight, wide DDT head, we
achieve strong <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=image%20generation">image generation</a> results on <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=ImageNet">ImageNet</a>: 1.51 <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=FID">FID</a> at 256x256 (no
guidance) and 1.13 at both 256x256 and 512x512 (with guidance). RAE offers
clear advantages and should be the new default for diffusion transformer
training.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="WEEKLY TRENDING MONTHLY" data-date="2025-10-13" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2510.11696" target="_blank" onclick="event.stopPropagation()">QeRL: Beyond Efficiency -- Quantization-enhanced Reinforcement Learning
  for LLMs</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-10-13</div>
                                        <div class="tags"><span class="tag">MONTHLY</span><span class="tag">TRENDING</span><span class="tag">WEEKLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2510.11696" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/NVlabs/QeRL" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 138</div>
                                    <div class="link-item stars-item">‚≠ê 261</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">QeRL, a quantization-enhanced reinforcement learning framework, accelerates RL training for large language models by combining NVFP4 quantization with Low-Rank Adaptation and an Adaptive Quantization Noise mechanism, achieving significant speedups and improved performance.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                We propose QeRL, a Quantization-enhanced <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Reinforcement%20Learning">Reinforcement Learning</a> framework for
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=large%20language%20models%20(LLMs)">large language models (LLMs)</a>. While RL is essential for LLMs' reasoning
capabilities, it is resource-intensive, requiring substantial GPU memory and
long rollout durations. QeRL addresses these issues by combining NVFP4
quantization with <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Low-Rank%20Adaptation%20(LoRA)">Low-Rank Adaptation (LoRA)</a>, accelerating <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=rollout%20phase">rollout phase</a> of RL
while reducing memory overhead. Beyond efficiency, our findings show that
quantization noise increases <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=policy%20entropy">policy entropy</a>, enhancing <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=exploration">exploration</a>, and
enabling the discovery of better strategies during RL. To further optimize
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=exploration">exploration</a>, QeRL introduces an <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Adaptive%20Quantization%20Noise%20(AQN)">Adaptive Quantization Noise (AQN)</a> mechanism,
which dynamically adjusts noise during training. Experiments demonstrate that
QeRL delivers over 1.5 times speedup in the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=rollout%20phase">rollout phase</a>. Moreover, this is
the first framework to enable RL training of a 32B LLM on a single H100 80GB
GPU, while delivering overall speedups for RL training. It also achieves faster
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=reward%20growth">reward growth</a> and higher final accuracy than 16-bit LoRA and QLoRA, while
matching the performance of full-parameter fine-tuning on mathematical
benchmarks such as <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=GSM8K">GSM8K</a> (90.8%) and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=MATH%20500">MATH 500</a> (77.4%) in the 7B model. These
results establish QeRL as an efficient and effective framework for RL training
in LLMs.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="MONTHLY TRENDING" data-date="2025-10-06" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2510.04871" target="_blank" onclick="event.stopPropagation()">Less is More: Recursive Reasoning with Tiny Networks</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-10-06</div>
                                        <div class="tags"><span class="tag">MONTHLY</span><span class="tag">TRENDING</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2510.04871" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/SamsungSAILMontreal/TinyRecursiveModels" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 383</div>
                                    <div class="link-item stars-item">‚≠ê 4.41k</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">Tiny Recursive Model (TRM) achieves high generalization on complex puzzle tasks using a small, two-layer network with minimal parameters, outperforming larger language models.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Hierarchical%20Reasoning%20Model">Hierarchical Reasoning Model</a> (<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=HRM">HRM</a>) is a novel approach using two small neural
networks recursing at different frequencies. This biologically inspired method
beats Large Language models (LLMs) on hard puzzle tasks such as Sudoku, Maze,
and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=ARC-AGI">ARC-AGI</a> while trained with small models (27M parameters) on small data
(around 1000 examples). <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=HRM">HRM</a> holds great promise for solving hard problems with
small networks, but it is not yet well understood and may be suboptimal. We
propose <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Tiny%20Recursive%20Model">Tiny Recursive Model</a> (<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=TRM">TRM</a>), a much simpler <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=recursive%20reasoning">recursive reasoning</a> approach
that achieves significantly higher generalization than <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=HRM">HRM</a>, while using a
single tiny network with only 2 layers. With only 7M parameters, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=TRM">TRM</a> obtains
45% test-accuracy on <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=ARC-AGI">ARC-AGI</a>-1 and 8% on <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=ARC-AGI">ARC-AGI</a>-2, higher than most LLMs
(e.g., <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Deepseek%20R1">Deepseek R1</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=o3-mini">o3-mini</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Gemini%202.5%20Pro">Gemini 2.5 Pro</a>) with less than 0.01% of the
parameters.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="TRENDING" data-date="2025-09-26" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2509.22186" target="_blank" onclick="event.stopPropagation()">MinerU2.5: A Decoupled Vision-Language Model for Efficient
  High-Resolution Document Parsing</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-09-26</div>
                                        <div class="tags"><span class="tag">TRENDING</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2509.22186" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/opendatalab/MinerU" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 117</div>
                                    <div class="link-item stars-item">‚≠ê 46.6k</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">MinerU2.5, a 1.2B-parameter document parsing vision-language model, achieves state-of-the-art recognition accuracy with computational efficiency through a coarse-to-fine parsing strategy.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                We introduce MinerU2.5, a 1.2B-parameter <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=document%20parsing">document parsing</a> vision-language
model that achieves state-of-the-art recognition accuracy while maintaining
exceptional computational efficiency. Our approach employs a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=coarse-to-fine">coarse-to-fine</a>,
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=two-stage%20parsing">two-stage parsing</a> strategy that decouples global <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=layout%20analysis">layout analysis</a> from local
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=content%20recognition">content recognition</a>. In the first stage, the model performs efficient layout
analysis on <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=downsampled%20images">downsampled images</a> to identify structural elements, circumventing
the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=computational%20overhead">computational overhead</a> of processing high-resolution inputs. In the second
stage, guided by the global layout, it performs targeted <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=content%20recognition">content recognition</a> on
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=native-resolution%20crops">native-resolution crops</a> extracted from the original image, preserving
fine-grained details in dense text, complex formulas, and tables. To support
this strategy, we developed a comprehensive <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=data%20engine">data engine</a> that generates diverse,
large-scale training corpora for both <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=pretraining">pretraining</a> and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=fine-tuning">fine-tuning</a>. Ultimately,
MinerU2.5 demonstrates strong <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=document%20parsing">document parsing</a> ability, achieving
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=state-of-the-art%20performance">state-of-the-art performance</a> on multiple benchmarks, surpassing both
general-purpose and domain-specific models across various recognition tasks,
while maintaining significantly lower <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=computational%20overhead">computational overhead</a>.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="WEEKLY MONTHLY" data-date="2025-10-14" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2510.12276" target="_blank" onclick="event.stopPropagation()">Spatial Forcing: Implicit Spatial Representation Alignment for
  Vision-language-action Model</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-10-14</div>
                                        <div class="tags"><span class="tag">MONTHLY</span><span class="tag">WEEKLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2510.12276" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/OpenHelix-Team/Spatial-Forcing" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 134</div>
                                    <div class="link-item stars-item">‚≠ê 37</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">(No summary available)</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Vision-language-action (VLA) models have recently shown strong potential in
enabling robots to follow language instructions and execute precise actions.
However, most VLAs are built upon vision-language models pretrained solely on
2D data, which lack accurate spatial awareness and hinder their ability to
operate in the 3D physical world. Existing solutions attempt to incorporate
explicit 3D sensor inputs such as depth maps or point clouds, but these
approaches face challenges due to sensor noise, hardware heterogeneity, and
incomplete depth coverage in existing datasets. Alternative methods that
estimate 3D cues from 2D images also suffer from the limited performance of
depth estimators.We propose Spatial Forcing (SF), a simple yet effective
alignment strategy that implicitly forces VLA models to develop spatial
comprehension capabilities without relying on explicit 3D inputs or depth
estimators. SF aligns intermediate visual embeddings of VLAs with geometric
representations produced by pretrained 3D foundation models. By enforcing
alignment at intermediate layers, SF guides VLAs to encode richer spatial
representations that enhance action precision.Extensive experiments in
simulation and real-world environments demonstrate that SF achieves
state-of-the-art results, surpassing both 2D- and 3D-based VLAs. SF further
accelerates training by up to 3.8x and improves data efficiency across diverse
robotic tasks. Project page is at https://spatial-forcing.github.io/
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="DAILY" data-date="2025-10-15" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2510.13747" target="_blank" onclick="event.stopPropagation()">InteractiveOmni: A Unified Omni-modal Model for Audio-Visual Multi-turn
  Dialogue</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-10-15</div>
                                        <div class="tags"><span class="tag">DAILY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2510.13747" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <div class="link-item" style="background: #f5f5f5; color: #999;">No GitHub</div>
                                    <div class="link-item upvote-item">üî• 18</div>
                                    <div class="link-item stars-item">‚≠ê 0</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">InteractiveOmni is a unified omni-modal large language model for audio-visual multi-turn interactions, offering comprehensive understanding and speech generation capabilities with efficient parameter usage.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                We introduce InteractiveOmni, a unified and open-source omni-modal large
language model for <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=audio-visual%20multi-turn%20interaction">audio-visual multi-turn interaction</a>, ranging from 4B to 8B
parameters, designed to lead the field of lightweight models by offering
comprehensive <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=omni-modal%20understanding">omni-modal understanding</a> and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=speech%20generation">speech generation</a> capabilities. To
achieve this, we integrate the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=vision%20encoder">vision encoder</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=audio%20encoder">audio encoder</a>, large language
model, and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=speech%20decoder">speech decoder</a> into a unified model for understanding and generation
tasks. We design a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=multi-stage%20training%20strategy">multi-stage training strategy</a> to ensure robust cross-modal
capabilities, including pre-training for <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=omni-modal%20understanding">omni-modal understanding</a>, followed by
post-training with <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=speech%20conversation">speech conversation</a> and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=audio-visual%20interaction">audio-visual interaction</a>. To enable
human-like long-term conversational ability, we meticulously curate a
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=multi-turn%20training%20dataset">multi-turn training dataset</a> that enhances the model's ability to handle complex
and multi-turn interactions. To effectively evaluate the multi-turn memory and
speech interaction capabilities, we construct the multi-modal multi-turn memory
benchmark and the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=multi-turn%20speech%20interaction%20benchmark">multi-turn speech interaction benchmark</a>. Experiments
demonstrate that InteractiveOmni significantly outperforms leading open-source
models and provides a more intelligent multi-turn audio-visual experience,
particularly in its <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=long-term%20memory%20capabilities">long-term memory capabilities</a>. Notably, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=InteractiveOmni-4B">InteractiveOmni-4B</a>
is comparable to the much larger model like <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Qwen2.5-Omni-7B">Qwen2.5-Omni-7B</a> on general
benchmarks, and it can retain 97% of the performance of the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=InteractiveOmni-8B">InteractiveOmni-8B</a>
while utilizing only 50% of the model size. Achieving state-of-the-art results
against similarly sized models across image, audio, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=video%20understanding">video understanding</a>, and
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=speech%20generation">speech generation</a> tasks, InteractiveOmni is an accessible, open-source
foundation for next-generation intelligent interactive systems.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="WEEKLY MONTHLY" data-date="2025-10-07" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2510.05684" target="_blank" onclick="event.stopPropagation()">D2E: Scaling Vision-Action Pretraining on Desktop Data for Transfer to
  Embodied AI</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-10-07</div>
                                        <div class="tags"><span class="tag">MONTHLY</span><span class="tag">WEEKLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2510.05684" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/worv-ai/D2E" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 123</div>
                                    <div class="link-item stars-item">‚≠ê 33</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">D2E framework uses desktop interactions to pretrain embodied AI, achieving high success rates in physical manipulation and navigation tasks.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Large language models leverage internet-scale text data, yet <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=embodied%20AI">embodied AI</a>
remains constrained by the prohibitive costs of physical trajectory collection.
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Desktop%20environments">Desktop environments</a> -- particularly gaming -- offer a compelling alternative:
they provide rich <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=sensorimotor%20interactions">sensorimotor interactions</a> at scale while maintaining the
structured observation-action coupling essential for embodied learning. We
present D2E (Desktop to <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Embodied%20AI">Embodied AI</a>), a framework that demonstrates desktop
interactions can serve as an effective pretraining substrate for robotics
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=embodied%20AI">embodied AI</a> tasks. Unlike prior work that remained domain-specific (e.g., VPT
for Minecraft) or kept data proprietary (e.g., SIMA), D2E establishes a
complete pipeline from scalable desktop data collection to verified transfer in
embodied domains. Our framework comprises three components: (1) the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=OWA%20Toolkit">OWA Toolkit</a>
that unifies diverse desktop interactions into a standardized format with 152x
compression, (2) the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Generalist-IDM">Generalist-IDM</a> that achieves strong zero-shot
generalization across unseen games through <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=timestamp-based%20event%20prediction">timestamp-based event prediction</a>,
enabling <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=internet-scale%20pseudo-labeling">internet-scale pseudo-labeling</a>, and (3) <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=VAPT">VAPT</a> that transfers
desktop-pretrained representations to physical manipulation and navigation.
Using 1.3K+ hours of data (259 hours of human demonstrations, and 1K+ hours of
pseudo-labeled gameplay), we achieve a total of 96.6% success rate on LIBERO
manipulation and 83.3% on <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=CANVAS%20navigation">CANVAS navigation</a> benchmarks. This validates that
sensorimotor primitives in digital interactions exhibit sufficient invariance
to transfer meaningfully to physical embodied tasks, establishing desktop
pretraining as a practical paradigm for robotics. We will make all our work
public, including the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=OWA%20toolkit">OWA toolkit</a>, datasets of human-collected and
pseudo-labeled, and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=VAPT">VAPT</a>-trained models available at
https://worv-ai.github.io/d2e/
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="DAILY" data-date="2025-10-09" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2510.07944" target="_blank" onclick="event.stopPropagation()">CVD-STORM: Cross-View Video Diffusion with Spatial-Temporal
  Reconstruction Model for Autonomous Driving</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-10-09</div>
                                        <div class="tags"><span class="tag">DAILY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2510.07944" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <div class="link-item" style="background: #f5f5f5; color: #999;">No GitHub</div>
                                    <div class="link-item upvote-item">üî• 18</div>
                                    <div class="link-item stars-item">‚≠ê 0</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">CVD-STORM, a cross-view video diffusion model with a spatial-temporal reconstruction VAE, enhances video generation quality and provides depth estimation for dynamic scenes.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Generative models have been widely applied to world modeling for environment
simulation and future state prediction. With advancements in autonomous
driving, there is a growing demand not only for high-<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=fid">fid</a>elity video generation
under various controls, but also for producing diverse and meaningful
information such as depth estimation. To address this, we propose CVD-STORM, a
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=cross-view%20video%20diffusion%20model">cross-view video diffusion model</a> utilizing a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=spatial-temporal%20reconstruction">spatial-temporal reconstruction</a>
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Variational%20Autoencoder%20(VAE)">Variational Autoencoder (VAE)</a> that generates long-term, multi-view videos with
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=4D%20reconstruction">4D reconstruction</a> capabilities under various control inputs. Our approach first
fine-tunes the VAE with an auxiliary <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=4D%20reconstruction">4D reconstruction</a> task, enhancing its
ability to encode 3D structures and temporal dynamics. Subsequently, we
integrate this VAE into the video diffusion process to significantly improve
generation quality. Experimental results demonstrate that our model achieves
substantial improvements in both <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=FID">FID</a> and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=FVD">FVD</a> metrics. Additionally, the
jointly-trained <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Gaussian%20Splatting%20Decoder">Gaussian Splatting Decoder</a> effectively reconstructs dynamic
scenes, providing valuable geometric information for comprehensive scene
understanding.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="MONTHLY" data-date="2025-10-09" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2510.08558" target="_blank" onclick="event.stopPropagation()">Agent Learning via Early Experience</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-10-09</div>
                                        <div class="tags"><span class="tag">MONTHLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2510.08558" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <div class="link-item" style="background: #f5f5f5; color: #999;">No GitHub</div>
                                    <div class="link-item upvote-item">üî• 209</div>
                                    <div class="link-item stars-item">‚≠ê 0</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">Early experience, using agent-generated interaction data without reward signals, improves policy effectiveness and generalization, serving as a bridge between imitation learning and reinforcement learning.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                A long-term goal of language agents is to learn and improve through their own
experience, ultimately outperforming humans in complex, real-world tasks.
However, training agents from experience data with <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=reinforcement%20learning">reinforcement learning</a>
remains difficult in many environments, which either lack verifiable rewards
(e.g., websites) or require inefficient long-horizon rollouts (e.g., multi-turn
tool use). As a result, most current agents rely on supervised fine-tuning on
expert data, which is challenging to scale and generalizes poorly. This
limitation stems from the nature of expert demonstrations: they capture only a
narrow range of scenarios and expose the agent to limited environment
diversity. We address this limitation with a middle-ground paradigm we call
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=early%20experience">early experience</a>: interaction data generated by the agent's own actions, where
the resulting future states serve as supervision without reward signals. Within
this paradigm we study two strategies of using such data: (1) Implicit world
modeling, which uses collected states to ground the policy in environment
dynamics; and (2) <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Self-reflection">Self-reflection</a>, where the agent learns from its suboptimal
actions to improve reasoning and decision-making. We evaluate across eight
diverse environments and multiple model families. Our approaches consistently
improve effectiveness and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=out-of-domain%20generalization">out-of-domain generalization</a>, highlighting the value
of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=early%20experience">early experience</a>. Moreover, in environments with verifiable rewards, our
results provide promising signals that <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=early%20experience">early experience</a> offers a strong
foundation for subsequent <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=reinforcement%20learning">reinforcement learning</a>, positioning it as a practical
bridge between imitation learning and fully experience-driven agents.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="TRENDING" data-date="2025-10-14" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2510.12798" target="_blank" onclick="event.stopPropagation()">Detect Anything via Next Point Prediction</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-10-14</div>
                                        <div class="tags"><span class="tag">TRENDING</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2510.12798" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/IDEA-Research/Rex-Omni" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 35</div>
                                    <div class="link-item stars-item">‚≠ê 180</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">(No summary available)</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Object detection has long been dominated by traditional coordinate
regression-based models, such as YOLO, DETR, and Grounding DINO. Although
recent efforts have attempted to leverage MLLMs to tackle this task, they face
challenges like low recall rate, duplicate predictions, coordinate
misalignment, etc. In this work, we bridge this gap and propose Rex-Omni, a
3B-scale MLLM that achieves state-of-the-art object perception performance. On
benchmarks like COCO and LVIS, Rex-Omni attains performance comparable to or
exceeding regression-based models (e.g., DINO, Grounding DINO) in a zero-shot
setting. This is enabled by three key designs: 1) Task Formulation: we use
special tokens to represent quantized coordinates from 0 to 999, reducing the
model's learning difficulty and improving token efficiency for coordinate
prediction; 2) Data Engines: we construct multiple data engines to generate
high-quality grounding, referring, and pointing data, providing semantically
rich supervision for training; \3) Training Pipelines: we employ a two-stage
training process, combining supervised fine-tuning on 22 million data with
GRPO-based reinforcement post-training. This RL post-training leverages
geometry-aware rewards to effectively bridge the discrete-to-continuous
coordinate prediction gap, improve box accuracy, and mitigate undesirable
behaviors like duplicate predictions that stem from the teacher-guided nature
of the initial SFT stage. Beyond conventional detection, Rex-Omni's inherent
language understanding enables versatile capabilities such as object referring,
pointing, visual prompting, GUI grounding, spatial referring, OCR and
key-pointing, all systematically evaluated on dedicated benchmarks. We believe
that Rex-Omni paves the way for more versatile and language-aware visual
perception systems.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="WEEKLY MONTHLY" data-date="2025-10-09" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2510.08673" target="_blank" onclick="event.stopPropagation()">Thinking with Camera: A Unified Multimodal Model for Camera-Centric
  Understanding and Generation</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-10-09</div>
                                        <div class="tags"><span class="tag">MONTHLY</span><span class="tag">WEEKLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2510.08673" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/KangLiao929/Puffin" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 108</div>
                                    <div class="link-item stars-item">‚≠ê 133</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">Puffin, a unified multimodal model, integrates language regression and diffusion-based generation to enhance camera-centric spatial understanding and generation by treating camera parameters as language.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Camera-centric">Camera-centric</a> understanding and generation are two cornerstones of spatial
intelligence, yet they are typically studied in isolation. We present Puffin, a
unified <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=camera-centric">camera-centric</a> <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=multimodal%20model">multimodal model</a> that extends <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=spatial%20awareness">spatial awareness</a> along
the camera dimension. Puffin integrates <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=language%20regression">language regression</a> and diffusion-based
generation to interpret and create scenes from arbitrary viewpoints. To bridge
the modality gap between cameras and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=vision-language">vision-language</a>, we introduce a novel
paradigm that treats <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=camera%20as%20language">camera as language</a>, enabling thinking with camera. This
guides the model to align spatially grounded visual cues with photographic
terminology while reasoning across <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=geometric%20context">geometric context</a>. Puffin is trained on
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Puffin-4M">Puffin-4M</a>, a large-scale dataset of 4 million <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=vision-language">vision-language</a>-camera triplets.
We incorporate both <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=global%20camera%20parameters">global camera parameters</a> and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=pixel-wise%20camera%20maps">pixel-wise camera maps</a>,
yielding flexible and reliable <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=spatial%20generation">spatial generation</a>. Experiments demonstrate
Puffin superior performance over specialized models for <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=camera-centric">camera-centric</a>
generation and understanding. With <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=instruction%20tuning">instruction tuning</a>, Puffin generalizes to
diverse cross-view tasks such as <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=spatial%20imagination">spatial imagination</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=world%20exploration">world exploration</a>, and
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=photography%20guidance">photography guidance</a>. We will release the code, models, dataset pipeline, and
benchmark to advance multimodal spatial intelligence research.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="DAILY" data-date="2025-10-15" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2510.13795" target="_blank" onclick="event.stopPropagation()">Bee: A High-Quality Corpus and Full-Stack Suite to Unlock Advanced Fully
  Open MLLMs</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-10-15</div>
                                        <div class="tags"><span class="tag">DAILY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2510.13795" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <div class="link-item" style="background: #f5f5f5; color: #999;">No GitHub</div>
                                    <div class="link-item upvote-item">üî• 15</div>
                                    <div class="link-item stars-item">‚≠ê 0</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">A new dataset and pipeline for data curation improve the performance of fully open multimodal large language models, achieving state-of-the-art results competitive with semi-open models.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Fully open <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=multimodal%20large%20language%20models">multimodal large language models</a> (MLLMs) currently lag behind
proprietary counterparts, primarily due to a significant gap in data quality
for <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=supervised%20fine-tuning">supervised fine-tuning</a> (SFT). Existing open-source datasets are often
plagued by widespread noise and a critical deficit in complex reasoning data,
such as <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Chain-of-Thought">Chain-of-Thought</a> (CoT), which hinders the development of advanced model
capabilities. Addressing these challenges, our work makes three primary
contributions. First, we introduce <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Honey-Data-15M">Honey-Data-15M</a>, a new SFT dataset comprising
approximately 15 million QA pairs, processed through multiple cleaning
techniques and enhanced with a novel dual-level (short and long) CoT enrichment
strategy. Second, we introduce <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=HoneyPipe">HoneyPipe</a>, the data curation pipeline, and its
underlying framework <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=DataStudio">DataStudio</a>, providing the community with a transparent and
adaptable methodology for data curation that moves beyond static dataset
releases. Finally, to validate our dataset and pipeline, we train <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Bee-8B">Bee-8B</a>, an 8B
model on <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Honey-Data-15M">Honey-Data-15M</a>. Experiments show that <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Bee-8B">Bee-8B</a> establishes a new
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=state-of-the-art">state-of-the-art</a> (SOTA) for fully open MLLMs, achieving performance that is
competitive with, and in some cases surpasses, recent semi-open models such as
InternVL3.5-8B. Our work delivers to the community a suite of foundational
resources, including: the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Honey-Data-15M">Honey-Data-15M</a> corpus; the full-stack suite
comprising <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=HoneyPipe">HoneyPipe</a> and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=DataStudio">DataStudio</a>; training recipes; an evaluation harness;
and the model weights. This effort demonstrates that a principled focus on data
quality is a key pathway to developing fully open MLLMs that are highly
competitive with their semi-open counterparts.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="TRENDING WEEKLY" data-date="2025-10-14" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2510.12403" target="_blank" onclick="event.stopPropagation()">Robot Learning: A Tutorial</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-10-14</div>
                                        <div class="tags"><span class="tag">TRENDING</span><span class="tag">WEEKLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2510.12403" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/fracapuano/robot-learning-tutorial" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 47</div>
                                    <div class="link-item stars-item">‚≠ê 93</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">Robot learning transitions from model-based to data-driven methods, leveraging reinforcement learning and behavioral cloning to develop versatile, language-conditioned models for diverse tasks and robot types.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Robot learning is at an inflection point, driven by rapid advancements in
machine learning and the growing availability of large-scale robotics data.
This shift from classical, model-based methods to data-driven, learning-based
paradigms is unlocking unprecedented capabilities in autonomous systems. This
tutorial navigates the landscape of modern robot learning, charting a course
from the foundational principles of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Reinforcement%20Learning">Reinforcement Learning</a> and Behavioral
Cloning to generalist, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=language-conditioned%20models">language-conditioned models</a> capable of operating across
diverse tasks and even robot embodiments. This work is intended as a guide for
researchers and practitioners, and our goal is to equip the reader with the
conceptual understanding and practical tools necessary to contribute to
developments in robot learning, with ready-to-use examples implemented in
lerobot.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="MONTHLY" data-date="2025-09-28" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2509.24002" target="_blank" onclick="event.stopPropagation()">MCPMark: A Benchmark for Stress-Testing Realistic and Comprehensive MCP
  Use</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-09-28</div>
                                        <div class="tags"><span class="tag">MONTHLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2509.24002" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/eval-sys/mcpmark" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 162</div>
                                    <div class="link-item stars-item">‚≠ê 269</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">MCPMark is a comprehensive benchmark for evaluating MCP use in real-world workflows, featuring diverse tasks that require richer interactions with the environment, and reveals that current LLMs perform poorly on these tasks.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=MCP">MCP</a> standardizes how <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=LLMs">LLMs</a> interact with external systems, forming the
foundation for <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=general%20agents">general agents</a>. However, existing <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=MCP%20benchmarks">MCP benchmarks</a> remain narrow
in scope: they focus on read-heavy tasks or tasks with limited interaction
depth, and fail to capture the complexity and realism of real-world workflows.
To address this gap, we propose <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=MCPMark">MCPMark</a>, a benchmark designed to evaluate <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=MCP">MCP</a>
use in a more realistic and comprehensive manner. It consists of 127
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=high-quality%20tasks">high-quality tasks</a> collaboratively created by <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=domain%20experts">domain experts</a> and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=AI%20agents">AI agents</a>.
Each task begins with a curated <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=initial%20state">initial state</a> and includes a programmatic
script for <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=automatic%20verification">automatic verification</a>. These tasks demand richer and more diverse
interactions with the environment, involving a broad range of create, read,
update, and delete (CRUD) operations. We conduct a comprehensive evaluation of
cutting-edge <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=LLMs">LLMs</a> using a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=minimal%20agent%20framework">minimal agent framework</a> that operates in a
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=tool-calling%20loop">tool-calling loop</a>. Empirical results show that the best-performing model,
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=gpt-5-medium">gpt-5-medium</a>, reaches only 52.56\% <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=pass%401">pass@1</a> and 33.86\% <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=pass%5E4">pass^4</a>, while other
widely regarded strong models, including <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=claude-sonnet-4">claude-sonnet-4</a> and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=o3">o3</a>, fall below
30\% <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=pass%401">pass@1</a> and 15\% <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=pass%5E4">pass^4</a>. On average, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=LLMs">LLMs</a> require 16.2 execution
turns and 17.4 <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=tool%20calls">tool calls</a> per task, significantly surpassing those in
previous <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=MCP%20benchmarks">MCP benchmarks</a> and highlighting the stress-testing nature of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=MCPMark">MCPMark</a>.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="DAILY" data-date="2025-10-15" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2510.13804" target="_blank" onclick="event.stopPropagation()">Generative Universal Verifier as Multimodal Meta-Reasoner</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-10-15</div>
                                        <div class="tags"><span class="tag">DAILY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2510.13804" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <div class="link-item" style="background: #f5f5f5; color: #999;">No GitHub</div>
                                    <div class="link-item upvote-item">üî• 14</div>
                                    <div class="link-item stars-item">‚≠ê 0</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">Generative Universal Verifier enhances multimodal reasoning by providing reliable visual verification through ViVerBench, OmniVerifier-7B, and OmniVerifier-TTS, improving generation and refinement capabilities.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                We introduce <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Generative%20Universal%20Verifier">Generative Universal Verifier</a>, a novel concept and plugin
designed for next-generation <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=multimodal%20reasoning">multimodal reasoning</a> in <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=vision-language%20models">vision-language models</a> and
unified multimodal models, providing the fundamental capability of reflection
and refinement on visual outcomes during the reasoning and generation process.
This work makes three main contributions: (1) We build <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=ViVerBench">ViVerBench</a>, a
comprehensive benchmark spanning 16 categories of critical tasks for evaluating
visual outcomes in <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=multimodal%20reasoning">multimodal reasoning</a>. Results show that existing VLMs
consistently underperform across these tasks, underscoring a substantial gap
from human-level capability in reliable <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=visual%20verification">visual verification</a>. (2) We design two
automated pipelines to construct large-scale <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=visual%20verification">visual verification</a> data and train
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=OmniVerifier-7B">OmniVerifier-7B</a>, the first omni-capable generative verifier trained for
universal <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=visual%20verification">visual verification</a> and achieves notable gains on <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=ViVerBench">ViVerBench</a>(+8.3).
Through training, we identify three <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=atomic%20capabilities">atomic capabilities</a> in <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=visual%20verification">visual verification</a>
and demonstrate how they generalize and interact synergistically. (3) We
propose <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=OmniVerifier-TTS">OmniVerifier-TTS</a>, a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=sequential%20test-time%20scaling">sequential test-time scaling</a> paradigm that
leverages the universal verifier to bridge image generation and editing within
unified models, enhancing the upper bound of generative ability through
iterative fine-grained optimization. Beyond generation, we extend universal
verifier to broader world-modeling interleaved reasoning scenarios.
Empirically, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=OmniVerifier-TTS">OmniVerifier-TTS</a> achieves improvements on <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=T2I-ReasonBench">T2I-ReasonBench</a>(+3.7),
and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=GenEval%2B%2B">GenEval++</a>(+4.3), outperforming existing parallel test-time scaling methods,
such as Best-of-N. By endowing <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=multimodal%20reasoning">multimodal reasoning</a> with reliable visual
verification, OmniVerifier advances both reliable reflection during generation
and scalable test-time refinement, marking a step toward more trustworthy and
controllable next-generation reasoning systems.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="WEEKLY TRENDING" data-date="2025-10-14" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2510.12586" target="_blank" onclick="event.stopPropagation()">Advancing End-to-End Pixel Space Generative Modeling via Self-supervised
  Pre-training</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-10-14</div>
                                        <div class="tags"><span class="tag">TRENDING</span><span class="tag">WEEKLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2510.12586" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/AMAP-ML/EPG" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 91</div>
                                    <div class="link-item stars-item">‚≠ê 68</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">(No summary available)</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Pixel-space generative models are often more difficult to train and generally
underperform compared to their latent-space counterparts, leaving a persistent
performance and efficiency gap. In this paper, we introduce a novel two-stage
training framework that closes this gap for pixel-space diffusion and
consistency models. In the first stage, we pre-train encoders to capture
meaningful semantics from clean images while aligning them with points along
the same deterministic sampling trajectory, which evolves points from the prior
to the data distribution. In the second stage, we integrate the encoder with a
randomly initialized decoder and fine-tune the complete model end-to-end for
both diffusion and consistency models. Our training framework demonstrates
strong empirical performance on ImageNet dataset. Specifically, our diffusion
model reaches an FID of 2.04 on ImageNet-256 and 2.35 on ImageNet-512 with 75
number of function evaluations (NFE), surpassing prior pixel-space methods by a
large margin in both generation quality and efficiency while rivaling leading
VAE-based models at comparable training cost. Furthermore, on ImageNet-256, our
consistency model achieves an impressive FID of 8.82 in a single sampling step,
significantly surpassing its latent-space counterpart. To the best of our
knowledge, this marks the first successful training of a consistency model
directly on high-resolution images without relying on pre-trained VAEs or
diffusion models.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="TRENDING MONTHLY" data-date="2025-10-06" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2510.05096" target="_blank" onclick="event.stopPropagation()">Paper2Video: Automatic Video Generation from Scientific Papers</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-10-06</div>
                                        <div class="tags"><span class="tag">MONTHLY</span><span class="tag">TRENDING</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2510.05096" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/showlab/Paper2Video" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 101</div>
                                    <div class="link-item stars-item">‚≠ê 950</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">PaperTalker is a multi-agent framework that automates academic presentation video generation by integrating slide generation, layout refinement, subtitling, speech synthesis, and talking-head rendering, outperforming existing methods.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Academic presentation videos have become an essential medium for research
communication, yet producing them remains highly labor-intensive, often
requiring hours of slide design, recording, and editing for a short 2 to 10
minutes video. Unlike natural video, presentation video generation involves
distinctive challenges: inputs from research papers, dense multi-modal
information (text, figures, tables), and the need to coordinate multiple
aligned channels such as slides, subtitles, speech, and human talker. To
address these challenges, we introduce PaperTalker, the first benchmark of 101
research papers paired with author-created presentation videos, slides, and
speaker metadata. We further design four tailored evaluation metrics--Meta
Similarity, PresentArena, PresentQuiz, and IP Memory--to measure how videos
convey the paper's information to the audience. Building on this foundation, we
propose PaperTalker, the first <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=multi-agent%20framework">multi-agent framework</a> for academic presentation
video generation. It integrates <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=slide%20generation">slide generation</a> with effective layout
refinement by a novel effective <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=tree%20search%20visual%20choice">tree search visual choice</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=cursor%20grounding">cursor grounding</a>,
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=subtitling">subtitling</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=speech%20synthesis">speech synthesis</a>, and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=talking-head%20rendering">talking-head rendering</a>, while parallelizing
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=slide-wise%20generation">slide-wise generation</a> for efficiency. Experiments on Paper2Video demonstrate
that the presentation videos produced by our approach are more faithful and
informative than existing baselines, establishing a practical step toward
automated and ready-to-use academic video generation. Our dataset, agent, and
code are available at https://github.com/showlab/Paper2Video.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="MONTHLY" data-date="2025-09-29" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2509.25541" target="_blank" onclick="event.stopPropagation()">Vision-Zero: Scalable VLM Self-Improvement via Strategic Gamified
  Self-Play</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-09-29</div>
                                        <div class="tags"><span class="tag">MONTHLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2509.25541" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/wangqinsi1/Vision-Zero" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 133</div>
                                    <div class="link-item stars-item">‚≠ê 77</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">Vision-Zero is a domain-agnostic framework that enhances vision-language models through self-improvement in competitive visual games, using Iterative Self-Play Policy Optimization and achieving state-of-the-art performance without human annotation.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Although <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=reinforcement%20learning">reinforcement learning</a> (RL) can effectively enhance the reasoning
capabilities of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=vision-language%20models">vision-language models</a> (VLMs), current methods remain heavily
dependent on labor-intensive datasets that require extensive manual
construction and verification, leading to extremely high training costs and
consequently constraining the practical deployment of VLMs. To address this
challenge, we propose Vision-Zero, a domain-agnostic framework enabling VLM
self-improvement through competitive visual games generated from arbitrary
image pairs. Specifically, Vision-Zero encompasses three main attributes: (1)
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Strategic%20Self-Play%20Framework">Strategic Self-Play Framework</a>: Vision-Zero trains VLMs in "Who Is the
Spy"-style games, where the models engage in strategic reasoning and actions
across multiple roles. Through interactive gameplay, models autonomously
generate their training data without human annotation. (2) Gameplay from
Arbitrary Images: Unlike existing gamified frameworks, Vision-Zero can generate
games from arbitrary images, thereby enhancing the model's reasoning ability
across diverse domains and showing strong generalization to different tasks. We
demonstrate this versatility using three distinct types of image datasets:
CLEVR-based synthetic scenes, charts, and real-world images. (3) Sustainable
Performance Gain: We introduce <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Iterative%20Self-Play%20Policy%20Optimization">Iterative Self-Play Policy Optimization</a>
(Iterative-SPO), a novel training algorithm that alternates between <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Self-Play">Self-Play</a>
and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=reinforcement%20learning%20with%20verifiable%20rewards">reinforcement learning with verifiable rewards</a> (RLVR), mitigating the
performance plateau often seen in <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=self-play">self-play</a>-only training and achieving
sustained long-term improvements. Despite using label-free data, Vision-Zero
achieves state-of-the-art performance on reasoning, chart question answering,
and vision-centric understanding tasks, surpassing other annotation-based
methods. Models and code has been released at
https://github.com/wangqinsi1/Vision-Zero.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="DAILY" data-date="2025-10-15" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2510.13626" target="_blank" onclick="event.stopPropagation()">LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action
  Models</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-10-15</div>
                                        <div class="tags"><span class="tag">DAILY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2510.13626" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/sylvestf/LIBERO-plus" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 13</div>
                                    <div class="link-item stars-item">‚≠ê 4</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">State-of-the-art Visual-Language-Action models show high benchmark scores but are brittle to various perturbations, particularly in camera viewpoints and robot initial states, and often ignore language instructions.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Visual-Language-Action (VLA) models report impressive success rates on
robotic manipulation benchmarks, yet these results may mask fundamental
weaknesses in robustness. We perform a systematic <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=vulnerability%20analysis">vulnerability analysis</a> by
introducing controlled <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=perturbations">perturbations</a> across seven dimensions: <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=objects%20layout">objects layout</a>,
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=camera%20viewpoints">camera viewpoints</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=robot%20initial%20states">robot initial states</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=language%20instructions">language instructions</a>, light
conditions, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=background%20textures">background textures</a> and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=sensor%20noise">sensor noise</a>. We comprehensively analyzed
multiple state-of-the-art models and revealed consistent <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=brittleness">brittleness</a> beneath
apparent competence. Our analysis exposes critical weaknesses: models exhibit
extreme sensitivity to perturbation factors, including <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=camera%20viewpoints">camera viewpoints</a> and
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=robot%20initial%20states">robot initial states</a>, with performance dropping from 95% to below 30% under
modest <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=perturbations">perturbations</a>. Surprisingly, models are largely insensitive to language
variations, with further experiments revealing that models tend to ignore
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=language%20instructions">language instructions</a> completely. Our findings challenge the assumption that
high benchmark scores equate to true competency and highlight the need for
evaluation practices that assess <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=reliability">reliability</a> under realistic variation.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="DAILY" data-date="2025-10-06" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2510.04767" target="_blank" onclick="event.stopPropagation()">ParallelBench: Understanding the Trade-offs of Parallel Decoding in
  Diffusion LLMs</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-10-06</div>
                                        <div class="tags"><span class="tag">DAILY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2510.04767" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/furiosa-ai/ParallelBench" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 13</div>
                                    <div class="link-item stars-item">‚≠ê 7</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">Parallel decoding in diffusion LLMs degrades generation quality due to ignored token dependencies, highlighting the need for new decoding methods and benchmarks.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                While most <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=autoregressive%20LLMs">autoregressive LLMs</a> are constrained to one-by-one decoding,
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=diffusion%20LLMs">diffusion LLMs</a> (<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=dLLMs">dLLMs</a>) have attracted growing interest for their potential to
dramatically accelerate inference through <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=parallel%20decoding">parallel decoding</a>. Despite this
promise, the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=conditional%20independence%20assumption">conditional independence assumption</a> in <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=dLLMs">dLLMs</a> causes parallel
decoding to ignore token dependencies, inevitably degrading generation quality
when these dependencies are strong. However, existing works largely overlook
these inherent challenges, and evaluations on standard benchmarks (e.g., math
and coding) are not sufficient to capture the quality degradation caused by
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=parallel%20decoding">parallel decoding</a>. To address this gap, we first provide an
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=information-theoretic%20analysis">information-theoretic analysis</a> of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=parallel%20decoding">parallel decoding</a>. We then conduct case
studies on analytically tractable <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=synthetic%20list%20operations">synthetic list operations</a> from both data
distribution and decoding strategy perspectives, offering quantitative insights
that highlight the fundamental limitations of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=parallel%20decoding">parallel decoding</a>. Building on
these insights, we propose <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=ParallelBench">ParallelBench</a>, the first benchmark specifically
designed for <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=dLLMs">dLLMs</a>, featuring realistic tasks that are trivial for humans and
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=autoregressive%20LLMs">autoregressive LLMs</a> yet exceptionally challenging for <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=dLLMs">dLLMs</a> under parallel
decoding. Using <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=ParallelBench">ParallelBench</a>, we systematically analyze both <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=dLLMs">dLLMs</a> and
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=autoregressive%20LLMs">autoregressive LLMs</a>, revealing that: (i) <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=dLLMs">dLLMs</a> under <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=parallel%20decoding">parallel decoding</a> can
suffer dramatic quality degradation in <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=real-world%20scenarios">real-world scenarios</a>, and (ii) current
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=parallel%20decoding">parallel decoding</a> strategies struggle to adapt their degree of parallelism
based on task difficulty, thus failing to achieve meaningful speedup without
compromising quality. Our findings underscore the pressing need for innovative
decoding methods that can overcome the current <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=speed-quality%20trade-off">speed-quality trade-off</a>. We
release our benchmark to help accelerate the development of truly efficient
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=dLLMs">dLLMs</a>.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="WEEKLY" data-date="2025-10-10" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2510.09116" target="_blank" onclick="event.stopPropagation()">DITING: A Multi-Agent Evaluation Framework for Benchmarking Web Novel
  Translation</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-10-10</div>
                                        <div class="tags"><span class="tag">WEEKLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2510.09116" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/WHUNextGen/DITING" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 91</div>
                                    <div class="link-item stars-item">‚≠ê 7</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">A new evaluation framework, DITING, and a reasoning-driven multi-agent evaluation framework, AgentEval, are introduced to assess the quality of web novel translations, revealing that Chinese-trained LLMs outperform larger foreign models.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Large%20language%20models">Large language models</a> (LLMs) have substantially advanced <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=machine%20translation">machine translation</a>
(MT), yet their effectiveness in translating <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=web%20novels">web novels</a> remains unclear.
Existing benchmarks rely on surface-level metrics that fail to capture the
distinctive traits of this genre. To address these gaps, we introduce <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=DITING">DITING</a>,
the first comprehensive evaluation framework for web novel translation,
assessing narrative and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=cultural%20fidelity">cultural fidelity</a> across six dimensions: idiom
translation, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=lexical%20ambiguity">lexical ambiguity</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=terminology%20localization">terminology localization</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=tense%20consistency">tense consistency</a>,
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=zero-pronoun%20resolution">zero-pronoun resolution</a>, and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=cultural%20safety">cultural safety</a>, supported by over 18K
expert-annotated Chinese-English sentence pairs. We further propose <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=AgentEval">AgentEval</a>,
a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=reasoning-driven">reasoning-driven</a> <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=multi-agent%20evaluation">multi-agent evaluation</a> framework that simulates expert
deliberation to assess translation quality beyond lexical overlap, achieving
the highest correlation with human judgments among seven tested automatic
metrics. To enable metric comparison, we develop <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=MetricAlign">MetricAlign</a>, a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=meta-evaluation">meta-evaluation</a>
dataset of 300 sentence pairs annotated with error labels and scalar quality
scores. Comprehensive evaluation of fourteen open, closed, and commercial
models reveals that Chinese-trained LLMs surpass larger foreign counterparts,
and that <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=DeepSeek-V3">DeepSeek-V3</a> delivers the most faithful and stylistically coherent
translations. Our work establishes a new paradigm for exploring LLM-based web
novel translation and provides public resources to advance future research.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="MONTHLY" data-date="2025-09-29" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2509.25454" target="_blank" onclick="event.stopPropagation()">DeepSearch: Overcome the Bottleneck of Reinforcement Learning with
  Verifiable Rewards via Monte Carlo Tree Search</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-09-29</div>
                                        <div class="tags"><span class="tag">MONTHLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2509.25454" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <div class="link-item" style="background: #f5f5f5; color: #999;">No GitHub</div>
                                    <div class="link-item upvote-item">üî• 126</div>
                                    <div class="link-item stars-item">‚≠ê 0</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">DeepSearch integrates Monte Carlo Tree Search into RLVR training to enhance exploration and credit assignment, achieving state-of-the-art performance with reduced computational cost.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Although <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=RLVR">RLVR</a> has become an essential component for developing advanced
reasoning skills in LLMs, contemporary studies have documented training
plateaus that emerge following thousands of optimization steps, demonstrating
notable decreases in performance gains despite increased computational
investment. This limitation stems from the sparse exploration patterns inherent
in current <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=RLVR">RLVR</a> practices, where models rely on limited rollouts that often
miss critical reasoning paths and fail to provide systematic coverage of the
solution space. We present DeepSearch, a framework that integrates Monte Carlo
Tree Search directly into <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=RLVR">RLVR</a> training. In contrast to existing methods that
rely on tree search only at inference, DeepSearch embeds structured search into
the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=training%20loop">training loop</a>, enabling <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=systematic%20exploration">systematic exploration</a> and fine-grained credit
assignment across reasoning steps. Through training-time exploration,
DeepSearch addresses the fundamental bottleneck of insufficient exploration,
which leads to diminishing performance improvements over prolonged training
steps. Our contributions include: (1) a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=global%20frontier%20selection">global frontier selection</a> strategy that
prioritizes promising nodes across the search tree, (2) selection with
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=entropy-based%20guidance">entropy-based guidance</a> that identifies confident paths for supervision, and (3)
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=adaptive%20replay%20buffer">adaptive replay buffer</a> training with <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=solution%20caching">solution caching</a> for efficiency.
Experiments on <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=mathematical%20reasoning%20benchmarks">mathematical reasoning benchmarks</a> show that DeepSearch achieves
62.95% average accuracy and establishes a new state-of-the-art for 1.5B
reasoning models - using 5.7x fewer GPU hours than extended training
approaches. These results highlight the importance of strategic exploration
over brute-force scaling and demonstrate the promise of algorithmic innovation
for advancing <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=RLVR">RLVR</a> methodologies. DeepSearch establishes a new direction for
scaling reasoning capabilities through systematic search rather than prolonged
computation.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="TRENDING" data-date="2025-10-10" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2510.09608" target="_blank" onclick="event.stopPropagation()">StreamingVLM: Real-Time Understanding for Infinite Video Streams</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-10-10</div>
                                        <div class="tags"><span class="tag">TRENDING</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2510.09608" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/mit-han-lab/streaming-vlm" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 40</div>
                                    <div class="link-item stars-item">‚≠ê 409</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">StreamingVLM is a real-time vision-language model that efficiently processes infinite video streams using a compact KV cache and supervised fine-tuning, achieving high performance on long videos and diverse benchmarks.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Vision-language%20models">Vision-language models</a> (<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=VLMs">VLMs</a>) could power <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=real-time%20assistants">real-time assistants</a> and autonomous
agents, but they face a critical challenge: understanding near-infinite video
streams without escalating latency and memory usage. Processing entire videos
with full attention leads to <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=quadratic%20computational%20costs">quadratic computational costs</a> and poor performance
on long videos. Meanwhile, simple <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=sliding%20window%20methods">sliding window methods</a> are also flawed, as
they either break coherence or suffer from high latency due to redundant
recomputation. In this paper, we introduce StreamingVLM, a model designed for
real-time, stable understanding of infinite visual input. Our approach is a
unified framework that aligns training with streaming inference. During
inference, we maintain a compact KV cache by reusing states of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=attention%20sinks">attention sinks</a>,
a short window of recent <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=vision%20tokens">vision tokens</a>, and a long window of recent text
tokens. This streaming ability is instilled via a simple <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=supervised%20fine-tuning">supervised fine-tuning</a>
(<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=SFT">SFT</a>) strategy that applies full attention on short, overlapped video chunks,
which effectively mimics the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=inference-time%20attention%20pattern">inference-time attention pattern</a> without training
on prohibitively long contexts. For evaluation, we build <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Inf-Streams-Eval">Inf-Streams-Eval</a>, a
new benchmark with videos averaging over two hours that requires dense,
per-second alignment between frames and text. On <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Inf-Streams-Eval">Inf-Streams-Eval</a>, StreamingVLM
achieves a 66.18% <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=win%20rate">win rate</a> against GPT-4O mini and maintains stable, real-time
performance at up to 8 FPS on a single <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=NVIDIA%20H100">NVIDIA H100</a>. Notably, our <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=SFT">SFT</a> strategy
also enhances general <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=VQA%20abilities">VQA abilities</a> without any VQA-specific fine-tuning,
improving performance on <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=LongVideoBench">LongVideoBench</a> by +4.30 and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=OVOBench%20Realtime">OVOBench Realtime</a> by
+5.96. Code is available at https://github.com/mit-han-lab/streaming-vlm.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="WEEKLY" data-date="2025-10-13" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2510.11693" target="_blank" onclick="event.stopPropagation()">Scaling Language-Centric Omnimodal Representation Learning</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-10-13</div>
                                        <div class="tags"><span class="tag">WEEKLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2510.11693" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/LCO-Embedding/LCO-Embedding" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 83</div>
                                    <div class="link-item stars-item">‚≠ê 18</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">(No summary available)</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Recent multimodal embedding approaches leveraging multimodal large language
models (MLLMs) fine-tuned with contrastive learning (CL) have shown promising
results, yet the underlying reasons behind their superiority remain
underexplored. This work argues that a crucial advantage of MLLM-based
approaches stems from implicit cross-modal alignment achieved during generative
pretraining, where the language decoder learns to exploit multimodal signals
within a shared representation space for generating unimodal outputs. Through
analysis of anisotropy and kernel similarity structure, we empirically confirm
that latent alignment emerges within MLLM representations, allowing CL to serve
as a lightweight refinement stage. Leveraging this insight, we propose a
Language-Centric Omnimodal Embedding framework, termed LCO-Emb. Extensive
experiments across diverse backbones and benchmarks demonstrate its
effectiveness, achieving state-of-the-art performance across modalities.
Furthermore, we identify a Generation-Representation Scaling Law (GRSL),
showing that the representational capabilities gained through contrastive
refinement scales positively with the MLLM's generative capabilities. This
suggests that improving generative abilities evolves as an effective paradigm
for enhancing representation quality. We provide a theoretical explanation of
GRSL, which formally links the MLLM's generative quality to the upper bound on
its representation performance, and validate it on a challenging, low-resource
visual-document retrieval task, showing that continual generative pretraining
before CL can further enhance the potential of a model's embedding
capabilities. Codes, models, and resources are available at
https://github.com/LCO-Embedding/LCO-Embedding.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="DAILY" data-date="2025-10-15" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2510.13802" target="_blank" onclick="event.stopPropagation()">Trace Anything: Representing Any Video in 4D via Trajectory Fields</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-10-15</div>
                                        <div class="tags"><span class="tag">DAILY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2510.13802" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <div class="link-item" style="background: #f5f5f5; color: #999;">No GitHub</div>
                                    <div class="link-item upvote-item">üî• 12</div>
                                    <div class="link-item stars-item">‚≠ê 0</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">Trace Anything, a neural network, predicts video trajectories in a single pass, achieving state-of-the-art performance and demonstrating efficiency and emergent abilities like motion forecasting.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Effective spatio-temporal representation is fundamental to modeling,
understanding, and predicting dynamics in videos. The atomic unit of a video,
the pixel, traces a continuous 3D trajectory over time, serving as the
primitive element of dynamics. Based on this principle, we propose representing
any video as a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Trajectory%20Field">Trajectory Field</a>: a dense mapping that assigns a continuous 3D
trajectory function of time to each pixel in every frame. With this
representation, we introduce <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Trace%20Anything">Trace Anything</a>, a neural network that predicts the
entire <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=trajectory%20field">trajectory field</a> in a single feed-forward pass. Specifically, for each
pixel in each frame, our model predicts a set of control points that
parameterizes a trajectory (i.e., a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=B-spline">B-spline</a>), yielding its 3D position at
arbitrary query time instants. We trained the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Trace%20Anything">Trace Anything</a> model on
large-scale 4D data, including data from our new platform, and our experiments
demonstrate that: (i) <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Trace%20Anything">Trace Anything</a> achieves state-of-the-art performance on
our new benchmark for <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=trajectory%20field%20estimation">trajectory field estimation</a> and performs competitively on
established <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=point-tracking%20benchmarks">point-tracking benchmarks</a>; (ii) it offers significant efficiency
gains thanks to its one-pass paradigm, without requiring iterative optimization
or auxiliary estimators; and (iii) it exhibits emergent abilities, including
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=goal-conditioned%20manipulation">goal-conditioned manipulation</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=motion%20forecasting">motion forecasting</a>, and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=spatio-temporal%20fusion">spatio-temporal fusion</a>.
Project page: https://trace-anything.github.io/.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="TRENDING" data-date="2025-10-14" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2510.12747" target="_blank" onclick="event.stopPropagation()">FlashVSR: Towards Real-Time Diffusion-Based Streaming Video
  Super-Resolution</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-10-14</div>
                                        <div class="tags"><span class="tag">TRENDING</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2510.12747" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/OpenImagingLab/FlashVSR" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 31</div>
                                    <div class="link-item stars-item">‚≠ê 87</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">(No summary available)</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Diffusion models have recently advanced video restoration, but applying them
to real-world video super-resolution (VSR) remains challenging due to high
latency, prohibitive computation, and poor generalization to ultra-high
resolutions. Our goal in this work is to make diffusion-based VSR practical by
achieving efficiency, scalability, and real-time performance. To this end, we
propose FlashVSR, the first diffusion-based one-step streaming framework
towards real-time VSR. FlashVSR runs at approximately 17 FPS for 768x1408
videos on a single A100 GPU by combining three complementary innovations: (i) a
train-friendly three-stage distillation pipeline that enables streaming
super-resolution, (ii) locality-constrained sparse attention that cuts
redundant computation while bridging the train-test resolution gap, and (iii) a
tiny conditional decoder that accelerates reconstruction without sacrificing
quality. To support large-scale training, we also construct VSR-120K, a new
dataset with 120k videos and 180k images. Extensive experiments show that
FlashVSR scales reliably to ultra-high resolutions and achieves
state-of-the-art performance with up to 12x speedup over prior one-step
diffusion VSR models. We will release the code, pretrained models, and dataset
to foster future research in efficient diffusion-based VSR.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="MONTHLY" data-date="2025-10-01" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2510.01141" target="_blank" onclick="event.stopPropagation()">Apriel-1.5-15b-Thinker</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-10-01</div>
                                        <div class="tags"><span class="tag">MONTHLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2510.01141" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <div class="link-item" style="background: #f5f5f5; color: #999;">No GitHub</div>
                                    <div class="link-item upvote-item">üî• 108</div>
                                    <div class="link-item stars-item">‚≠ê 0</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">A 15-billion parameter multimodal reasoning model achieves competitive performance through a progressive training methodology without reinforcement learning, demonstrating efficient use of computational resources.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                We present Apriel-1.5-15B-Thinker, a 15-billion parameter open-weights
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=multimodal%20reasoning%20model">multimodal reasoning model</a> that achieves frontier-level performance through
training design rather than sheer scale. Starting from Pixtral-12B, we apply a
progressive three-stage methodology: (1) <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=depth%20upscaling">depth upscaling</a> to expand reasoning
capacity without pretraining from scratch, (2) <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=staged%20continual%20pre-training">staged continual pre-training</a>
that first develops foundational text and vision understanding, then enhances
visual reasoning through targeted <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=synthetic%20data%20generation">synthetic data generation</a> addressing spatial
structure, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=compositional%20understanding">compositional understanding</a>, and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=fine-grained%20perception">fine-grained perception</a>, and (3)
high-quality <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=text-only%20supervised%20fine-tuning">text-only supervised fine-tuning</a> on curated instruction-response
pairs with explicit <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=reasoning%20traces">reasoning traces</a> spanning mathematics, coding, science, and
tool use. Notably, our model achieves competitive results without reinforcement
learning or preference optimization, isolating the contribution of our
data-centric continual pre-training approach. On the Artificial Analysis
Intelligence Index, Apriel-1.5-15B-Thinker attains a score of 52, matching
DeepSeek-R1-0528 despite requiring significantly fewer computational resources.
Across ten image benchmarks, its performance is on average within five points
of Gemini-2.5-Flash and Claude Sonnet-3.7, a key achievement for a model
operating within <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=single-GPU%20deployment">single-GPU deployment</a> constraints. Our results demonstrate
that thoughtful mid-training 2 design can close substantial capability gaps
without massive scale, making frontier-level multimodal reasoning accessible to
organizations with limited infrastructure. We release the model checkpoint, all
training recipes, and evaluation protocols under the MIT license to to advance
open-source research.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="DAILY" data-date="2025-10-15" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2510.13344" target="_blank" onclick="event.stopPropagation()">UniMoE-Audio: Unified Speech and Music Generation with Dynamic-Capacity
  MoE</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-10-15</div>
                                        <div class="tags"><span class="tag">DAILY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2510.13344" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <div class="link-item" style="background: #f5f5f5; color: #999;">No GitHub</div>
                                    <div class="link-item upvote-item">üî• 12</div>
                                    <div class="link-item stars-item">‚≠ê 0</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">UniMoE-Audio, a unified speech and music generation model using a Dynamic-Capacity Mixture-of-Experts framework, addresses data imbalance and task conflicts, achieving state-of-the-art performance and enhanced cross-domain synergy.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Recent advances in unified multimodal models indicate a clear trend towards
comprehensive content generation. However, the auditory domain remains a
significant challenge, with music and speech often developed in isolation,
hindering progress towards universal audio synthesis. This separation stems
from inherent task conflicts and severe data imbalances, which impede the
development of a truly unified audio generation model. To address this
challenge, we propose Uni<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=MoE">MoE</a>-Audio, a unified speech and music generation model
within a novel <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Dynamic-Capacity%20Mixture-of-Experts">Dynamic-Capacity Mixture-of-Experts</a> (<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=MoE">MoE</a>) framework.
Architecturally, Uni<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=MoE">MoE</a>-Audio introduces a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Top-P%20routing%20strategy">Top-P routing strategy</a> for dynamic
expert number allocation, and a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=hybrid%20expert%20design">hybrid expert design</a> comprising <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=routed%20experts">routed experts</a>
for domain-specific knowledge, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=shared%20experts">shared experts</a> for domain-agnostic features, and
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=null%20experts">null experts</a> for adaptive computation skipping. To tackle data imbalance, we
introduce a three-stage training curriculum: 1) <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Independent%20Specialist%20Training">Independent Specialist Training</a>
leverages original datasets to instill domain-specific knowledge into each
"proto-expert" without interference; 2) <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=MoE%20Integration%20and%20Warmup">MoE Integration and Warmup</a> incorporates
these specialists into the Uni<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=MoE">MoE</a>-Audio architecture, warming up the gate
module and shared expert using a subset of balanced dataset; and 3) Synergistic
Joint Training trains the entire model end-to-end on the fully balanced
dataset, fostering enhanced cross-domain synergy. Extensive experiments show
that Uni<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=MoE">MoE</a>-Audio not only achieves state-of-the-art performance on major
speech and music generation benchmarks, but also demonstrates superior
synergistic learning, mitigating the performance degradation typically seen in
naive joint training. Our findings highlight the substantial potential of
specialized <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=MoE">MoE</a> architecture and curated training strategies in advancing the
field of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=universal%20audio%20generation">universal audio generation</a>. Homepage:
https://mukioxun.github.io/Uni-<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=MoE">MoE</a>-site/home.html
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="WEEKLY" data-date="2025-10-10" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2510.09426" target="_blank" onclick="event.stopPropagation()">KORMo: Korean Open Reasoning Model for Everyone</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-10-10</div>
                                        <div class="tags"><span class="tag">WEEKLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2510.09426" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <div class="link-item" style="background: #f5f5f5; color: #999;">No GitHub</div>
                                    <div class="link-item upvote-item">üî• 60</div>
                                    <div class="link-item stars-item">‚≠ê 0</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">A large-scale investigation into constructing a fully open bilingual LLM for Korean using synthetic data demonstrates that such data can sustain pretraining and achieve performance comparable to multilingual baselines.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                This work presents the first large-scale investigation into constructing a
fully open bilingual <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=large%20language%20model">large language model</a> (<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=LLM">LLM</a>) for a non-English language,
specifically Korean, trained predominantly on <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=synthetic%20data">synthetic data</a>. We introduce
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=KORMo-10B">KORMo-10B</a>, a 10.8B-parameter model trained from scratch on a Korean-English
corpus in which 68.74% of the Korean portion is synthetic. Through systematic
experimentation, we demonstrate that <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=synthetic%20data">synthetic data</a>, when carefully curated
with balanced linguistic coverage and diverse instruction styles, does not
cause instability or degradation during large-scale <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=pretraining">pretraining</a>. Furthermore,
the model achieves performance comparable to that of contemporary open-weight
multilingual baselines across a wide range of reasoning, knowledge, and
instruction-following benchmarks. Our experiments reveal two key findings: (1)
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=synthetic%20data">synthetic data</a> can reliably sustain long-horizon <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=pretraining">pretraining</a> without model
collapse, and (2) <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=bilingual%20instruction%20tuning">bilingual instruction tuning</a> enables near-native reasoning
and discourse coherence in Korean. By fully releasing all components including
data, code, training recipes, and logs, this work establishes a transparent
framework for developing <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=synthetic%20data">synthetic data</a>-driven <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=fully%20open%20models">fully open models</a> (<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=FOMs">FOMs</a>) in
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=low-resource%20settings">low-resource settings</a> and sets a reproducible precedent for future multilingual
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=LLM">LLM</a> research.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="MONTHLY" data-date="2025-10-01" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2510.00446" target="_blank" onclick="event.stopPropagation()">LongCodeZip: Compress Long Context for Code Language Models</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-10-01</div>
                                        <div class="tags"><span class="tag">MONTHLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2510.00446" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/YerbaPage/LongCodeZip" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 105</div>
                                    <div class="link-item stars-item">‚≠ê 99</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">LongCodeZip is a code compression framework for LLMs that uses dual-stage compression to reduce context size without degrading performance, improving efficiency in code intelligence applications.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Code generation under long contexts is becoming increasingly critical as
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Large%20Language%20Models">Large Language Models</a> (LLMs) are required to reason over extensive information
in the codebase. While recent advances enable <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=code%20LLMs">code LLMs</a> to process long inputs,
high API costs and generation latency remain substantial bottlenecks. Existing
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=context%20pruning">context pruning</a> techniques, such as <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=LLMLingua">LLMLingua</a>, achieve promising results for
general text but overlook code-specific structures and dependencies, leading to
suboptimal performance in programming tasks. In this paper, we propose
LongCodeZip, a novel plug-and-play code compression framework designed
specifically for <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=code%20LLMs">code LLMs</a>. LongCodeZip employs a dual-stage strategy: (1)
coarse-grained compression, which identifies and ranks <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=function-level%20chunks">function-level chunks</a>
using <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=conditional%20perplexity">conditional perplexity</a> with respect to the instruction, retaining only
the most relevant functions; and (2) <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=fine-grained%20compression">fine-grained compression</a>, which segments
retained functions into blocks based on perplexity and selects an optimal
subset under an adaptive <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=token%20budget">token budget</a> to maximize relevance. Evaluations across
multiple tasks, including <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=code%20completion">code completion</a>, summarization, and question
answering, show that LongCodeZip consistently outperforms baseline methods,
achieving up to a 5.6x compression ratio without degrading task performance. By
effectively reducing context size while preserving essential information,
LongCodeZip enables LLMs to better scale to real-world, large-scale code
scenarios, advancing the efficiency and capability of code intelligence
applications.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="DAILY" data-date="2025-10-15" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2510.13515" target="_blank" onclick="event.stopPropagation()">UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-10-15</div>
                                        <div class="tags"><span class="tag">DAILY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2510.13515" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/GaryGuTC/UniME-v2" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 10</div>
                                    <div class="link-item stars-item">‚≠ê 13</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">A novel Universal Multimodal Embedding (UniME-V2) model uses MLLMs to enhance representation learning by identifying diverse, high-quality hard negatives and improving discriminative capacity through soft semantic matching scores.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Universal%20multimodal%20embedding">Universal multimodal embedding</a> models are foundational to various tasks.
Existing approaches typically employ <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=in-batch%20negative%20mining">in-batch negative mining</a> by measuring the
similarity of query-candidate pairs. However, these methods often struggle to
capture subtle semantic differences among candidates and lack diversity in
negative samples. Moreover, the embeddings exhibit limited discriminative
ability in distinguishing false and hard negatives. In this paper, we leverage
the advanced understanding capabilities of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=MLLMs">MLLMs</a> to enhance representation
learning and present a novel <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Universal%20Multimodal%20Embedding">Universal Multimodal Embedding</a> (<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=UniME-V2">UniME-V2</a>) model.
Our approach first constructs a potential hard negative set through global
retrieval. We then introduce the MLLM-as-a-Judge mechanism, which utilizes
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=MLLMs">MLLMs</a> to assess the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=semantic%20alignment">semantic alignment</a> of query-candidate pairs and generate
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=soft%20semantic%20matching%20scores">soft semantic matching scores</a>. These scores serve as a foundation for hard
negative mining, mitigating the impact of false negatives and enabling the
identification of diverse, high-quality hard negatives. Furthermore, the
semantic matching scores are used as <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=soft%20labels">soft labels</a> to mitigate the rigid
one-to-one mapping constraint. By aligning the similarity matrix with the soft
semantic matching score matrix, the model learns semantic distinctions among
candidates, significantly enhancing its discriminative capacity. To further
improve performance, we propose <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=UniME-V2">UniME-V2</a>-Reranker, a reranking model trained on
our mined hard negatives through a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=joint%20pairwise%20and%20listwise%20optimization">joint pairwise and listwise optimization</a>
approach. We conduct comprehensive experiments on the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=MMEB%20benchmark">MMEB benchmark</a> and
multiple <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=retrieval%20tasks">retrieval tasks</a>, demonstrating that our method achieves
state-of-the-art performance on average across all tasks.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="TRENDING" data-date="2025-09-16" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2509.13312" target="_blank" onclick="event.stopPropagation()">WebWeaver: Structuring Web-Scale Evidence with Dynamic Outlines for
  Open-Ended Deep Research</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-09-16</div>
                                        <div class="tags"><span class="tag">TRENDING</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2509.13312" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/Alibaba-NLP/DeepResearch" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 104</div>
                                    <div class="link-item stars-item">‚≠ê 16k</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">WebWeaver, a dual-agent framework, addresses open-ended deep research challenges by integrating adaptive planning and focused synthesis to produce high-quality, reliable reports.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                This paper tackles <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=open-ended%20deep%20research">open-ended deep research</a> (OEDR), a complex challenge where
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=AI%20agents">AI agents</a> must synthesize vast web-scale information into insightful reports.
Current approaches are plagued by dual-fold limitations: static research
pipelines that decouple planning from <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=evidence%20acquisition">evidence acquisition</a> and one-shot
generation paradigms that easily suffer from <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=long-context%20failure">long-context failure</a> issues like
"<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=loss%20in%20the%20middle">loss in the middle</a>" and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=hallucinations">hallucinations</a>. To address these challenges, we
introduce WebWeaver, a novel <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=dual-agent%20framework">dual-agent framework</a> that emulates the human
research process. The <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=planner">planner</a> operates in a dynamic cycle, iteratively
interleaving <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=evidence%20acquisition">evidence acquisition</a> with <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=outline%20optimization">outline optimization</a> to produce a
comprehensive, source-grounded outline linking to a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=memory%20bank">memory bank</a> of evidence.
The <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=writer">writer</a> then executes a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=hierarchical%20retrieval">hierarchical retrieval</a> and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=writing%20process">writing process</a>,
composing the report section by section. By performing targeted retrieval of
only the necessary evidence from the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=memory%20bank">memory bank</a> for each part, it effectively
mitigates long-context issues. Our framework establishes a new state-of-the-art
across major OEDR benchmarks, including <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=DeepResearch%20Bench">DeepResearch Bench</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=DeepConsult">DeepConsult</a>, and
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=DeepResearchGym">DeepResearchGym</a>. These results validate our human-centric, iterative
methodology, demonstrating that adaptive planning and focused synthesis are
crucial for producing high-quality, reliable, and well-structured reports.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="WEEKLY" data-date="2025-10-10" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2510.09558" target="_blank" onclick="event.stopPropagation()">AutoPR: Let's Automate Your Academic Promotion!</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-10-10</div>
                                        <div class="tags"><span class="tag">WEEKLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2510.09558" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/LightChen233/AutoPR" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 46</div>
                                    <div class="link-item stars-item">‚≠ê 46</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">AutoPR, a multi-agent framework, automates the promotion of research papers by transforming them into engaging public content, significantly improving engagement metrics compared to direct LLM pipelines.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                As the volume of peer-reviewed research surges, scholars increasingly rely on
social platforms for discovery, while authors invest considerable effort in
promoting their work to ensure visibility and citations. To streamline this
process and reduce the reliance on human effort, we introduce Automatic
Promotion (AutoPR), a novel task that transforms research papers into accurate,
engaging, and timely public content. To enable rigorous evaluation, we release
PRBench, a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=multimodal%20benchmark">multimodal benchmark</a> that links 512 peer-reviewed articles to
high-quality promotional posts, assessing systems along three axes: Fidelity
(accuracy and tone), Engagement (audience targeting and appeal), and Alignment
(timing and channel optimization). We also introduce PRAgent, a multi-agent
framework that automates AutoPR in three stages: <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=content%20extraction">content extraction</a> with
multimodal preparation, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=collaborative%20synthesis">collaborative synthesis</a> for polished outputs, and
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=platform-specific%20adaptation">platform-specific adaptation</a> to optimize norms, tone, and tagging for maximum
reach. When compared to direct <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=LLM%20pipelines">LLM pipelines</a> on PRBench, PRAgent demonstrates
substantial improvements, including a 604% increase in total watch time, a 438%
rise in likes, and at least a 2.9x boost in overall engagement. Ablation
studies show that platform modeling and targeted promotion contribute the most
to these gains. Our results position AutoPR as a tractable, measurable research
problem and provide a roadmap for scalable, impactful automated scholarly
communication.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="DAILY" data-date="2025-10-15" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2510.13759" target="_blank" onclick="event.stopPropagation()">Uni-MMMU: A Massive Multi-discipline Multimodal Unified Benchmark</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-10-15</div>
                                        <div class="tags"><span class="tag">DAILY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2510.13759" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/vchitect/Uni-MMMU" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 9</div>
                                    <div class="link-item stars-item">‚≠ê 0</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">Uni-MMMU is a benchmark that evaluates the bidirectional synergy between visual understanding and generation across multiple domains, providing insights into their integration and performance.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Unified%20multimodal%20models">Unified multimodal models</a> aim to jointly enable <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=visual%20understanding">visual understanding</a> and
generation, yet current benchmarks rarely examine their true integration.
Existing evaluations either treat the two abilities in isolation or overlook
tasks that inherently couple them. To address this gap, we present Uni-MMMU, a
comprehensive and discipline-aware benchmark that systematically unfolds the
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=bidirectional%20synergy">bidirectional synergy</a> between generation and understanding across eight
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=reasoning-centric%20domains">reasoning-centric domains</a>, including science, coding, mathematics, and puzzles.
Each task is bidirectionally coupled, demanding models to (i) leverage
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=conceptual%20understanding">conceptual understanding</a> to guide <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=precise%20visual%20synthesis">precise visual synthesis</a>, or (ii) utilize
generation as a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=cognitive%20scaffold">cognitive scaffold</a> for <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=analytical%20reasoning">analytical reasoning</a>. Uni-MMMU
incorporates <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=verifiable%20intermediate%20reasoning%20steps">verifiable intermediate reasoning steps</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=unique%20ground%20truths">unique ground truths</a>, and
a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=reproducible%20scoring%20protocol">reproducible scoring protocol</a> for both textual and visual outputs. Through
extensive evaluation of state-of-the-art unified, generation-only, and
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=understanding-only%20models">understanding-only models</a>, we reveal substantial performance disparities and
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=cross-modal%20dependencies">cross-modal dependencies</a>, offering new insights into when and how these
abilities reinforce one another, and establishing a reliable foundation for
advancing <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=unified%20models">unified models</a>.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="TRENDING" data-date="2025-09-16" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2509.13313" target="_blank" onclick="event.stopPropagation()">ReSum: Unlocking Long-Horizon Search Intelligence via Context
  Summarization</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-09-16</div>
                                        <div class="tags"><span class="tag">TRENDING</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2509.13313" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/Alibaba-NLP/DeepResearch//" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 78</div>
                                    <div class="link-item stars-item">‚≠ê 16k</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">ReSum, a novel paradigm with periodic context summarization, enhances web agents' performance on knowledge-intensive tasks by overcoming context window limitations, achieving significant improvements over ReAct.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Large%20Language%20Model">Large Language Model</a> (<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=LLM">LLM</a>)-based <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=web%20agents">web agents</a> demonstrate strong performance on
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=knowledge-intensive%20tasks">knowledge-intensive tasks</a> but are hindered by <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=context%20window%20limitations">context window limitations</a> in
paradigms like <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=ReAct">ReAct</a>. Complex queries involving multiple entities, intertwined
relationships, and high uncertainty demand extensive search cycles that rapidly
exhaust context budgets before reaching complete solutions. To overcome this
challenge, we introduce <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=ReSum">ReSum</a>, a novel paradigm that enables indefinite
exploration through periodic <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=context%20summarization">context summarization</a>. <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=ReSum">ReSum</a> converts growing
interaction histories into compact <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=reasoning%20states">reasoning states</a>, maintaining awareness of
prior discoveries while bypassing context constraints. For paradigm adaptation,
we propose <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=ReSum-GRPO">ReSum-GRPO</a>, integrating <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=GRPO">GRPO</a> with <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=segmented%20trajectory%20training">segmented trajectory training</a> and
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=advantage%20broadcasting">advantage broadcasting</a> to familiarize agents with summary-conditioned
reasoning. Extensive experiments on <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=web%20agents">web agents</a> of varying scales across three
benchmarks demonstrate that <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=ReSum">ReSum</a> delivers an average absolute improvement of
4.5\% over <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=ReAct">ReAct</a>, with further gains of up to 8.2\% following <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=ReSum-GRPO">ReSum-GRPO</a>
training. Notably, with only 1K training samples, our <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=WebResummer-30B">WebResummer-30B</a> (a
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=ReSum-GRPO">ReSum-GRPO</a>-trained version of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=WebSailor-30B">WebSailor-30B</a>) achieves 33.3\% <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Pass%401">Pass@1</a> on
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=BrowseComp-zh">BrowseComp-zh</a> and 18.3\% on <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=BrowseComp-en">BrowseComp-en</a>, surpassing existing open-source web
agents.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="MONTHLY" data-date="2025-10-09" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2510.08540" target="_blank" onclick="event.stopPropagation()">MM-HELIX: Boosting Multimodal Long-Chain Reflective Reasoning with
  Holistic Platform and Adaptive Hybrid Policy Optimization</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-10-09</div>
                                        <div class="tags"><span class="tag">MONTHLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2510.08540" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/PhoenixZ810/MM-HELIX" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 103</div>
                                    <div class="link-item stars-item">‚≠ê 63</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">Existing Multimodal Large Language Models show performance deficits in long-chain reflective reasoning, which is addressed by developing MM-HELIX-100K and Adaptive Hybrid Policy Optimization, leading to improved accuracy and generalization.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                While current <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Multimodal%20Large%20Language%20Models">Multimodal Large Language Models</a> (MLLMs) have demonstrated
proficiency in reasoning tasks such as mathematics and logic, their capacity
for <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=long-chain%20reflective%20reasoning">long-chain reflective reasoning</a>, a prerequisite for solving complex
real-world problems, remains largely underexplored. In this work, we first
conduct an extensive empirical investigation to evaluate this capability.
Leveraging a carefully designed data synthesis engine, we construct <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=MM-HELIX">MM-HELIX</a>, a
multimodal benchmark consisting 1,260 samples of 42 challenging synthetic tasks
that require iterative thinking and backtracking. Empirical results on this
benchmark reveal that existing MLLMs exhibit significant performance deficits
in <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=long-chain%20reflective%20reasoning">long-chain reflective reasoning</a>. To address this limitation, we generate
post-training data and further explore learning paradigms for exploiting such
data. We first develop the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Step-Elicited%20Response%20Generation">Step-Elicited Response Generation</a> pipeline to create
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=MM-HELIX">MM-HELIX</a>-100K, a large-scale dataset of 100k high-quality, reflective reasoning
traces for instruction-tuning stage. Given that standard Reinforcement Learning
fails on complex tasks due to sparse reward signals and catastrophic forgetting
after Supervised Fine-Tuning, we propose <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Adaptive%20Hybrid%20Policy%20Optimization">Adaptive Hybrid Policy Optimization</a>
(AHPO), a novel training strategy that dynamically unifies <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=offline%20supervision">offline supervision</a>
and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=online%20optimization">online optimization</a> into a single stage. This strategy enables the model to
learn from expert data when rewards are sparse and conduct independent
exploration once proficient. When applied to the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Qwen2.5-VL-7B">Qwen2.5-VL-7B</a> baseline, our
method achieves a +18.6\% accuracy improvement on <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=MM-HELIX">MM-HELIX</a> benchmark and
demonstrates strong generalization with a +5.7\% average performance gain on
general mathematic and logic tasks. Our work demonstrate that reflective
reasoning in MLLMs can be effectively learned and generalized, paving the way
for developing more capable MLLMs.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="WEEKLY" data-date="2025-10-13" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2510.11052" target="_blank" onclick="event.stopPropagation()">Latent Refinement Decoding: Enhancing Diffusion-Based Language Models by
  Refining Belief States</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-10-13</div>
                                        <div class="tags"><span class="tag">WEEKLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2510.11052" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <div class="link-item" style="background: #f5f5f5; color: #999;">No GitHub</div>
                                    <div class="link-item upvote-item">üî• 45</div>
                                    <div class="link-item stars-item">‚≠ê 0</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">Latent Refinement Decoding (LRD) improves parallel sequence generation by maintaining global consistency and iterative refinement, enhancing accuracy and reducing latency.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Autoregressive (AR) models remain the standard for natural language
generation but still suffer from high latency due to strictly sequential
decoding. Recent <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=diffusion-inspired%20approaches">diffusion-inspired approaches</a>, such as <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=LlaDA">LlaDA</a> and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Dream">Dream</a>,
mitigate this by generating in parallel, yet they suffer from two core
limitations: information loss, as predictive distributions for non-finalized
tokens are discarded at each step, and premature commitment, where local
decisions are made without sufficient global coordination. We introduce Latent
Refinement Decoding (LRD), a two-stage framework with <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Latent%20Refinement">Latent Refinement</a> and a
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Predictive%20Feedback%20Loop">Predictive Feedback Loop</a>. The first stage maintains masked positions as
distributional mixtures of predicted tokens and the mask embedding, allowing
the model to establish more globally consistent beliefs. The second stage
progressively finalizes confident tokens while retaining uncertain ones for
iterative feedback. <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=KL-divergence%20dynamics">KL-divergence dynamics</a> provide a principled and reliable
criterion for convergence and early stopping. Experiments across coding
(<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=HumanEval">HumanEval</a> +6.3, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=MBPP">MBPP</a> +2.6) and reasoning (<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=GSM8K">GSM8K</a> +2.9, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=MATH500">MATH500</a> +3.8) show that
LRD improves accuracy while delivering speedups of up to 10.6x, making it a
strong and versatile alternative for parallel sequence generation.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="DAILY" data-date="2025-10-15" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2510.13621" target="_blank" onclick="event.stopPropagation()">The Role of Computing Resources in Publishing Foundation Model Research</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-10-15</div>
                                        <div class="tags"><span class="tag">DAILY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2510.13621" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <div class="link-item" style="background: #f5f5f5; color: #999;">No GitHub</div>
                                    <div class="link-item upvote-item">üî• 9</div>
                                    <div class="link-item stars-item">‚≠ê 0</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">Increased computing resources are correlated with national funding and citations in foundation model research, but not with research environment, domain, or methodology.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Cutting-edge research in Artificial Intelligence (AI) requires considerable
resources, including Graphics Processing Units (GPUs), data, and human
resources. In this paper, we evaluate of the relationship between these
resources and the scientific advancement of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=foundation%20models">foundation models</a> (FM). We reviewed
6517 FM papers published between 2022 to 2024, and surveyed 229 first-authors
to the impact of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=computing%20resources">computing resources</a> on scientific output. We find that
increased computing is correlated with <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=national%20funding">national funding</a> allocations and
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=citations">citations</a>, but our findings don't observe the strong correlations with research
environment (academic or industrial), <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=domain">domain</a>, or <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=study%20methodology">study methodology</a>. We advise
that individuals and institutions focus on creating shared and affordable
computing opportunities to lower the entry barrier for under-resourced
researchers. These steps can help expand participation in FM research, foster
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=diversity%20of%20ideas">diversity of ideas</a> and contributors, and sustain <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=innovation">innovation</a> and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=progress%20in%20AI">progress in AI</a>.
The data will be available at: https://mit-calc.csail.mit.edu/
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="TRENDING" data-date="2025-09-16" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2509.13305" target="_blank" onclick="event.stopPropagation()">WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic
  Data and Scalable Reinforcement Learning</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-09-16</div>
                                        <div class="tags"><span class="tag">TRENDING</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2509.13305" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/Alibaba-NLP/DeepResearch/" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 87</div>
                                    <div class="link-item stars-item">‚≠ê 16k</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">WebSailor, a post-training methodology, enhances open-source models with systematic uncertainty reduction, matching proprietary agents' performance in complex information-seeking tasks.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Transcending human cognitive limitations represents a critical frontier in
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=LLM%20training">LLM training</a>. Proprietary agentic systems like <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=DeepResearch">DeepResearch</a> have demonstrated
superhuman capabilities on extremely complex information-seeking benchmarks
such as <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=BrowseComp">BrowseComp</a>, a feat previously unattainable. We posit that their success
hinges on a sophisticated <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=reasoning%20pattern">reasoning pattern</a> absent in open-source models: the
ability to systematically reduce extreme uncertainty when navigating vast
information landscapes. Based on this insight, we introduce WebSailor, a
complete post-training methodology designed to instill this crucial capability.
Our approach involves generating novel, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=high-uncertainty%20tasks">high-uncertainty tasks</a> through
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=structured%20sampling">structured sampling</a> and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=information%20obfuscation">information obfuscation</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=RFT%20cold%20start">RFT cold start</a>, and an
efficient <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=agentic%20RL%20training">agentic RL training</a> algorithm, Duplicating Sampling Policy
Optimization (<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=DUPO">DUPO</a>). With this integrated pipeline, WebSailor significantly
outperforms all open-source agents in complex information-seeking tasks,
matching proprietary agents' performance and closing the capability gap.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="DAILY" data-date="2025-10-13" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2510.10921" target="_blank" onclick="event.stopPropagation()">FG-CLIP 2: A Bilingual Fine-grained Vision-Language Alignment Model</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-10-13</div>
                                        <div class="tags"><span class="tag">DAILY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2510.10921" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <div class="link-item" style="background: #f5f5f5; color: #999;">No GitHub</div>
                                    <div class="link-item upvote-item">üî• 8</div>
                                    <div class="link-item stars-item">‚≠ê 0</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">FG-CLIP 2, a bilingual vision-language model, enhances fine-grained alignment for English and Chinese through rich supervision and a new TIC loss, achieving state-of-the-art performance across multiple datasets and tasks.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Fine-grained vision-language understanding requires precise alignment between
visual content and linguistic descriptions, a capability that remains limited
in current models, particularly in non-English settings. While models like <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=CLIP">CLIP</a>
perform well on global alignment, they often struggle to capture fine-grained
details in object attributes, spatial relations, and linguistic expressions,
with limited support for bilingual comprehension. To address these challenges,
we introduce <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=FG-CLIP%202">FG-CLIP 2</a>, a bilingual vision-language model designed to advance
fine-grained alignment for both English and Chinese. Our approach leverages
rich <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=fine-grained%20supervision">fine-grained supervision</a>, including <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=region-text%20matching">region-text matching</a> and long-caption
modeling, alongside <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=multiple%20discriminative%20objectives">multiple discriminative objectives</a>. We further introduce
the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Textual%20Intra-modal%20Contrastive%20(TIC)%20loss">Textual Intra-modal Contrastive (TIC) loss</a> to better distinguish
semantically similar captions. Trained on a carefully curated mixture of
large-scale English and Chinese data, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=FG-CLIP%202">FG-CLIP 2</a> achieves powerful bilingual
performance. To enable rigorous evaluation, we present a new benchmark for
Chinese multimodal understanding, featuring <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=long-caption%20retrieval">long-caption retrieval</a> and bounding
box classification. Extensive experiments on 29 datasets across 8 tasks show
that <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=FG-CLIP%202">FG-CLIP 2</a> outperforms existing methods, achieving state-of-the-art results
in both languages. We release the model, code, and benchmark to facilitate
future research on bilingual fine-grained alignment.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="WEEKLY" data-date="2025-10-06" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2510.04533" target="_blank" onclick="event.stopPropagation()">TAG:Tangential Amplifying Guidance for Hallucination-Resistant Diffusion
  Sampling</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-10-06</div>
                                        <div class="tags"><span class="tag">WEEKLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2510.04533" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/hyeon-cho/Tangential-Amplifying-Guidance" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 45</div>
                                    <div class="link-item stars-item">‚≠ê 7</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">Tangential Amplifying Guidance (TAG) improves diffusion model sample quality by directly amplifying tangential components of estimated scores without modifying the model architecture.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Recent <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=diffusion%20models">diffusion models</a> achieve the state-of-the-art performance in image
generation, but often suffer from <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=semantic%20inconsistencies">semantic inconsistencies</a> or <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=hallucinations">hallucinations</a>.
While various <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=inference-time%20guidance">inference-time guidance</a> methods can enhance generation, they
often operate indirectly by relying on external signals or architectural
modifications, which introduces additional computational overhead. In this
paper, we propose Tangential Amplifying Guidance (TAG), a more efficient and
direct guidance method that operates solely on <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=trajectory%20signals">trajectory signals</a> without
modifying the underlying diffusion model. TAG leverages an <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=intermediate%20sample">intermediate sample</a>
as a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=projection%20basis">projection basis</a> and amplifies the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=tangential%20components">tangential components</a> of the estimated
scores with respect to this basis to correct the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=sampling%20trajectory">sampling trajectory</a>. We
formalize this guidance process by leveraging a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=first-order%20Taylor%20expansion">first-order Taylor expansion</a>,
which demonstrates that amplifying the tangential component steers the state
toward <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=higher-probability%20regions">higher-probability regions</a>, thereby reducing inconsistencies and
enhancing sample quality. TAG is a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=plug-and-play">plug-and-play</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=architecture-agnostic">architecture-agnostic</a> module
that improves diffusion sampling fidelity with minimal computational addition,
offering a new perspective on diffusion guidance.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="TRENDING" data-date="2025-09-16" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2509.13310" target="_blank" onclick="event.stopPropagation()">Scaling Agents via Continual Pre-training</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-09-16</div>
                                        <div class="tags"><span class="tag">TRENDING</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2509.13310" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/Alibaba-NLP/DeepResearch///" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 112</div>
                                    <div class="link-item stars-item">‚≠ê 16k</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">AgentFounder, a deep research agent model incorporating Agentic Continual Pre-training, achieves state-of-the-art performance in agentic tasks while maintaining strong tool-use ability.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Large%20language%20models">Large language models</a> (LLMs) have evolved into <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=agentic%20systems">agentic systems</a> capable of
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=autonomous%20tool%20use">autonomous tool use</a> and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=multi-step%20reasoning">multi-step reasoning</a> for complex problem-solving.
However, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=post-training%20approaches">post-training approaches</a> building upon general-purpose foundation
models consistently underperform in agentic tasks, particularly in open-source
implementations. We identify the root cause: the absence of robust agentic
foundation models forces models during post-training to simultaneously learn
diverse agentic behaviors while aligning them to expert demonstrations, thereby
creating fundamental optimization tensions. To this end, we are the first to
propose incorporating <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Agentic%20Continual%20Pre-training">Agentic Continual Pre-training</a> (Agentic CPT) into the
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=deep%20research%20agents">deep research agents</a> training pipeline to build powerful agentic foundational
models. Based on this approach, we develop a deep research agent model named
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=AgentFounder">AgentFounder</a>. We evaluate our <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=AgentFounder">AgentFounder</a>-30B on 10 benchmarks and achieve
state-of-the-art performance while retains strong tool-use ability, notably
39.9% on <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=BrowseComp-en">BrowseComp-en</a>, 43.3% on <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=BrowseComp-zh">BrowseComp-zh</a>, and 31.5% Pass@1 on <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=HLE">HLE</a>.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="DAILY" data-date="2025-10-15" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2510.13809" target="_blank" onclick="event.stopPropagation()">PhysMaster: Mastering Physical Representation for Video Generation via
  Reinforcement Learning</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-10-15</div>
                                        <div class="tags"><span class="tag">DAILY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2510.13809" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <div class="link-item" style="background: #f5f5f5; color: #999;">No GitHub</div>
                                    <div class="link-item upvote-item">üî• 7</div>
                                    <div class="link-item stars-item">‚≠ê 0</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">PhysMaster enhances video generation by integrating physical knowledge through PhysEncoder, using reinforcement learning and Direct Preference Optimization to improve physics-awareness.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Video generation models nowadays are capable of generating visually realistic
videos, but often fail to adhere to physical laws, limiting their ability to
generate physically plausible videos and serve as ''world models''. To address
this issue, we propose <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=PhysMaster">PhysMaster</a>, which captures physical knowledge as a
representation for guiding video generation models to enhance their
physics-awareness. Specifically, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=PhysMaster">PhysMaster</a> is based on the image-to-video task
where the model is expected to predict physically plausible dynamics from the
input image. Since the input image provides physical priors like relative
positions and potential interactions of objects in the scenario, we devise
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=PhysEncoder">PhysEncoder</a> to encode physical information from it as an extra condition to
inject physical knowledge into the video generation process. The lack of proper
supervision on the model's physical performance beyond mere appearance
motivates <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=PhysEncoder">PhysEncoder</a> to apply <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=reinforcement%20learning">reinforcement learning</a> with human feedback to
physical representation learning, which leverages feedback from generation
models to optimize physical representations with <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Direct%20Preference%20Optimization">Direct Preference Optimization</a>
(DPO) in an end-to-end manner. <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=PhysMaster">PhysMaster</a> provides a feasible solution for
improving physics-awareness of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=PhysEncoder">PhysEncoder</a> and thus of video generation,
proving its ability on a simple proxy task and generalizability to wide-ranging
physical scenarios. This implies that our <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=PhysMaster">PhysMaster</a>, which unifies solutions
for various physical processes via representation learning in the reinforcement
learning paradigm, can act as a generic and plug-in solution for physics-aware
video generation and broader applications.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="WEEKLY" data-date="2025-10-09" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2510.08759" target="_blank" onclick="event.stopPropagation()">BEAR: Benchmarking and Enhancing Multimodal Language Models for Atomic
  Embodied Capabilities</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-10-09</div>
                                        <div class="tags"><span class="tag">WEEKLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2510.08759" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/yqi19/BEAR-official" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 43</div>
                                    <div class="link-item stars-item">‚≠ê 16</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">BEAR is a comprehensive benchmark evaluating multimodal large language models' embodied capabilities, and BEAR-Agent enhances these models by integrating pretrained vision models, improving performance across various tasks.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Embodied%20capabilities">Embodied capabilities</a> refer to a suite of fundamental abilities for an agent
to perceive, comprehend, and interact with the physical world. While multimodal
large language models (MLLMs) show promise as embodied agents, a thorough and
systematic evaluation of their <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=embodied%20capabilities">embodied capabilities</a> remains underexplored, as
existing benchmarks primarily focus on specific domains such as planning or
spatial understanding. To bridge this gap, we introduce <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=BEAR">BEAR</a>, a comprehensive
and fine-grained benchmark that evaluates MLLMs on atomic embodied
capabilities. <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=BEAR">BEAR</a> comprises 4,469 interleaved <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=image-video-text%20entries">image-video-text entries</a> across
14 domains in 6 categories, including tasks from low-level pointing, trajectory
understanding, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=spatial%20reasoning">spatial reasoning</a>, to high-level planning. Extensive evaluation
results of 20 representative MLLMs reveal their persistent limitations across
all domains of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=embodied%20capabilities">embodied capabilities</a>. To tackle the shortfall, we propose
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=BEAR-Agent">BEAR-Agent</a>, a multimodal conversable agent that integrates pretrained vision
models to strengthen MLLM perception, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=3D%20understanding">3D understanding</a>, and planning
capabilities. It substantially enhances MLLM performance across diverse
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=embodied%20capabilities">embodied capabilities</a> on <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=BEAR">BEAR</a>, yielding a 9.12% absolute gain and a relative
improvement of 17.5% on GPT-5. Furthermore, our experiments indicate that
improving MLLM <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=embodied%20capabilities">embodied capabilities</a> can benefit embodied tasks in simulated
environments. Project website: https://<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=bear">bear</a>-official66.github.io/
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="WEEKLY" data-date="2025-10-12" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2510.10689" target="_blank" onclick="event.stopPropagation()">OmniVideoBench: Towards Audio-Visual Understanding Evaluation for Omni
  MLLMs</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-10-12</div>
                                        <div class="tags"><span class="tag">WEEKLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2510.10689" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/NJU-LINK/OmniVideoBench" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 42</div>
                                    <div class="link-item stars-item">‚≠ê 25</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">OmniVideoBench is a comprehensive benchmark for evaluating audio-visual reasoning in multimodal large language models, addressing modality complementarity and logical consistency.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Recent advances in <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=multimodal%20large%20language%20models">multimodal large language models</a> (MLLMs) have demonstrated
substantial potential in <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=video%20understanding">video understanding</a>. However, existing benchmarks fail
to comprehensively evaluate synergistic reasoning capabilities across audio and
visual modalities, often neglecting either one of the modalities or integrating
them in a logically inconsistent manner. To bridge this gap, we introduce
OmniVideoBench, a large-scale and rigorously designed benchmark dedicated to
assessing synergistic <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=audio-visual%20understanding">audio-visual understanding</a>, with a strong emphasis on
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=modality%20complementarity">modality complementarity</a> and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=logical%20consistency">logical consistency</a>. Specifically, OmniVideoBench
comprises 1000 high-quality question-answer(QA) pairs, each annotated with
step-by-step <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=reasoning%20traces">reasoning traces</a>, derived from 628 diverse videos ranging from
several seconds to 30 minutes, and manually verified to guarantee complete
correctness and uniqueness. Moreover, OmniVideoBench encompasses 13 carefully
designed question types, covering <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=temporal%20reasoning">temporal reasoning</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=spatial%20localization">spatial localization</a>,
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=counting">counting</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=causal%20inference">causal inference</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=summarization">summarization</a>, and beyond, thereby capturing the
essential challenges of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=video%20understanding">video understanding</a>. Evaluation of multiple MLLMs on
OmniVideoBench reveals a pronounced gap between model performance and human
reasoning, with open-source models lagging significantly behind their
closed-source counterparts, underscoring the inherent difficulty of genuine
audio-visual reasoning. We will release OmniVideoBench to foster the
development of MLLMs with stronger and more generalizable reasoning
capabilities.
                            </div>
                        </div>
                    </div>
                </div>
                </div>
                        </div>
                    </section>
                </div>
            </div>
        </main>

        <!-- Footer -->
        <footer class="footer">
            <div class="container">
                <p>&copy; 2025 Daily Digest. Curated with ‚ù§Ô∏è for the curious minds.</p>
            </div>
        </footer>
        <script src="static/script.js"></script>
    </body>
    </html>
    