
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <title>Daily Digest</title>
        <link rel="stylesheet" href="static/style.css">
    </head>
    <body>
        <!-- Header -->
        <header class="header">
            <div class="container">
                <nav class="nav">
                    <div class="logo">üì∞ Daily Digest</div>
                    <ul class="nav-links">
                        <li><a href="#astronomy">Astronomy</a></li>
                        <li><a href="#earth">Earth</a></li>
                        <li><a href="#tarot">Tarot</a></li>
                        <li><a href="#tech">Tech News</a></li>
                        <li><a href="#papers">Papers</a></li>
                    </ul>
                    <div class="date-badge">Fri, Oct 24, 2025</div>
                </nav>
            </div>
        </header>

        <!-- Main Content -->
        <main class="main-content">
            <div class="container">
                <!-- Hero Section -->
                <section class="hero">
                    <h1>Your Daily Tech & Science Digest</h1>
                    <p>Stay updated with the latest in astronomy, earth sciences, technology news, and cutting-edge research papers</p>
                </section>

                <!-- Content Grid -->
                <div class="content-grid">
                    <!-- APOD, Earth Observatory, and Tarot Row -->
                    <div class="three-column">
                        <!-- APOD Section -->
                        <section id="astronomy" class="card apod-card">
                            <div class="card-media" id="apod-media"></div>
                            <div class="card-content-wrapper">
                                <div class="card-header">
                                    <div class="card-icon">‚ú®</div>
                                    <div>
                                        <h2 class="card-title">Astronomy Picture</h2>
                                        <p class="card-subtitle">NASA's daily cosmic wonder</p>
                                    </div>
                                </div>
                                <div class="card-content" id="apod-content">
                                    
        <div style='display: flex; flex-direction: column; gap: 20px;'>
            <div style='display: none;'>
                <img src="https://apod.nasa.gov/apod/image/2510/WitchBroom_Meyers_1080.jpg" alt="APOD" class="apod-media-element">
            </div>
            <div>
                <h3>No Title Found</h3>
                <p><b> Explanation: </b> 
Ten thousand years ago, before the dawn of recorded human history, 
a new light would suddenly have appeared in the 
night sky and faded after a few weeks.  

Today we know this light was from a
<a href="http://chandra.harvard.edu/xray_sources/supernovas.html">supernova,
or exploding star</a>,
and record the expanding debris cloud as the 
<a href="https://apod.nasa.gov/apod/ap250602.html">Veil Nebula</a>, a 
<a href="http://en.wikipedia.org/wiki/List_of_supernova_remnants">supernova remnant</a>.  

This sharp telescopic view is centered on a
<a href="http://en.wikipedia.org/wiki/File:Cygnus_Loop_Labeled.png">western
segment</a> of the 
Veil Nebula cataloged as 
<a href="http://en.wikipedia.org/wiki/New_General_Catalog">NGC</a> 6960 but less formally known as the Witch's Broom Nebula.  

Blasted out in the cataclysmic explosion, an 
<a href="https://science.nasa.gov/solar-system/10-things-going-interstellar/" {="">interstellar</a> shock wave plows
through space sweeping up and exciting interstellar material.

Imaged with narrow band filters, the 
<a href="https://apod.nasa.gov/apod/ap170919.html">glowing filaments</a> are
like long ripples in a sheet seen almost edge on,
remarkably well separated into atomic hydrogen (red)
and oxygen (blue-green) gas.

The complete supernova remnant lies about 1400
<a href="https://spaceplace.nasa.gov/light-year/">light-years</a> 
away towards the
<a href="https://apod.nasa.gov/apod/ap101119.html">constellation Cygnus</a>.

This Witch's <a href="https://i.ytimg.com/vi/D3tZOUeTeRU/maxresdefault.jpg">Broom</a> actually spans about 35 light-years.

The bright star in the frame is 
<a href="http://stars.astro.illinois.edu/sow/52cyg.html">52 Cygni</a>,
visible with the unaided eye from 
a dark location but unrelated to the ancient supernova remnant.

<br><b> Tomorrow's picture: </b>the shadowy realm</p>
            </div>
        </div>
        
                                </div>
                            </div>
                        </section>

                        <!-- Earth Observatory -->
                        <section id="earth" class="card eo-card">
                            <div class="card-media" id="eo-media"></div>
                            <div class="card-content-wrapper">
                                <div class="card-header">
                                    <div class="card-icon">üåç</div>
                                    <div>
                                        <h2 class="card-title">Earth Observatory</h2>
                                        <p class="card-subtitle">Our planet from above</p>
                                    </div>
                                </div>
                                <div class="card-content" id="eo-content">
                                    
        <div style='display: flex; flex-direction: column; gap: 20px;'>
            <div style='display: none;'>
                <img src='https://eoimages.gsfc.nasa.gov/images/imagerecords/154000/154848/tarimbasinzm_oli2_20250911_th.jpg' alt='Earth Observatory' class='eo-media-element'>
            </div>
            <div>
                <h3>A Desert Intersection</h3>
                <p>A colorful ridge and winding glacial meltwater river meet amidst dune fields in western China. <a href='https://earthobservatory.nasa.gov/images/154848/a-desert-intersection' target='_blank'>[Read more]</a></p>
                <div style='background: #f0f8f0; padding: 15px; border-radius: 8px; border-left: 4px solid #2d5016; margin-top: 20px;'>
                    <p>
                        <strong>ü§ñ AI Summary:</strong> Mazartagh ridge in China's Tarim Basin spans 145 kilometers, rises 200 meters above the desert, and acts as a sand barrier blocking 62% of dune movement from north to south. Its northern side features iron-rich sandstones deposited in arid environments, while the southern side consists of gypsum and calcareous sandstones from shallow water settings. The seasonal Hotan River, fed by Kunlun Mountain glacial melt, is the only major waterway sustaining flow across the Takla Makan Desert, supporting vegetation despite saline soils. The ridge's obstruction creates distinct dune patterns: compound and star dunes accumulate north of it, while linear dunes dominate southward, with localized barchan dune fields forming through ridge notches. The river transports nephrite jade, historically collected for Silk Road trade, near an eighth-century Tibetan Empire fort site.
                    </p>
                </div>
            </div>
        </div>
        
                                </div>
                            </div>
                        </section>

                        <!-- Tarot Section -->
                        <section id="tarot" class="card tarot-card">
                            <div class="card-media" id="tarot-media"></div>
                            <div class="card-content-wrapper">
                                <div class="card-header">
                                    <div class="card-icon">üîÆ</div>
                                    <div>
                                        <h2 class="card-title">Daily Tarot</h2>
                                        <p class="card-subtitle">Your mystical guidance</p>
                                    </div>
                                </div>
                                <div class="card-content" id="tarot-content">
                                    
        <div style='display: flex; flex-direction: column; gap: 20px;'>
            <div style='display: none;'>
                <div class='tarot-card-container tarot-media-element'>
                    <div class='tarot-card' onclick='this.style.transform = this.style.transform.includes("rotateY(180deg)") ? "rotateY(0deg)" : "rotateY(180deg)"'>
                        <div>
                            <div style='font-size: 2.5rem; color: #d4af37; text-align: center; line-height: 1.2;'>
                                üîÆ<br>
                                <span style='font-size: 0.8rem; letter-spacing: 2px; font-weight: normal;'>DAILY TAROT</span><br>
                                <span style='font-size: 0.6rem; opacity: 0.8;'>Click to Reveal</span>
                            </div>
                        </div>
                        <div>
                            <img src='https://raw.githubusercontent.com/Haus226/daily-email/refs/heads/main/tarot_cards/Pictorial_Key_to_the_Tarot_Wands_02.jpg' alt='Two of Wands' />
                        </div>
                    </div>
                </div>
            </div>
            <div>
                <h3>Two of Wands</h3>                
                <div style='background: linear-gradient(135deg, #ffeaa7, #fdcb6e); padding: 5px; border-radius: 12px; border-left: 4px solid #e17055; margin-bottom: 20px; box-shadow: 0 4px 15px rgba(0,0,0,0.1);'>
                    <div style='font-weight: bold; color: #2d3436; margin-bottom: 5px; font-size: 1.2rem; text-transform: uppercase; letter-spacing: 1px;'>‚ú® Core Meaning</div>
                    <p style='margin: 0; color: #2d3436; line-height: 1.5; font-size: 1.0rem;'>Between the alternative readings there is no marriage possible; on the one hand, riches, fortune, magnificence; on the other, physical suffering, disease, chagrin, sadness, mortification. The design gives one suggestion; here is a lord overlooking his dominion and alternately contemplating a globe; it looks like the malady, the mortification, the sadness of Alexander amidst the grandeur of this world's wealth.</p>
                </div>
                <div style='background: linear-gradient(135deg, #ddd6fe, #c4b5fd); padding: 5px; border-radius: 12px; border-left: 4px solid #8b5cf6; box-shadow: 0 4px 15px rgba(0,0,0,0.1);'>
                    <div style='font-weight: bold; color: #2d3436; margin-bottom: 5px; font-size: 1.4rem; text-transform: uppercase; letter-spacing: 1px;'>üîç Daily Guidance</div>
                    <p style='margin: 0; color: #2d3436; line-height: 1.6; font-size: 1.3rem; font-weight: 500;'>"Embrace your power and vision, dear one. Your future holds greatness, not sorrow. Trust in your ability to navigate your domain with wisdom and compassion."</p>
                </div>
    
            </div>
        </div>
        
                                </div>
                            </div>
                        </section>
                        
                        <!-- Navigation Arrows -->
                        <button class="card-nav prev" onclick="prevCard()" aria-label="Previous card">‚Äπ</button>
                        <button class="card-nav next" onclick="nextCard()" aria-label="Next card">‚Ä∫</button>
                    </div>

                    <!-- Hacker News Section -->
                    <section id="tech" class="card hn-card featured-section">
                        <div class="card-header">
                            <div class="card-icon">üî•</div>
                            <div>
                                <h2 class="card-title">Hacker News Top 10</h2>
                                <p class="card-subtitle">What's trending in tech</p>
                            </div>
                        </div>
                        <div class="card-content">
                            <ol class='hn-list'><li><a href='https://americanhistory.si.edu/explore/stories/betty-white-world-war-ii' target='_blank'>Betty White's shoulder bag is a time capsule of World War II</a></li><li><a href='https://www.anthropic.com/news/memory' target='_blank'>Claude Memory</a></li><li><a href='https://info.varnish-software.com/blog/how-memory-maps-mmap-deliver-25x-faster-file-access-in-go' target='_blank'>How memory maps (mmap) deliver faster file access in Go</a></li><li><a href='https://jyu.dev/blog/why-dev-null-is-an-acid-compliant-database/' target='_blank'>/dev/null is an ACID compliant database</a></li><li><a href='https://adrs-ucb.notion.site/moe-load-balancing' target='_blank'>AI discovers a 5x faster MoE load balancing algorithm than human experts</a></li><li><a href='https://github.com/xyflow/xyflow' target='_blank'>React Flow, open source libraries for node-based UIs with React or Svelte</a></li><li><a href='https://www.volts.wtf/p/can-second-life-ev-batteries-work' target='_blank'>Can ‚Äúsecond life‚Äù EV batteries work as grid-scale energy storage?</a></li><li><a href='https://joeyh.name/blog/entry/cheap_DIY_solar_fence_design/' target='_blank'>Cheap DIY solar fence design</a></li><li><a href='https://www.henrikkarlsson.xyz/p/wordless-thought' target='_blank'>When is it better to think without words?</a></li><li><a href='https://notes.xeome.dev/notes/Zram' target='_blank'>Zram Performance Analysis</a></li></ol>
                        </div>
                    </section>

                    <!-- Hugging Face Papers -->
                    <section id="papers" class="card hf-card-container featured-section">
                        <div class="card-header">
                            <div class="card-icon">üìö</div>
                            <div>
                                <h2 class="card-title">Latest Research Papers</h2>
                                <p class="card-subtitle">Cutting-edge AI & ML research</p>
                            </div>
                        </div>
                        
                        <!-- Paper Filters -->
                        <div class="paper-filters" id="hf-filters">
                            <button class="filter-btn active" data-tag="ALL" onclick="filterPapers('ALL')">All Papers</button>
                            <button class="filter-btn" data-tag="DAILY" onclick="filterPapers('DAILY')">Daily</button>
                            <button class="filter-btn" data-tag="WEEKLY" onclick="filterPapers('WEEKLY')">Weekly</button>
                            <button class="filter-btn" data-tag="MONTHLY" onclick="filterPapers('MONTHLY')">Monthly</button>
                            <button class="filter-btn" data-tag="TRENDING" onclick="filterPapers('TRENDING')">Trending</button>
                        </div>

                        <!-- Papers Content -->
                        <div class="card-content">
                            
        <div id="hf-grid" class="papers-grid">
        
                <div class="paper-card hf-card flip-card" data-tags="DAILY" data-date="2025-10-23" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2510.20187" target="_blank" onclick="event.stopPropagation()">Every Question Has Its Own Value: Reinforcement Learning with Explicit
  Human Values</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-10-23</div>
                                        <div class="tags"><span class="tag">DAILY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2510.20187" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <div class="link-item" style="background: #f5f5f5; color: #999;">No GitHub</div>
                                    <div class="link-item upvote-item">üî• 2</div>
                                    <div class="link-item stars-item">‚≠ê 0</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">RLEV, a reinforcement learning method, aligns LLM optimization with human value signals, improving value-weighted accuracy and learning value-sensitive termination policies.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                We propose <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Reinforcement%20Learning%20with%20Explicit%20Human%20Values%20(RLEV)">Reinforcement Learning with Explicit Human Values (RLEV)</a>, a method
that aligns <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Large%20Language%20Model%20(LLM)">Large Language Model (LLM)</a> optimization directly with quantifiable
human <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=value%20signals">value signals</a>. While Reinforcement Learning with Verifiable Rewards
(RLVR) effectively trains models in objective domains using binary correctness
rewards, it overlooks that not all tasks are equally significant. RLEV extends
this framework by incorporating human-defined <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=value%20signals">value signals</a> directly into the
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=reward%20function">reward function</a>. Using <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=exam-style%20data">exam-style data</a> with explicit ground-truth value labels,
RLEV consistently outperforms correctness-only baselines across multiple RL
algorithms and model scales. Crucially, RLEV policies not only improve
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=value-weighted%20accuracy">value-weighted accuracy</a> but also learn a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=value-sensitive%20termination%20policy">value-sensitive termination policy</a>:
concise for low-value prompts, thorough for high-value ones. We demonstrate
this behavior stems from value-weighted <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=gradient%20amplification">gradient amplification</a> on
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=end-of-sequence%20tokens">end-of-sequence tokens</a>. <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Ablation%20studies">Ablation studies</a> confirm the gain is causally linked to
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=value%20alignment">value alignment</a>. RLEV remains robust under <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=noisy%20value%20signals">noisy value signals</a>, such as
difficulty-based labels, demonstrating that optimizing for an explicit utility
function offers a practical path to aligning LLMs with human priorities.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="DAILY" data-date="2025-10-23" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2510.20822" target="_blank" onclick="event.stopPropagation()">HoloCine: Holistic Generation of Cinematic Multi-Shot Long Video
  Narratives</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-10-23</div>
                                        <div class="tags"><span class="tag">DAILY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2510.20822" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <div class="link-item" style="background: #f5f5f5; color: #999;">No GitHub</div>
                                    <div class="link-item upvote-item">üî• 1</div>
                                    <div class="link-item stars-item">‚≠ê 0</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">HoloCine generates coherent multi-shot narratives using a Window Cross-Attention mechanism and Sparse Inter-Shot Self-Attention, enabling end-to-end cinematic creation.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                State-of-the-art <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=text-to-video%20models">text-to-video models</a> excel at generating isolated clips but
fall short of creating the coherent, multi-shot narratives, which are the
essence of storytelling. We bridge this "narrative gap" with HoloCine, a model
that generates entire scenes holistically to ensure global consistency from the
first shot to the last. Our architecture achieves precise directorial control
through a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Window%20Cross-Attention">Window Cross-Attention</a> mechanism that localizes text prompts to
specific shots, while a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Sparse%20Inter-Shot%20Self-Attention">Sparse Inter-Shot Self-Attention</a> pattern (dense within
shots but sparse between them) ensures the efficiency required for minute-scale
generation. Beyond setting a new state-of-the-art in <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=narrative%20coherence">narrative coherence</a>,
HoloCine develops remarkable emergent abilities: a persistent memory for
characters and scenes, and an intuitive grasp of cinematic techniques. Our work
marks a pivotal shift from clip synthesis towards <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=automated%20filmmaking">automated filmmaking</a>, making
end-to-end cinematic creation a tangible future. Our code is available at:
https://holo-cine.github.io/.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="TRENDING" data-date="2025-10-16" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2510.14528" target="_blank" onclick="event.stopPropagation()">PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B
  Ultra-Compact Vision-Language Model</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-10-16</div>
                                        <div class="tags"><span class="tag">TRENDING</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2510.14528" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/PaddlePaddle/PaddleOCR" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 62</div>
                                    <div class="link-item stars-item">‚≠ê 60.7k</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">PaddleOCR-VL, a vision-language model combining NaViT-style visual encoder and ERNIE-4.5 language model, achieves state-of-the-art performance in document parsing with minimal resource consumption.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                In this report, we propose PaddleOCR-VL, a SOTA and resource-efficient model
tailored for document parsing. Its core component is PaddleOCR-VL-0.9B, a
compact yet powerful <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=vision-language%20model">vision-language model</a> (VLM) that integrates a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=NaViT-style">NaViT-style</a>
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=dynamic%20resolution%20visual%20encoder">dynamic resolution visual encoder</a> with the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=ERNIE-4.5">ERNIE-4.5</a>-0.3B language model to
enable accurate <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=element%20recognition">element recognition</a>. This innovative model efficiently supports
109 languages and excels in recognizing complex elements (e.g., text, tables,
formulas, and charts), while maintaining minimal resource consumption. Through
comprehensive evaluations on widely used public benchmarks and in-house
benchmarks, PaddleOCR-VL achieves SOTA performance in both page-level document
parsing and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=element-level%20recognition">element-level recognition</a>. It significantly outperforms existing
solutions, exhibits strong competitiveness against top-tier VLMs, and delivers
fast <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=inference%20speeds">inference speeds</a>. These strengths make it highly suitable for practical
deployment in real-world scenarios.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="DAILY" data-date="2025-10-23" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2510.20820" target="_blank" onclick="event.stopPropagation()">LayerComposer: Interactive Personalized T2I via Spatially-Aware Layered
  Canvas</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-10-23</div>
                                        <div class="tags"><span class="tag">DAILY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2510.20820" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <div class="link-item" style="background: #f5f5f5; color: #999;">No GitHub</div>
                                    <div class="link-item upvote-item">üî• 1</div>
                                    <div class="link-item stars-item">‚≠ê 0</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">LayerComposer provides interactive control over spatial composition and scalability in multi-subject text-to-image generation through a layered canvas and locking mechanism.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Despite their impressive visual fidelity, existing personalized generative
models lack interactive control over spatial composition and scale poorly to
multiple subjects. To address these limitations, we present LayerComposer, an
interactive framework for personalized, multi-subject <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=text-to-image%20generation">text-to-image generation</a>.
Our approach introduces two main contributions: (1) a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=layered%20canvas">layered canvas</a>, a novel
representation in which each subject is placed on a distinct layer, enabling
occlusion-free composition; and (2) a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=locking%20mechanism">locking mechanism</a> that preserves selected
layers with high fidelity while allowing the remaining layers to adapt flexibly
to the surrounding context. Similar to professional image-editing software, the
proposed <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=layered%20canvas">layered canvas</a> allows users to place, resize, or lock input subjects
through intuitive layer manipulation. Our versatile <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=locking%20mechanism">locking mechanism</a> requires
no architectural changes, relying instead on inherent <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=positional%20embeddings">positional embeddings</a>
combined with a new complementary <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=data%20sampling%20strategy">data sampling strategy</a>. Extensive experiments
demonstrate that LayerComposer achieves superior <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=spatial%20control">spatial control</a> and identity
preservation compared to the state-of-the-art methods in multi-subject
personalized image generation.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="WEEKLY MONTHLY" data-date="2025-10-17" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2510.15444" target="_blank" onclick="event.stopPropagation()">A Theoretical Study on Bridging Internal Probability and
  Self-Consistency for LLM Reasoning</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-10-17</div>
                                        <div class="tags"><span class="tag">MONTHLY</span><span class="tag">WEEKLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2510.15444" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <div class="link-item" style="background: #f5f5f5; color: #999;">No GitHub</div>
                                    <div class="link-item upvote-item">üî• 133</div>
                                    <div class="link-item stars-item">‚≠ê 0</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">A theoretical framework for sampling-based test-time scaling in large language models reveals limitations in self-consistency and perplexity, and introduces RPC to improve reasoning performance and reduce sampling costs.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Test-time scaling seeks to improve the reasoning performance of large
language models (LLMs) by adding computational resources. A prevalent approach
within the field is <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=sampling-based%20test-time%20scaling">sampling-based test-time scaling</a> methods, which enhance
reasoning by generating multiple reasoning paths for a given input during
inference. However, despite its practical success, the theoretical foundations
remain underexplored. In this paper, we provide the first theoretical framework
for analyzing <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=sampling-based%20test-time%20scaling">sampling-based test-time scaling</a> methods, grounded in the
perspective of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=confidence%20estimation">confidence estimation</a>. Based on the framework, we analyze two
dominant paradigms: <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=self-consistency">self-consistency</a> and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=perplexity">perplexity</a>, and reveal key
limitations: <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=self-consistency">self-consistency</a> suffers from high estimation error while
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=perplexity">perplexity</a> exhibits substantial modeling error and possible degradation of the
estimation error convergence. To address these limitations, we introduce RPC, a
hybrid method that leverages our theoretical insights through two key
components: <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Perplexity%20Consistency">Perplexity Consistency</a> and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Reasoning%20Pruning">Reasoning Pruning</a>. <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Perplexity">Perplexity</a>
Consistency combines the strengths of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=self-consistency">self-consistency</a> and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=perplexity">perplexity</a>, boosting
the convergence rate of estimation error from linear to exponential while
preserving model error. <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Reasoning%20Pruning">Reasoning Pruning</a> prevents degradation by eliminating
low-probability reasoning paths. Both theoretical analysis and empirical
results across seven <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=benchmark%20datasets">benchmark datasets</a> demonstrate that RPC has a strong
potential for reducing <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=reasoning%20error">reasoning error</a>. Notably, RPC achieves reasoning
performance comparable to <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=self-consistency">self-consistency</a> while not only enhancing confidence
reliability but also reducing sampling costs by 50%. The code and resources are
available at https://wnjxyk.github.io/RPC.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="DAILY" data-date="2025-10-23" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2510.20771" target="_blank" onclick="event.stopPropagation()">AlphaFlow: Understanding and Improving MeanFlow Models</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-10-23</div>
                                        <div class="tags"><span class="tag">DAILY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2510.20771" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <div class="link-item" style="background: #f5f5f5; color: #999;">No GitHub</div>
                                    <div class="link-item upvote-item">üî• 1</div>
                                    <div class="link-item stars-item">‚≠ê 0</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">The $\alpha$-Flow framework improves few-step generative modeling by unifying and disentangling conflicting objectives, leading to better convergence and state-of-the-art performance on ImageNet-1K.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=MeanFlow">MeanFlow</a> has recently emerged as a powerful framework for few-step generative
modeling trained from scratch, but its success is not yet fully understood. In
this work, we show that the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=MeanFlow">MeanFlow</a> objective naturally decomposes into two
parts: <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=trajectory%20flow%20matching">trajectory flow matching</a> and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=trajectory%20consistency">trajectory consistency</a>. Through gradient
analysis, we find that these terms are strongly negatively correlated, causing
optimization conflict and slow convergence. Motivated by these insights, we
introduce alpha-Flow, a broad family of objectives that unifies trajectory
flow matching, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Shortcut%20Model">Shortcut Model</a>, and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=MeanFlow">MeanFlow</a> under one formulation. By adopting
a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=curriculum%20strategy">curriculum strategy</a> that smoothly anneals from <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=trajectory%20flow%20matching">trajectory flow matching</a> to
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=MeanFlow">MeanFlow</a>, alpha-Flow disentangles the conflicting objectives, and achieves
better convergence. When trained from scratch on <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=class-conditional">class-conditional</a> <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=ImageNet-1K">ImageNet-1K</a>
256x256 with vanilla <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=DiT%20backbones">DiT backbones</a>, alpha-Flow consistently outperforms
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=MeanFlow">MeanFlow</a> across scales and settings. Our largest alpha-Flow-XL/2+ model
achieves new state-of-the-art results using vanilla <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=DiT%20backbones">DiT backbones</a>, with FID
scores of 2.58 (1-NFE) and 2.15 (2-NFE).
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="MONTHLY" data-date="2025-09-30" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2509.26507" target="_blank" onclick="event.stopPropagation()">The Dragon Hatchling: The Missing Link between the Transformer and
  Models of the Brain</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-09-30</div>
                                        <div class="tags"><span class="tag">MONTHLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2509.26507" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/pathwaycom/bdh" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 503</div>
                                    <div class="link-item stars-item">‚≠ê 3.18k</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">BDH, a biologically inspired Large Language Model, combines scale-free network architecture with Hebbian learning to achieve Transformer-like performance while maintaining interpretability.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                The relationship between computing systems and the brain has served as
motivation for pioneering theoreticians since John von Neumann and Alan Turing.
Uniform, scale-free biological networks, such as the brain, have powerful
properties, including generalizing over time, which is the main barrier for
Machine Learning on the path to Universal Reasoning Models.
  We introduce `Dragon Hatchling' (BDH), a new <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Large%20Language%20Model">Large Language Model</a>
architecture based on a scale-free biologically inspired network of \n
locally-interacting neuron particles. BDH couples strong theoretical
foundations and inherent <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=interpretability">interpretability</a> without sacrificing Transformer-like
performance.
  BDH is a practical, performant state-of-the-art attention-based state space
sequence learning architecture. In addition to being a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=graph%20model">graph model</a>, BDH admits
a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=GPU-friendly">GPU-friendly</a> formulation. It exhibits <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Transformer-like%20scaling%20laws">Transformer-like scaling laws</a>:
empirically BDH rivals GPT2 performance on language and translation tasks, at
the same number of parameters (10M to 1B), for the same training data.
  BDH can be represented as a brain model. The working memory of BDH during
inference entirely relies on <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=synaptic%20plasticity">synaptic plasticity</a> with <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Hebbian%20learning">Hebbian learning</a> using
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=spiking%20neurons">spiking neurons</a>. We confirm empirically that specific, individual synapses
strengthen connection whenever BDH hears or reasons about a specific concept
while processing language inputs. The neuron interaction network of BDH is a
graph of high modularity with heavy-tailed degree distribution. The BDH model
is biologically plausible, explaining one possible mechanism which human
neurons could use to achieve speech.
  BDH is designed for <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=interpretability">interpretability</a>. Activation vectors of BDH are sparse
and positive. We demonstrate <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=monosemanticity">monosemanticity</a> in BDH on language tasks.
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Interpretability">Interpretability</a> of state, which goes beyond <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=interpretability">interpretability</a> of neurons and
model parameters, is an inherent feature of the BDH architecture.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="TRENDING" data-date="2020-12-31" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2012.15840" target="_blank" onclick="event.stopPropagation()">Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective
  with Transformers</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2020-12-31</div>
                                        <div class="tags"><span class="tag">TRENDING</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2012.15840" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/fudan-zvg/SETR" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 1</div>
                                    <div class="link-item stars-item">‚≠ê 1.09k</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">A pure transformer-based approach to semantic segmentation, termed SETransformer (SETR), is proposed and achieves state-of-the-art performance on benchmark datasets by leveraging global context modeling.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Most recent semantic segmentation methods adopt a fully-convolutional network
(FCN) with an <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=encoder-decoder%20architecture">encoder-decoder architecture</a>. The encoder progressively reduces
the spatial resolution and learns more abstract/semantic visual concepts with
larger receptive fields. Since context modeling is critical for segmentation,
the latest efforts have been focused on increasing the receptive field, through
either <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=dilated%2Fatrous%20convolutions">dilated/atrous convolutions</a> or inserting <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=attention%20modules">attention modules</a>. However, the
encoder-decoder based FCN architecture remains unchanged. In this paper, we aim
to provide an alternative perspective by treating semantic segmentation as a
sequence-to-sequence prediction task. Specifically, we deploy a pure
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=transformer">transformer</a> (ie, without convolution and resolution reduction) to encode an
image as a sequence of patches. With the global context modeled in every layer
of the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=transformer">transformer</a>, this encoder can be combined with a simple decoder to
provide a powerful segmentation model, termed SEgmentation <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=TRansformer">TRansformer</a> (SETR).
Extensive experiments show that SETR achieves new state of the art on <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=ADE20K">ADE20K</a>
(50.28% <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=mIoU">mIoU</a>), <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Pascal%20Context">Pascal Context</a> (55.83% <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=mIoU">mIoU</a>) and competitive results on
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Cityscapes">Cityscapes</a>. Particularly, we achieve the first position in the highly
competitive <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=ADE20K">ADE20K</a> test server leaderboard on the day of submission.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="DAILY" data-date="2025-10-23" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2510.20579" target="_blank" onclick="event.stopPropagation()">Open-o3 Video: Grounded Video Reasoning with Explicit Spatio-Temporal
  Evidence</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-10-23</div>
                                        <div class="tags"><span class="tag">DAILY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2510.20579" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <div class="link-item" style="background: #f5f5f5; color: #999;">No GitHub</div>
                                    <div class="link-item upvote-item">üî• 1</div>
                                    <div class="link-item stars-item">‚≠ê 0</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">Open-o3 Video integrates spatio-temporal evidence into video reasoning, achieving state-of-the-art performance on multiple benchmarks and providing valuable reasoning traces for test-time scaling.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Most <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=video%20reasoning">video reasoning</a> models only generate textual <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=reasoning%20traces">reasoning traces</a> without
indicating when and where key evidence appears. Recent models such as OpenAI-o3
have sparked wide interest in evidence-centered reasoning for images, yet
extending this ability to videos is more challenging, as it requires joint
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=temporal%20tracking">temporal tracking</a> and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=spatial%20localization">spatial localization</a> across dynamic scenes. We introduce
Open-o3 Video, a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=non-agent%20framework">non-agent framework</a> that integrates explicit spatio-temporal
evidence into <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=video%20reasoning">video reasoning</a>, and carefully collect training data and design
training strategies to address the aforementioned challenges. The model
highlights key timestamps, objects, and bounding boxes alongside its answers,
allowing reasoning to be grounded in concrete visual observations. To enable
this functionality, we first curate and build two high-quality datasets,
STGR-CoT-30k for <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=SFT">SFT</a> and STGR-<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=RL">RL</a>-36k for <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=RL">RL</a>, with carefully constructed
temporal and spatial annotations, since most existing datasets offer either
temporal spans for videos or spatial boxes on images, lacking unified
spatio-temporal supervision and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=reasoning%20traces">reasoning traces</a>. Then, we adopt a cold-start
reinforcement learning strategy with multiple specially designed rewards that
jointly encourage answer accuracy, temporal alignment, and spatial precision.
On <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=V-STAR%20benchmark">V-STAR benchmark</a>, Open-o3 Video achieves state-of-the-art performance,
raising <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=mAM">mAM</a> by 14.4% and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=mLGM">mLGM</a> by 24.2% on the Qwen2.5-VL baseline. Consistent
improvements are also observed on a broad range of video understanding
benchmarks, including <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=VideoMME">VideoMME</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=WorldSense">WorldSense</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=VideoMMMU">VideoMMMU</a>, and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=TVGBench">TVGBench</a>. Beyond
accuracy, the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=reasoning%20traces">reasoning traces</a> produced by Open-o3 Video also provide valuable
signals for test-time scaling, enabling <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=confidence-aware%20verification">confidence-aware verification</a> and
improving answer reliability.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="MONTHLY TRENDING" data-date="2025-10-06" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2510.04871" target="_blank" onclick="event.stopPropagation()">Less is More: Recursive Reasoning with Tiny Networks</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-10-06</div>
                                        <div class="tags"><span class="tag">MONTHLY</span><span class="tag">TRENDING</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2510.04871" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/SamsungSAILMontreal/TinyRecursiveModels" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 434</div>
                                    <div class="link-item stars-item">‚≠ê 5.11k</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">Tiny Recursive Model (TRM) achieves high generalization on complex puzzle tasks using a small, two-layer network with minimal parameters, outperforming larger language models.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Hierarchical%20Reasoning%20Model">Hierarchical Reasoning Model</a> (<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=HRM">HRM</a>) is a novel approach using two small neural
networks recursing at different frequencies. This biologically inspired method
beats Large Language models (LLMs) on hard puzzle tasks such as Sudoku, Maze,
and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=ARC-AGI">ARC-AGI</a> while trained with small models (27M parameters) on small data
(around 1000 examples). <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=HRM">HRM</a> holds great promise for solving hard problems with
small networks, but it is not yet well understood and may be suboptimal. We
propose <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Tiny%20Recursive%20Model">Tiny Recursive Model</a> (<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=TRM">TRM</a>), a much simpler <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=recursive%20reasoning">recursive reasoning</a> approach
that achieves significantly higher generalization than <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=HRM">HRM</a>, while using a
single tiny network with only 2 layers. With only 7M parameters, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=TRM">TRM</a> obtains
45% test-accuracy on <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=ARC-AGI">ARC-AGI</a>-1 and 8% on <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=ARC-AGI">ARC-AGI</a>-2, higher than most LLMs
(e.g., <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Deepseek%20R1">Deepseek R1</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=o3-mini">o3-mini</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Gemini%202.5%20Pro">Gemini 2.5 Pro</a>) with less than 0.01% of the
parameters.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="TRENDING" data-date="2024-11-02" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2411.01156" target="_blank" onclick="event.stopPropagation()">Fish-Speech: Leveraging Large Language Models for Advanced Multilingual
  Text-to-Speech Synthesis</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2024-11-02</div>
                                        <div class="tags"><span class="tag">TRENDING</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2411.01156" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/fishaudio/fish-speech" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 8</div>
                                    <div class="link-item stars-item">‚≠ê 23.7k</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">Fish-Speech enhances Text-to-Speech systems using a Dual-AR architecture with GFSQ and FF-GAN to improve linguistic feature extraction, compression, and multilingual support.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Text-to-Speech">Text-to-Speech</a> (<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=TTS">TTS</a>) systems face ongoing challenges in processing complex
linguistic features, handling polyphonic expressions, and producing
natural-sounding multilingual speech - capabilities that are crucial for future
AI applications. In this paper, we present Fish-Speech, a novel framework that
implements a serial fast-slow <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Dual%20Autoregressive">Dual Autoregressive</a> (Dual-AR) architecture to
enhance the stability of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Grouped%20Finite%20Scalar%20Vector%20Quantization">Grouped Finite Scalar Vector Quantization</a> (<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=GFSQ">GFSQ</a>) in
sequence generation tasks. This architecture improves codebook processing
efficiency while maintaining high-fidelity outputs, making it particularly
effective for AI interactions and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=voice%20cloning">voice cloning</a>.
  Fish-Speech leverages <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Large%20Language%20Models">Large Language Models</a> (LLMs) for linguistic feature
extraction, eliminating the need for traditional <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=grapheme-to-phoneme">grapheme-to-phoneme</a> (G2P)
conversion and thereby streamlining the synthesis pipeline and enhancing
multilingual support. Additionally, we developed <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=FF-GAN">FF-GAN</a> through <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=GFSQ">GFSQ</a> to achieve
superior <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=compression%20ratios">compression ratios</a> and near 100\% <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=codebook%20utilization">codebook utilization</a>.
  Our approach addresses key limitations of current <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=TTS">TTS</a> systems while providing
a foundation for more sophisticated, context-aware speech synthesis.
Experimental results show that Fish-Speech significantly outperforms baseline
models in handling complex linguistic scenarios and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=voice%20cloning">voice cloning</a> tasks,
demonstrating its potential to advance <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=TTS">TTS</a> technology in AI applications. The
implementation is open source at
https://github.com/fishaudio/fish-speech{https://github.com/fishaudio/fish-speech}.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="WEEKLY TRENDING" data-date="2025-10-21" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2510.18866" target="_blank" onclick="event.stopPropagation()">LightMem: Lightweight and Efficient Memory-Augmented Generation</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-10-21</div>
                                        <div class="tags"><span class="tag">TRENDING</span><span class="tag">WEEKLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2510.18866" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/zjunlp/LightMem" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 95</div>
                                    <div class="link-item stars-item">‚≠ê 174</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">LightMem, a memory system inspired by human memory, enhances LLMs by efficiently managing historical interaction information, improving accuracy and reducing computational costs.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Despite their remarkable capabilities, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Large%20Language%20Models">Large Language Models</a> (<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=LLMs">LLMs</a>) struggle
to effectively leverage historical interaction information in dynamic and
complex environments. <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Memory%20systems">Memory systems</a> enable <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=LLMs">LLMs</a> to move beyond stateless
interactions by introducing persistent information storage, retrieval, and
utilization mechanisms. However, existing <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=memory%20systems">memory systems</a> often introduce
substantial time and computational overhead. To this end, we introduce a new
memory system called <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=LightMem">LightMem</a>, which strikes a balance between the performance
and efficiency of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=memory%20systems">memory systems</a>. Inspired by the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Atkinson-Shiffrin%20model">Atkinson-Shiffrin model</a> of
human memory, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=LightMem">LightMem</a> organizes memory into three complementary stages. First,
cognition-inspired <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=sensory%20memory">sensory memory</a> rapidly filters irrelevant information
through lightweight compression and groups information according to their
topics. Next, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=topic-aware%20short-term%20memory">topic-aware short-term memory</a> consolidates these topic-based
groups, organizing and summarizing content for more structured access. Finally,
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=long-term%20memory">long-term memory</a> with <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=sleep-time%20update">sleep-time update</a> employs an offline procedure that
decouples consolidation from online inference. Experiments on <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=LongMemEval">LongMemEval</a> with
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=GPT">GPT</a> and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Qwen">Qwen</a> backbones show that <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=LightMem">LightMem</a> outperforms strong baselines in
accuracy (up to 10.9% gains) while reducing <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=token%20usage">token usage</a> by up to 117x, API
calls by up to 159x, and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=runtime">runtime</a> by over 12x. The code is available at
https://github.com/zjunlp/<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=LightMem">LightMem</a>.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="WEEKLY" data-date="2025-10-20" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2510.18121" target="_blank" onclick="event.stopPropagation()">Efficient Long-context Language Model Training by Core Attention
  Disaggregation</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-10-20</div>
                                        <div class="tags"><span class="tag">WEEKLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2510.18121" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <div class="link-item" style="background: #f5f5f5; color: #999;">No GitHub</div>
                                    <div class="link-item upvote-item">üî• 91</div>
                                    <div class="link-item stars-item">‚≠ê 0</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">CAD, a technique for long-context large language model training, improves throughput and balance by decoupling and distributing core attention computations.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                We present <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=core%20attention%20disaggregation">core attention disaggregation</a> (CAD), a technique that improves
long-context large language model training by decoupling the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=core%20attention">core attention</a>
computation, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=softmax(QK%5ET)V">softmax(QK^T)V</a>, from the rest of the model and executing it on a
separate pool of devices. In existing systems, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=core%20attention">core attention</a> is colocated with
other layers; at long context lengths, its quadratic compute growth compared to
the near-linear growth of other components causes load imbalance and stragglers
across data and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=pipeline%20parallel">pipeline parallel</a> groups. CAD is enabled by two observations.
First, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=core%20attention">core attention</a> is stateless: it has no trainable parameters and only
minimal transient data, so balancing reduces to scheduling compute-bound tasks.
Second, it is composable: modern attention kernels retain high efficiency when
processing fused batches of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=token-level%20shards">token-level shards</a> with arbitrary lengths. CAD
partitions <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=core%20attention">core attention</a> into token-level tasks and dispatches them to
dedicated <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=attention%20servers">attention servers</a>, which dynamically rebatch tasks to equalize
compute without sacrificing kernel efficiency. We implement CAD in a system
called <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=DistCA">DistCA</a>, which uses a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=ping-pong%20execution">ping-pong execution</a> scheme to fully overlap
communication with computation and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=in-place%20execution">in-place execution</a> on <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=attention%20servers">attention servers</a> to
reduce memory use. On 512 H200 GPUs and context lengths up to 512k tokens,
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=DistCA">DistCA</a> improves end-to-end training throughput by up to 1.35x, eliminates data
and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=pipeline%20parallel">pipeline parallel</a> stragglers, and achieves near-perfect compute and memory
balance.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="MONTHLY" data-date="2025-10-09" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2510.08558" target="_blank" onclick="event.stopPropagation()">Agent Learning via Early Experience</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-10-09</div>
                                        <div class="tags"><span class="tag">MONTHLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2510.08558" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <div class="link-item" style="background: #f5f5f5; color: #999;">No GitHub</div>
                                    <div class="link-item upvote-item">üî• 236</div>
                                    <div class="link-item stars-item">‚≠ê 0</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">Early experience, using agent-generated interaction data without reward signals, improves policy effectiveness and generalization, serving as a bridge between imitation learning and reinforcement learning.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                A long-term goal of language agents is to learn and improve through their own
experience, ultimately outperforming humans in complex, real-world tasks.
However, training agents from experience data with <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=reinforcement%20learning">reinforcement learning</a>
remains difficult in many environments, which either lack verifiable rewards
(e.g., websites) or require inefficient long-horizon rollouts (e.g., multi-turn
tool use). As a result, most current agents rely on supervised fine-tuning on
expert data, which is challenging to scale and generalizes poorly. This
limitation stems from the nature of expert demonstrations: they capture only a
narrow range of scenarios and expose the agent to limited environment
diversity. We address this limitation with a middle-ground paradigm we call
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=early%20experience">early experience</a>: interaction data generated by the agent's own actions, where
the resulting future states serve as supervision without reward signals. Within
this paradigm we study two strategies of using such data: (1) Implicit world
modeling, which uses collected states to ground the policy in environment
dynamics; and (2) <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Self-reflection">Self-reflection</a>, where the agent learns from its suboptimal
actions to improve reasoning and decision-making. We evaluate across eight
diverse environments and multiple model families. Our approaches consistently
improve effectiveness and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=out-of-domain%20generalization">out-of-domain generalization</a>, highlighting the value
of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=early%20experience">early experience</a>. Moreover, in environments with verifiable rewards, our
results provide promising signals that <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=early%20experience">early experience</a> offers a strong
foundation for subsequent <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=reinforcement%20learning">reinforcement learning</a>, positioning it as a practical
bridge between imitation learning and fully experience-driven agents.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="WEEKLY" data-date="2025-10-22" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2510.19338" target="_blank" onclick="event.stopPropagation()">Every Attention Matters: An Efficient Hybrid Architecture for
  Long-Context Reasoning</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-10-22</div>
                                        <div class="tags"><span class="tag">WEEKLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2510.19338" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <div class="link-item" style="background: #f5f5f5; color: #999;">No GitHub</div>
                                    <div class="link-item upvote-item">üî• 78</div>
                                    <div class="link-item stars-item">‚≠ê 0</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">The Ring-linear model series, including Ring-mini-linear-2.0 and Ring-flash-linear-2.0, uses a hybrid architecture combining linear and softmax attention to reduce inference costs and improve training efficiency.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                In this technical report, we present the Ring-linear model series,
specifically including Ring-mini-linear-2.0 and Ring-flash-linear-2.0.
Ring-mini-linear-2.0 comprises 16B parameters and 957M activations, while
Ring-flash-linear-2.0 contains 104B parameters and 6.1B activations. Both
models adopt a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=hybrid%20architecture">hybrid architecture</a> that effectively integrates <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=linear%20attention">linear attention</a>
and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=softmax%20attention">softmax attention</a>, significantly reducing I/O and computational overhead in
long-context inference scenarios. Compared to a 32 billion parameter dense
model, this series reduces inference cost to 1/10, and compared to the original
Ring series, the cost is also reduced by over 50%. Furthermore, through
systematic exploration of the ratio between different attention mechanisms in
the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=hybrid%20architecture">hybrid architecture</a>, we have identified the currently optimal model
structure. Additionally, by leveraging our self-developed high-performance FP8
operator library-linghe, overall training efficiency has been improved by 50%.
Benefiting from the high alignment between the training and inference engine
operators, the models can undergo long-term, stable, and highly efficient
optimization during the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=reinforcement%20learning">reinforcement learning</a> phase, consistently maintaining
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=SOTA%20performance">SOTA performance</a> across multiple challenging complex reasoning benchmarks.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="MONTHLY" data-date="2025-09-28" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2509.24002" target="_blank" onclick="event.stopPropagation()">MCPMark: A Benchmark for Stress-Testing Realistic and Comprehensive MCP
  Use</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-09-28</div>
                                        <div class="tags"><span class="tag">MONTHLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2509.24002" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/eval-sys/mcpmark" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 166</div>
                                    <div class="link-item stars-item">‚≠ê 286</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">MCPMark is a comprehensive benchmark for evaluating MCP use in real-world workflows, featuring diverse tasks that require richer interactions with the environment, and reveals that current LLMs perform poorly on these tasks.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=MCP">MCP</a> standardizes how <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=LLMs">LLMs</a> interact with external systems, forming the
foundation for <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=general%20agents">general agents</a>. However, existing <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=MCP%20benchmarks">MCP benchmarks</a> remain narrow
in scope: they focus on read-heavy tasks or tasks with limited interaction
depth, and fail to capture the complexity and realism of real-world workflows.
To address this gap, we propose <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=MCPMark">MCPMark</a>, a benchmark designed to evaluate <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=MCP">MCP</a>
use in a more realistic and comprehensive manner. It consists of 127
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=high-quality%20tasks">high-quality tasks</a> collaboratively created by <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=domain%20experts">domain experts</a> and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=AI%20agents">AI agents</a>.
Each task begins with a curated <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=initial%20state">initial state</a> and includes a programmatic
script for <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=automatic%20verification">automatic verification</a>. These tasks demand richer and more diverse
interactions with the environment, involving a broad range of create, read,
update, and delete (CRUD) operations. We conduct a comprehensive evaluation of
cutting-edge <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=LLMs">LLMs</a> using a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=minimal%20agent%20framework">minimal agent framework</a> that operates in a
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=tool-calling%20loop">tool-calling loop</a>. Empirical results show that the best-performing model,
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=gpt-5-medium">gpt-5-medium</a>, reaches only 52.56\% <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=pass%401">pass@1</a> and 33.86\% <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=pass%5E4">pass^4</a>, while other
widely regarded strong models, including <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=claude-sonnet-4">claude-sonnet-4</a> and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=o3">o3</a>, fall below
30\% <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=pass%401">pass@1</a> and 15\% <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=pass%5E4">pass^4</a>. On average, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=LLMs">LLMs</a> require 16.2 execution
turns and 17.4 <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=tool%20calls">tool calls</a> per task, significantly surpassing those in
previous <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=MCP%20benchmarks">MCP benchmarks</a> and highlighting the stress-testing nature of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=MCPMark">MCPMark</a>.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="WEEKLY" data-date="2025-10-20" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2510.18135" target="_blank" onclick="event.stopPropagation()">World-in-World: World Models in a Closed-Loop World</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-10-20</div>
                                        <div class="tags"><span class="tag">WEEKLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2510.18135" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/World-In-World/world-in-world" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 72</div>
                                    <div class="link-item stars-item">‚≠ê 39</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">World-in-World evaluates generative world models in closed-loop environments, emphasizing task success over visual quality and revealing insights into controllability, data scaling, and compute allocation.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Generative%20world%20models">Generative world models</a> (<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=WM">WM</a>s) can now simulate worlds with striking visual
realism, which naturally raises the question of whether they can endow embodied
agents with <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=predictive%20perception">predictive perception</a> for <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=decision%20making">decision making</a>. Progress on this
question has been limited by fragmented evaluation: most existing benchmarks
adopt <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=open-loop%20protocols">open-loop protocols</a> that emphasize <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=visual%20quality">visual quality</a> in isolation, leaving
the core issue of embodied utility unresolved, i.e., do <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=WM">WM</a>s actually help
agents succeed at embodied tasks? To address this gap, we introduce
World-in-World, the first open platform that benchmarks <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=WM">WM</a>s in a closed-loop
world that mirrors real <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=agent-environment%20interactions">agent-environment interactions</a>. World-in-World provides
a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=unified%20online%20planning%20strategy">unified online planning strategy</a> and a standardized <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=action%20API">action API</a>, enabling
heterogeneous <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=WM">WM</a>s for <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=decision%20making">decision making</a>. We curate four closed-loop environments
that rigorously evaluate diverse <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=WM">WM</a>s, prioritize <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=task%20success">task success</a> as the primary
metric, and move beyond the common focus on <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=visual%20quality">visual quality</a>; we also present the
first <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=data%20scaling%20law">data scaling law</a> for world models in embodied settings. Our study
uncovers three surprises: (1) <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=visual%20quality">visual quality</a> alone does not guarantee task
success, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=controllability">controllability</a> matters more; (2) scaling post-training with
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=action-observation%20data">action-observation data</a> is more effective than upgrading the pretrained video
generators; and (3) allocating more <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=inference-time%20compute">inference-time compute</a> allows <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=WM">WM</a>s to
substantially improve closed-loop performance.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="WEEKLY TRENDING" data-date="2025-10-19" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2510.16872" target="_blank" onclick="event.stopPropagation()">DeepAnalyze: Agentic Large Language Models for Autonomous Data Science</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-10-19</div>
                                        <div class="tags"><span class="tag">TRENDING</span><span class="tag">WEEKLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2510.16872" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/ruc-datalab/DeepAnalyze" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 69</div>
                                    <div class="link-item stars-item">‚≠ê 622</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">DeepAnalyze-8B, an agentic LLM, autonomously completes the data science pipeline from raw data to research reports using curriculum-based training and data-grounded trajectory synthesis.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Autonomous data science, from raw data sources to analyst-grade deep research
reports, has been a long-standing challenge, and is now becoming feasible with
the emergence of powerful <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=large%20language%20models">large language models</a> (<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=LLMs">LLMs</a>). Recent workflow-based
data agents have shown promising results on specific data tasks but remain
fundamentally limited in achieving fully autonomous data science due to their
reliance on predefined workflows. In this paper, we introduce DeepAnalyze-8B,
the first <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=agentic%20LLM">agentic LLM</a> designed for autonomous data science, capable of
automatically completing the end-toend pipeline from data sources to
analyst-grade deep research reports. To tackle high-complexity data science
tasks, we propose a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=curriculum-based%20agentic%20training">curriculum-based agentic training</a> paradigm that emulates
the learning trajectory of human data scientists, enabling <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=LLMs">LLMs</a> to
progressively acquire and integrate multiple capabilities in real-world
environments. We also introduce a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=data-grounded%20trajectory%20synthesis">data-grounded trajectory synthesis</a> framework
that constructs high-quality training data. Through agentic training,
DeepAnalyze learns to perform a broad spectrum of data tasks, ranging from data
question answering and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=specialized%20analytical%20tasks">specialized analytical tasks</a> to open-ended data
research. Experiments demonstrate that, with only 8B parameters, DeepAnalyze
outperforms previous workflow-based agents built on most advanced proprietary
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=LLMs">LLMs</a>. The model, code, and training data of DeepAnalyze are open-sourced,
paving the way toward autonomous data science.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="MONTHLY" data-date="2025-10-13" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2510.11696" target="_blank" onclick="event.stopPropagation()">QeRL: Beyond Efficiency -- Quantization-enhanced Reinforcement Learning
  for LLMs</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-10-13</div>
                                        <div class="tags"><span class="tag">MONTHLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2510.11696" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/NVlabs/QeRL" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 164</div>
                                    <div class="link-item stars-item">‚≠ê 362</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">QeRL, a quantization-enhanced reinforcement learning framework, accelerates RL training for large language models by combining NVFP4 quantization with Low-Rank Adaptation and an Adaptive Quantization Noise mechanism, achieving significant speedups and improved performance.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                We propose QeRL, a Quantization-enhanced <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Reinforcement%20Learning">Reinforcement Learning</a> framework for
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=large%20language%20models%20(LLMs)">large language models (LLMs)</a>. While RL is essential for LLMs' reasoning
capabilities, it is resource-intensive, requiring substantial GPU memory and
long rollout durations. QeRL addresses these issues by combining NVFP4
quantization with <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Low-Rank%20Adaptation%20(LoRA)">Low-Rank Adaptation (LoRA)</a>, accelerating <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=rollout%20phase">rollout phase</a> of RL
while reducing memory overhead. Beyond efficiency, our findings show that
quantization noise increases <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=policy%20entropy">policy entropy</a>, enhancing <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=exploration">exploration</a>, and
enabling the discovery of better strategies during RL. To further optimize
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=exploration">exploration</a>, QeRL introduces an <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Adaptive%20Quantization%20Noise%20(AQN)">Adaptive Quantization Noise (AQN)</a> mechanism,
which dynamically adjusts noise during training. Experiments demonstrate that
QeRL delivers over 1.5 times speedup in the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=rollout%20phase">rollout phase</a>. Moreover, this is
the first framework to enable RL training of a 32B LLM on a single H100 80GB
GPU, while delivering overall speedups for RL training. It also achieves faster
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=reward%20growth">reward growth</a> and higher final accuracy than 16-bit LoRA and QLoRA, while
matching the performance of full-parameter fine-tuning on mathematical
benchmarks such as <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=GSM8K">GSM8K</a> (90.8%) and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=MATH%20500">MATH 500</a> (77.4%) in the 7B model. These
results establish QeRL as an efficient and effective framework for RL training
in LLMs.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="WEEKLY" data-date="2025-10-21" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2510.18927" target="_blank" onclick="event.stopPropagation()">BAPO: Stabilizing Off-Policy Reinforcement Learning for LLMs via
  Balanced Policy Optimization with Adaptive Clipping</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-10-21</div>
                                        <div class="tags"><span class="tag">WEEKLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2510.18927" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/WooooDyy/BAPO" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 67</div>
                                    <div class="link-item stars-item">‚≠ê 49</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">BAlanced Policy Optimization with Adaptive Clipping (BAPO) addresses challenges in off-policy reinforcement learning by dynamically adjusting clipping bounds to improve sample efficiency, stability, and performance in large language models.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Reinforcement%20learning">Reinforcement learning</a> (RL) has recently become the core paradigm for
aligning and strengthening <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=large%20language%20models">large language models</a> (LLMs). Yet, applying RL in
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=off-policy%20settings">off-policy settings</a>--where stale data from past policies are used for
training--improves sample efficiency, but remains challenging: <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=policy%20entropy">policy entropy</a>
declines sharply, optimization often becomes unstable and may even collapse.
Through theoretical and empirical analysis, we identify two key insights: (i)
an imbalance in optimization, where negative-advantage samples dominate the
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=policy%20gradient">policy gradient</a>, suppressing useful behaviors and risking <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=gradient%20explosions">gradient explosions</a>;
and (ii) the derived <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Entropy-Clip%20Rule">Entropy-Clip Rule</a>, which reveals that the fixed clipping
mechanism in <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=PPO-like%20objectives">PPO-like objectives</a> systematically blocks entropy-increasing
updates, thereby driving the policy toward over-exploitation at the expense of
exploration. Building on these insights, we propose BAlanced Policy
Optimization with Adaptive Clipping (BAPO), a simple yet effective method that
dynamically adjusts clipping bounds to adaptively re-balance positive and
negative contributions, preserve entropy, and stabilize RL optimization. Across
diverse off-policy scenarios--including <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=sample%20replay">sample replay</a> and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=partial%20rollout">partial rollout</a>--BAPO
achieves fast, stable, and data-efficient training. On <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=AIME%202024">AIME 2024</a> and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=AIME%202025">AIME 2025</a>
benchmarks, our 7B BAPO model surpasses open-source counterparts such as
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=SkyWork-OR1-7B">SkyWork-OR1-7B</a>, while our 32B BAPO model not only achieves state-of-the-art
results among models of the same scale but also outperforms leading proprietary
systems like <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=o3-mini">o3-mini</a> and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Gemini-2.5-Flash-Thinking">Gemini-2.5-Flash-Thinking</a>.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="MONTHLY" data-date="2025-10-13" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2510.11690" target="_blank" onclick="event.stopPropagation()">Diffusion Transformers with Representation Autoencoders</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-10-13</div>
                                        <div class="tags"><span class="tag">MONTHLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2510.11690" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/bytetriper/RAE" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 157</div>
                                    <div class="link-item stars-item">‚≠ê 1.31k</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">Replacing VAEs with pretrained representation encoders in Diffusion Transformers enhances generative quality and convergence speed without auxiliary losses.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Latent%20generative%20modeling">Latent generative modeling</a>, where a pretrained autoencoder maps pixels into a
latent space for the diffusion process, has become the standard strategy for
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Diffusion%20Transformers%20(DiT)">Diffusion Transformers (DiT)</a>; however, the autoencoder component has barely
evolved. Most DiTs continue to rely on the original <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=VAE%20encoder">VAE encoder</a>, which
introduces several limitations: outdated backbones that compromise
architectural simplicity, low-dimensional latent spaces that restrict
information capacity, and weak representations that result from purely
reconstruction-based training and ultimately limit generative quality. In this
work, we explore replacing the VAE with <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=pretrained%20representation%20encoders">pretrained representation encoders</a>
(e.g., <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=DINO">DINO</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=SigLIP">SigLIP</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=MAE">MAE</a>) paired with trained decoders, forming what we term
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Representation%20Autoencoders%20(RAEs)">Representation Autoencoders (RAEs)</a>. These models provide both high-quality
reconstructions and semantically rich latent spaces, while allowing for a
scalable <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=transformer-based%20architecture">transformer-based architecture</a>. Since these latent spaces are
typically high-dimensional, a key challenge is enabling <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=diffusion%20transformers">diffusion transformers</a>
to operate effectively within them. We analyze the sources of this difficulty,
propose theoretically motivated solutions, and validate them empirically. Our
approach achieves faster convergence without auxiliary representation alignment
losses. Using a DiT variant equipped with a lightweight, wide DDT head, we
achieve strong <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=image%20generation">image generation</a> results on <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=ImageNet">ImageNet</a>: 1.51 <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=FID">FID</a> at 256x256 (no
guidance) and 1.13 at both 256x256 and 512x512 (with guidance). RAE offers
clear advantages and should be the new default for diffusion transformer
training.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="WEEKLY" data-date="2025-10-17" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2510.15870" target="_blank" onclick="event.stopPropagation()">OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding
  LLM</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-10-17</div>
                                        <div class="tags"><span class="tag">WEEKLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2510.15870" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/NVlabs/OmniVinci" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 66</div>
                                    <div class="link-item stars-item">‚≠ê 247</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">OmniVinci, an open-source omni-modal LLM, enhances cross-modal understanding and performance across audio, vision, and robotics applications with innovative architecture and efficient data curation.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Advancing machine intelligence requires developing the ability to perceive
across multiple modalities, much as humans sense the world. We introduce
OmniVinci, an initiative to build a strong, open-source, omni-modal LLM. We
carefully study the design choices across model architecture and data curation.
For model architecture, we present three key innovations: (i) <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=OmniAlignNet">OmniAlignNet</a> for
strengthening alignment between vision and audio embeddings in a shared
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=omni-modal%20latent%20space">omni-modal latent space</a>; (ii) <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Temporal%20Embedding%20Grouping">Temporal Embedding Grouping</a> for capturing
relative temporal alignment between vision and audio signals; and (iii)
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Constrained%20Rotary%20Time%20Embedding">Constrained Rotary Time Embedding</a> for encoding absolute temporal information in
omni-modal embeddings. We introduce a curation and synthesis pipeline that
generates 24M single-modal and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=omni-modal%20conversations">omni-modal conversations</a>. We find that
modalities reinforce one another in both perception and reasoning. Our model,
OmniVinci, outperforms Qwen2.5-Omni with +19.05 on <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=DailyOmni">DailyOmni</a> (cross-modal
understanding), +1.7 on <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=MMAR">MMAR</a> (audio), and +3.9 on <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Video-MME">Video-MME</a> (vision), while
using just 0.2T training tokens - a 6 times reduction compared to
Qwen2.5-Omni's 1.2T. We finally demonstrate <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=omni-modal%20advantages">omni-modal advantages</a> in downstream
applications spanning robotics, medical AI, and smart factory.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="TRENDING" data-date="2024-10-23" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2410.17799" target="_blank" onclick="event.stopPropagation()">OmniFlatten: An End-to-end GPT Model for Seamless Voice Conversation</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2024-10-23</div>
                                        <div class="tags"><span class="tag">TRENDING</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2410.17799" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/karpathy/nanogpt" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 1</div>
                                    <div class="link-item stars-item">‚≠ê 47.7k</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">A novel GPT-based model, OmniFlatten, enables real-time natural full-duplex spoken dialogue through a multi-stage post-training technique that integrates speech and text without altering the original model's architecture.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Full-duplex spoken dialogue systems significantly advance over traditional
turn-based dialogue systems, as they allow simultaneous bidirectional
communication, closely mirroring human-human interactions. However, achieving
low latency and natural interactions in full-duplex dialogue systems remains a
significant challenge, especially considering human conversation dynamics such
as interruptions, backchannels, and overlapping speech. In this paper, we
introduce a novel <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=End-to-End%20GPT-based%20model">End-to-End GPT-based model</a> <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=OmniFlatten">OmniFlatten</a> for full-duplex
conversation, capable of effectively modeling the complex behaviors inherent to
natural conversations with low latency. To achieve full-duplex communication
capabilities, we propose a multi-stage post-training scheme that progressively
adapts a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=text-based%20large%20language%20model%20(LLM)">text-based large language model (LLM)</a> backbone into a speech-text
dialogue LLM, capable of generating text and speech in real time, without
modifying the architecture of the backbone LLM. The training process comprises
three stages: <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=modality%20alignment">modality alignment</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=half-duplex%20dialogue%20learning">half-duplex dialogue learning</a>, and
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=full-duplex%20dialogue%20learning">full-duplex dialogue learning</a>. Throughout all training stages, we standardize
the data using a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=flattening%20operation">flattening operation</a>, which allows us to unify the training
methods and the model architecture across different modalities and tasks. Our
approach offers a straightforward modeling technique and a promising research
direction for developing efficient and natural end-to-end full-duplex spoken
dialogue systems. Audio samples of dialogues generated by <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=OmniFlatten">OmniFlatten</a> can be
found at this web site (https://<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=omniflatten">omniflatten</a>.github.io/).
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="MONTHLY" data-date="2025-10-14" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2510.12276" target="_blank" onclick="event.stopPropagation()">Spatial Forcing: Implicit Spatial Representation Alignment for
  Vision-language-action Model</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-10-14</div>
                                        <div class="tags"><span class="tag">MONTHLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2510.12276" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/OpenHelix-Team/Spatial-Forcing" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 139</div>
                                    <div class="link-item stars-item">‚≠ê 73</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">(No summary available)</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Vision-language-action (VLA) models have recently shown strong potential in
enabling robots to follow language instructions and execute precise actions.
However, most VLAs are built upon vision-language models pretrained solely on
2D data, which lack accurate spatial awareness and hinder their ability to
operate in the 3D physical world. Existing solutions attempt to incorporate
explicit 3D sensor inputs such as depth maps or point clouds, but these
approaches face challenges due to sensor noise, hardware heterogeneity, and
incomplete depth coverage in existing datasets. Alternative methods that
estimate 3D cues from 2D images also suffer from the limited performance of
depth estimators.We propose Spatial Forcing (SF), a simple yet effective
alignment strategy that implicitly forces VLA models to develop spatial
comprehension capabilities without relying on explicit 3D inputs or depth
estimators. SF aligns intermediate visual embeddings of VLAs with geometric
representations produced by pretrained 3D foundation models. By enforcing
alignment at intermediate layers, SF guides VLAs to encode richer spatial
representations that enhance action precision.Extensive experiments in
simulation and real-world environments demonstrate that SF achieves
state-of-the-art results, surpassing both 2D- and 3D-based VLAs. SF further
accelerates training by up to 3.8x and improves data efficiency across diverse
robotic tasks. Project page is at https://spatial-forcing.github.io/
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="TRENDING" data-date="2025-10-17" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2510.15862" target="_blank" onclick="event.stopPropagation()">PokeeResearch: Effective Deep Research via Reinforcement Learning from
  AI Feedback and Robust Reasoning Scaffold</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-10-17</div>
                                        <div class="tags"><span class="tag">TRENDING</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2510.15862" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/Pokee-AI/PokeeResearchOSS" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• -</div>
                                    <div class="link-item stars-item">‚≠ê 283</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">PokeeResearch-7B, a 7B-parameter deep research agent, achieves state-of-the-art performance using reinforcement learning and chain-of-thought reasoning to enhance robustness and alignment.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Tool-augmented large language models (LLMs) are emerging as deep research
agents, systems that decompose complex queries, retrieve external evidence, and
synthesize grounded responses. Yet current agents remain limited by shallow
retrieval, weak alignment metrics, and brittle tool-use behavior. We introduce
PokeeResearch-7B, a 7B-parameter deep research agent built under a unified
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=reinforcement%20learning">reinforcement learning</a> framework for robustness, alignment, and scalability.
PokeeResearch-7B is trained by an annotation-free <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Reinforcement%20Learning">Reinforcement Learning</a> from
AI Feedback (RLAIF) framework to optimize policies using LLM-based reward
signals that capture factual accuracy, citation faithfulness, and instruction
adherence. A chain-of-thought-driven multi-call reasoning scaffold further
enhances robustness through <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=self-verification">self-verification</a> and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=adaptive%20recovery">adaptive recovery</a> from tool
failures. Among 10 popular <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=deep%20research%20benchmarks">deep research benchmarks</a>, PokeeResearch-7B achieves
state-of-the-art performance among 7B-scale <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=deep%20research%20agents">deep research agents</a>. This
highlights that careful <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=reinforcement%20learning">reinforcement learning</a> and reasoning design can produce
efficient, resilient, and research-grade AI agents. The model and inference
code is open-sourced under MIT license at
https://github.com/Pokee-AI/PokeeResearchOSS.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="MONTHLY" data-date="2025-09-29" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2509.25541" target="_blank" onclick="event.stopPropagation()">Vision-Zero: Scalable VLM Self-Improvement via Strategic Gamified
  Self-Play</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-09-29</div>
                                        <div class="tags"><span class="tag">MONTHLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2509.25541" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/wangqinsi1/Vision-Zero" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 136</div>
                                    <div class="link-item stars-item">‚≠ê 92</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">Vision-Zero is a domain-agnostic framework that enhances vision-language models through self-improvement in competitive visual games, using Iterative Self-Play Policy Optimization and achieving state-of-the-art performance without human annotation.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Although <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=reinforcement%20learning">reinforcement learning</a> (RL) can effectively enhance the reasoning
capabilities of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=vision-language%20models">vision-language models</a> (VLMs), current methods remain heavily
dependent on labor-intensive datasets that require extensive manual
construction and verification, leading to extremely high training costs and
consequently constraining the practical deployment of VLMs. To address this
challenge, we propose Vision-Zero, a domain-agnostic framework enabling VLM
self-improvement through competitive visual games generated from arbitrary
image pairs. Specifically, Vision-Zero encompasses three main attributes: (1)
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Strategic%20Self-Play%20Framework">Strategic Self-Play Framework</a>: Vision-Zero trains VLMs in "Who Is the
Spy"-style games, where the models engage in strategic reasoning and actions
across multiple roles. Through interactive gameplay, models autonomously
generate their training data without human annotation. (2) Gameplay from
Arbitrary Images: Unlike existing gamified frameworks, Vision-Zero can generate
games from arbitrary images, thereby enhancing the model's reasoning ability
across diverse domains and showing strong generalization to different tasks. We
demonstrate this versatility using three distinct types of image datasets:
CLEVR-based synthetic scenes, charts, and real-world images. (3) Sustainable
Performance Gain: We introduce <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Iterative%20Self-Play%20Policy%20Optimization">Iterative Self-Play Policy Optimization</a>
(Iterative-SPO), a novel training algorithm that alternates between <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Self-Play">Self-Play</a>
and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=reinforcement%20learning%20with%20verifiable%20rewards">reinforcement learning with verifiable rewards</a> (RLVR), mitigating the
performance plateau often seen in <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=self-play">self-play</a>-only training and achieving
sustained long-term improvements. Despite using label-free data, Vision-Zero
achieves state-of-the-art performance on reasoning, chart question answering,
and vision-centric understanding tasks, surpassing other annotation-based
methods. Models and code has been released at
https://github.com/wangqinsi1/Vision-Zero.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="WEEKLY TRENDING" data-date="2025-10-21" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2510.18701" target="_blank" onclick="event.stopPropagation()">UniGenBench++: A Unified Semantic Evaluation Benchmark for Text-to-Image
  Generation</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-10-21</div>
                                        <div class="tags"><span class="tag">TRENDING</span><span class="tag">WEEKLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2510.18701" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/CodeGoat24/UniGenBench" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 63</div>
                                    <div class="link-item stars-item">‚≠ê 98</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">UniGenBench++ is a comprehensive benchmark for text-to-image generation that evaluates semantic consistency across diverse scenarios and languages using a hierarchical prompt structure and a robust evaluation pipeline.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Recent progress in text-to-image (T2I) generation underscores the importance
of reliable <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=benchmark">benchmark</a>s in evaluating how accurately generated images reflect
the semantics of their textual prompt. However, (1) existing <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=benchmark">benchmark</a>s lack
the diversity of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=prompt%20scenarios">prompt scenarios</a> and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=multilingual%20support">multilingual support</a>, both essential for
real-world applicability; (2) they offer only coarse evaluations across primary
dimensions, covering a narrow range of sub-dimensions, and fall short in
fine-grained sub-dimension assessment. To address these limitations, we
introduce UniGenBench++, a unified <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=semantic%20assessment">semantic assessment</a> <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=benchmark">benchmark</a> for T2I
generation. Specifically, it comprises 600 prompts organized hierarchically to
ensure both coverage and efficiency: (1) spans across diverse real-world
scenarios, i.e., 5 main prompt themes and 20 subthemes; (2) comprehensively
probes T2I models' <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=semantic%20consistency">semantic consistency</a> over 10 primary and 27 sub evaluation
criteria, with each prompt assessing multiple testpoints. To rigorously assess
model robustness to variations in language and prompt length, we provide both
English and Chinese versions of each prompt in short and long forms. Leveraging
the general world knowledge and fine-grained image understanding capabilities
of a closed-source <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Multi-modal%20Large%20Language%20Model">Multi-modal Large Language Model</a> (<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=MLLM">MLLM</a>), i.e.,
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Gemini-2.5-Pro">Gemini-2.5-Pro</a>, an effective pipeline is developed for reliable <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=benchmark">benchmark</a>
construction and streamlined model assessment. Moreover, to further facilitate
community use, we train a robust evaluation model that enables offline
assessment of T2I model outputs. Through comprehensive <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=benchmark">benchmark</a>ing of both
open- and closed-sourced T2I models, we systematically reveal their strengths
and weaknesses across various aspects.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="TRENDING" data-date="2025-10-14" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2510.12323" target="_blank" onclick="event.stopPropagation()">RAG-Anything: All-in-One RAG Framework</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-10-14</div>
                                        <div class="tags"><span class="tag">TRENDING</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2510.12323" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/HKUDS/RAG-Anything" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 36</div>
                                    <div class="link-item stars-item">‚≠ê 9.39k</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">RAG-Anything is a unified framework that enhances multimodal knowledge retrieval by integrating cross-modal relationships and semantic matching, outperforming existing methods on complex benchmarks.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Retrieval-Augmented%20Generation">Retrieval-Augmented Generation</a> (<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=RAG">RAG</a>) has emerged as a fundamental paradigm
for expanding <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Large%20Language%20Models">Large Language Models</a> beyond their static training limitations.
However, a critical misalignment exists between current <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=RAG">RAG</a> capabilities and
real-world information environments. Modern knowledge repositories are
inherently <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=multimodal">multimodal</a>, containing rich combinations of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=textual%20content">textual content</a>, visual
elements, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=structured%20tables">structured tables</a>, and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=mathematical%20expressions">mathematical expressions</a>. Yet existing <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=RAG">RAG</a>
frameworks are limited to <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=textual%20content">textual content</a>, creating fundamental gaps when
processing <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=multimodal">multimodal</a> documents. We present <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=RAG">RAG</a>-Anything, a unified framework
that enables comprehensive knowledge retrieval across all modalities. Our
approach reconceptualizes <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=multimodal">multimodal</a> content as interconnected knowledge
entities rather than isolated data types. The framework introduces dual-graph
construction to capture both cross-modal relationships and textual semantics
within a unified representation. We develop <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=cross-modal%20hybrid%20retrieval">cross-modal hybrid retrieval</a> that
combines <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=structural%20knowledge%20navigation">structural knowledge navigation</a> with <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=semantic%20matching">semantic matching</a>. This enables
effective reasoning over heterogeneous content where relevant evidence spans
multiple modalities. <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=RAG">RAG</a>-Anything demonstrates superior performance on
challenging <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=multimodal%20benchmarks">multimodal benchmarks</a>, achieving significant improvements over
state-of-the-art methods. Performance gains become particularly pronounced on
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=long%20documents">long documents</a> where traditional approaches fail. Our framework establishes a
new paradigm for <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=multimodal">multimodal</a> knowledge access, eliminating the architectural
f<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=rag">rag</a>mentation that constrains current systems. Our framework is open-sourced
at: https://github.com/HKUDS/<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=RAG">RAG</a>-Anything.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="MONTHLY" data-date="2025-09-29" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2509.25454" target="_blank" onclick="event.stopPropagation()">DeepSearch: Overcome the Bottleneck of Reinforcement Learning with
  Verifiable Rewards via Monte Carlo Tree Search</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-09-29</div>
                                        <div class="tags"><span class="tag">MONTHLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2509.25454" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <div class="link-item" style="background: #f5f5f5; color: #999;">No GitHub</div>
                                    <div class="link-item upvote-item">üî• 133</div>
                                    <div class="link-item stars-item">‚≠ê 0</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">DeepSearch integrates Monte Carlo Tree Search into RLVR training to enhance exploration and credit assignment, achieving state-of-the-art performance with reduced computational cost.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Although <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=RLVR">RLVR</a> has become an essential component for developing advanced
reasoning skills in LLMs, contemporary studies have documented training
plateaus that emerge following thousands of optimization steps, demonstrating
notable decreases in performance gains despite increased computational
investment. This limitation stems from the sparse exploration patterns inherent
in current <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=RLVR">RLVR</a> practices, where models rely on limited rollouts that often
miss critical reasoning paths and fail to provide systematic coverage of the
solution space. We present DeepSearch, a framework that integrates Monte Carlo
Tree Search directly into <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=RLVR">RLVR</a> training. In contrast to existing methods that
rely on tree search only at inference, DeepSearch embeds structured search into
the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=training%20loop">training loop</a>, enabling <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=systematic%20exploration">systematic exploration</a> and fine-grained credit
assignment across reasoning steps. Through training-time exploration,
DeepSearch addresses the fundamental bottleneck of insufficient exploration,
which leads to diminishing performance improvements over prolonged training
steps. Our contributions include: (1) a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=global%20frontier%20selection">global frontier selection</a> strategy that
prioritizes promising nodes across the search tree, (2) selection with
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=entropy-based%20guidance">entropy-based guidance</a> that identifies confident paths for supervision, and (3)
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=adaptive%20replay%20buffer">adaptive replay buffer</a> training with <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=solution%20caching">solution caching</a> for efficiency.
Experiments on <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=mathematical%20reasoning%20benchmarks">mathematical reasoning benchmarks</a> show that DeepSearch achieves
62.95% average accuracy and establishes a new state-of-the-art for 1.5B
reasoning models - using 5.7x fewer GPU hours than extended training
approaches. These results highlight the importance of strategic exploration
over brute-force scaling and demonstrate the promise of algorithmic innovation
for advancing <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=RLVR">RLVR</a> methodologies. DeepSearch establishes a new direction for
scaling reasoning capabilities through systematic search rather than prolonged
computation.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="MONTHLY" data-date="2025-10-07" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2510.05684" target="_blank" onclick="event.stopPropagation()">D2E: Scaling Vision-Action Pretraining on Desktop Data for Transfer to
  Embodied AI</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-10-07</div>
                                        <div class="tags"><span class="tag">MONTHLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2510.05684" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/worv-ai/D2E" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 129</div>
                                    <div class="link-item stars-item">‚≠ê 47</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">D2E framework uses desktop interactions to pretrain embodied AI, achieving high success rates in physical manipulation and navigation tasks.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Large language models leverage internet-scale text data, yet <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=embodied%20AI">embodied AI</a>
remains constrained by the prohibitive costs of physical trajectory collection.
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Desktop%20environments">Desktop environments</a> -- particularly gaming -- offer a compelling alternative:
they provide rich <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=sensorimotor%20interactions">sensorimotor interactions</a> at scale while maintaining the
structured observation-action coupling essential for embodied learning. We
present D2E (Desktop to <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Embodied%20AI">Embodied AI</a>), a framework that demonstrates desktop
interactions can serve as an effective pretraining substrate for robotics
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=embodied%20AI">embodied AI</a> tasks. Unlike prior work that remained domain-specific (e.g., VPT
for Minecraft) or kept data proprietary (e.g., SIMA), D2E establishes a
complete pipeline from scalable desktop data collection to verified transfer in
embodied domains. Our framework comprises three components: (1) the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=OWA%20Toolkit">OWA Toolkit</a>
that unifies diverse desktop interactions into a standardized format with 152x
compression, (2) the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Generalist-IDM">Generalist-IDM</a> that achieves strong zero-shot
generalization across unseen games through <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=timestamp-based%20event%20prediction">timestamp-based event prediction</a>,
enabling <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=internet-scale%20pseudo-labeling">internet-scale pseudo-labeling</a>, and (3) <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=VAPT">VAPT</a> that transfers
desktop-pretrained representations to physical manipulation and navigation.
Using 1.3K+ hours of data (259 hours of human demonstrations, and 1K+ hours of
pseudo-labeled gameplay), we achieve a total of 96.6% success rate on LIBERO
manipulation and 83.3% on <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=CANVAS%20navigation">CANVAS navigation</a> benchmarks. This validates that
sensorimotor primitives in digital interactions exhibit sufficient invariance
to transfer meaningfully to physical embodied tasks, establishing desktop
pretraining as a practical paradigm for robotics. We will make all our work
public, including the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=OWA%20toolkit">OWA toolkit</a>, datasets of human-collected and
pseudo-labeled, and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=VAPT">VAPT</a>-trained models available at
https://worv-ai.github.io/d2e/
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="MONTHLY" data-date="2025-10-09" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2510.08673" target="_blank" onclick="event.stopPropagation()">Thinking with Camera: A Unified Multimodal Model for Camera-Centric
  Understanding and Generation</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-10-09</div>
                                        <div class="tags"><span class="tag">MONTHLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2510.08673" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/KangLiao929/Puffin" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 116</div>
                                    <div class="link-item stars-item">‚≠ê 219</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">Puffin, a unified multimodal model, integrates language regression and diffusion-based generation to enhance camera-centric spatial understanding and generation by treating camera parameters as language.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Camera-centric">Camera-centric</a> understanding and generation are two cornerstones of spatial
intelligence, yet they are typically studied in isolation. We present Puffin, a
unified <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=camera-centric">camera-centric</a> <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=multimodal%20model">multimodal model</a> that extends <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=spatial%20awareness">spatial awareness</a> along
the camera dimension. Puffin integrates <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=language%20regression">language regression</a> and diffusion-based
generation to interpret and create scenes from arbitrary viewpoints. To bridge
the modality gap between cameras and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=vision-language">vision-language</a>, we introduce a novel
paradigm that treats <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=camera%20as%20language">camera as language</a>, enabling thinking with camera. This
guides the model to align spatially grounded visual cues with photographic
terminology while reasoning across <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=geometric%20context">geometric context</a>. Puffin is trained on
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Puffin-4M">Puffin-4M</a>, a large-scale dataset of 4 million <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=vision-language">vision-language</a>-camera triplets.
We incorporate both <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=global%20camera%20parameters">global camera parameters</a> and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=pixel-wise%20camera%20maps">pixel-wise camera maps</a>,
yielding flexible and reliable <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=spatial%20generation">spatial generation</a>. Experiments demonstrate
Puffin superior performance over specialized models for <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=camera-centric">camera-centric</a>
generation and understanding. With <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=instruction%20tuning">instruction tuning</a>, Puffin generalizes to
diverse cross-view tasks such as <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=spatial%20imagination">spatial imagination</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=world%20exploration">world exploration</a>, and
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=photography%20guidance">photography guidance</a>. We will release the code, models, dataset pipeline, and
benchmark to advance multimodal spatial intelligence research.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="TRENDING" data-date="2025-10-22" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2510.19808" target="_blank" onclick="event.stopPropagation()">Pico-Banana-400K: A Large-Scale Dataset for Text-Guided Image Editing</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-10-22</div>
                                        <div class="tags"><span class="tag">TRENDING</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2510.19808" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/apple/pico-banana-400k" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 16</div>
                                    <div class="link-item stars-item">‚≠ê 104</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">Pico-Banana-400K is a large-scale, high-quality dataset for instruction-based image editing, featuring diverse edit pairs, multi-turn editing, preference subsets, and long-short instruction pairs, enabling comprehensive research and benchmarking.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Recent advances in <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=multimodal%20models">multimodal models</a> have demonstrated remarkable text-guided
image editing capabilities, with systems like <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=GPT-4o">GPT-4o</a> and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Nano-Banana">Nano-Banana</a> setting
new benchmarks. However, the research community's progress remains constrained
by the absence of large-scale, high-quality, and openly accessible datasets
built from real images. We introduce Pico-Banana-400K, a comprehensive
400K-image dataset for instruction-based image editing. Our dataset is
constructed by leveraging <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Nano-Banana">Nano-Banana</a> to generate diverse edit pairs from real
photographs in the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=OpenImages">OpenImages</a> collection. What distinguishes Pico-Banana-400K
from previous synthetic datasets is our systematic approach to quality and
diversity. We employ a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=fine-grained%20image%20editing%20taxonomy">fine-grained image editing taxonomy</a> to ensure
comprehensive coverage of edit types while maintaining precise content
preservation and instruction faithfulness through <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=MLLM-based%20quality%20scoring">MLLM-based quality scoring</a>
and careful curation. Beyond single turn editing, Pico-Banana-400K enables
research into complex editing scenarios. The dataset includes three specialized
subsets: (1) a 72K-example multi-turn collection for studying sequential
editing, reasoning, and planning across consecutive modifications; (2) a
56K-example <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=preference%20subset">preference subset</a> for alignment research and reward model training;
and (3) paired long-short editing instructions for developing instruction
rewriting and summarization capabilities. By providing this large-scale,
high-quality, and task-rich resource, Pico-Banana-400K establishes a robust
foundation for training and benchmarking the next generation of text-guided
image editing models.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="TRENDING" data-date="2025-09-30" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2510.01284" target="_blank" onclick="event.stopPropagation()">Ovi: Twin Backbone Cross-Modal Fusion for Audio-Video Generation</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-09-30</div>
                                        <div class="tags"><span class="tag">TRENDING</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2510.01284" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/character-ai/Ovi" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 30</div>
                                    <div class="link-item stars-item">‚≠ê 955</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">Ovi is a unified audio-video generation model using twin-DiT modules with blockwise cross-modal fusion, enabling natural synchronization and high-quality multimodal outputs.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Audio-video generation has often relied on complex multi-stage architectures
or sequential synthesis of sound and visuals. We introduce Ovi, a unified
paradigm for audio-video generation that models the two modalities as a single
generative process. By using <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=blockwise%20cross-modal%20fusion">blockwise cross-modal fusion</a> of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=twin-DiT%20modules">twin-DiT modules</a>,
Ovi achieves natural synchronization and removes the need for separate
pipelines or post hoc alignment. To facilitate fine-grained multimodal fusion
modeling, we initialize an audio tower with an architecture identical to that
of a strong pretrained video model. Trained from scratch on hundreds of
thousands of hours of raw audio, the audio tower learns to generate realistic
sound effects, as well as speech that conveys rich speaker identity and
emotion. Fusion is obtained by jointly training the identical video and audio
towers via blockwise exchange of timing (via <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=scaled-RoPE%20embeddings">scaled-RoPE embeddings</a>) and
semantics (through <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=bidirectional%20cross-attention">bidirectional cross-attention</a>) on a vast video corpus. Our
model enables <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=cinematic%20storytelling">cinematic storytelling</a> with natural speech and accurate,
context-matched sound effects, producing <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=movie-grade%20video%20clips">movie-grade video clips</a>. All the
demos, code and model weights are published at https://aaxwaz.github.io/Ovi
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="MONTHLY" data-date="2025-10-01" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2510.01141" target="_blank" onclick="event.stopPropagation()">Apriel-1.5-15b-Thinker</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-10-01</div>
                                        <div class="tags"><span class="tag">MONTHLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2510.01141" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <div class="link-item" style="background: #f5f5f5; color: #999;">No GitHub</div>
                                    <div class="link-item upvote-item">üî• 110</div>
                                    <div class="link-item stars-item">‚≠ê 0</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">A 15-billion parameter multimodal reasoning model achieves competitive performance through a progressive training methodology without reinforcement learning, demonstrating efficient use of computational resources.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                We present Apriel-1.5-15B-Thinker, a 15-billion parameter open-weights
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=multimodal%20reasoning%20model">multimodal reasoning model</a> that achieves frontier-level performance through
training design rather than sheer scale. Starting from Pixtral-12B, we apply a
progressive three-stage methodology: (1) <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=depth%20upscaling">depth upscaling</a> to expand reasoning
capacity without pretraining from scratch, (2) <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=staged%20continual%20pre-training">staged continual pre-training</a>
that first develops foundational text and vision understanding, then enhances
visual reasoning through targeted <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=synthetic%20data%20generation">synthetic data generation</a> addressing spatial
structure, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=compositional%20understanding">compositional understanding</a>, and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=fine-grained%20perception">fine-grained perception</a>, and (3)
high-quality <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=text-only%20supervised%20fine-tuning">text-only supervised fine-tuning</a> on curated instruction-response
pairs with explicit <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=reasoning%20traces">reasoning traces</a> spanning mathematics, coding, science, and
tool use. Notably, our model achieves competitive results without reinforcement
learning or preference optimization, isolating the contribution of our
data-centric continual pre-training approach. On the Artificial Analysis
Intelligence Index, Apriel-1.5-15B-Thinker attains a score of 52, matching
DeepSeek-R1-0528 despite requiring significantly fewer computational resources.
Across ten image benchmarks, its performance is on average within five points
of Gemini-2.5-Flash and Claude Sonnet-3.7, a key achievement for a model
operating within <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=single-GPU%20deployment">single-GPU deployment</a> constraints. Our results demonstrate
that thoughtful mid-training 2 design can close substantial capability gaps
without massive scale, making frontier-level multimodal reasoning accessible to
organizations with limited infrastructure. We release the model checkpoint, all
training recipes, and evaluation protocols under the MIT license to to advance
open-source research.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="TRENDING" data-date="2025-09-26" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2509.22186" target="_blank" onclick="event.stopPropagation()">MinerU2.5: A Decoupled Vision-Language Model for Efficient
  High-Resolution Document Parsing</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-09-26</div>
                                        <div class="tags"><span class="tag">TRENDING</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2509.22186" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/opendatalab/MinerU" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 121</div>
                                    <div class="link-item stars-item">‚≠ê 47.3k</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">MinerU2.5, a 1.2B-parameter document parsing vision-language model, achieves state-of-the-art recognition accuracy with computational efficiency through a coarse-to-fine parsing strategy.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                We introduce MinerU2.5, a 1.2B-parameter <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=document%20parsing">document parsing</a> vision-language
model that achieves state-of-the-art recognition accuracy while maintaining
exceptional computational efficiency. Our approach employs a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=coarse-to-fine">coarse-to-fine</a>,
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=two-stage%20parsing">two-stage parsing</a> strategy that decouples global <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=layout%20analysis">layout analysis</a> from local
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=content%20recognition">content recognition</a>. In the first stage, the model performs efficient layout
analysis on <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=downsampled%20images">downsampled images</a> to identify structural elements, circumventing
the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=computational%20overhead">computational overhead</a> of processing high-resolution inputs. In the second
stage, guided by the global layout, it performs targeted <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=content%20recognition">content recognition</a> on
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=native-resolution%20crops">native-resolution crops</a> extracted from the original image, preserving
fine-grained details in dense text, complex formulas, and tables. To support
this strategy, we developed a comprehensive <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=data%20engine">data engine</a> that generates diverse,
large-scale training corpora for both <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=pretraining">pretraining</a> and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=fine-tuning">fine-tuning</a>. Ultimately,
MinerU2.5 demonstrates strong <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=document%20parsing">document parsing</a> ability, achieving
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=state-of-the-art%20performance">state-of-the-art performance</a> on multiple benchmarks, surpassing both
general-purpose and domain-specific models across various recognition tasks,
while maintaining significantly lower <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=computational%20overhead">computational overhead</a>.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="WEEKLY" data-date="2025-10-20" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2510.17681" target="_blank" onclick="event.stopPropagation()">PICABench: How Far Are We from Physically Realistic Image Editing?</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-10-20</div>
                                        <div class="tags"><span class="tag">WEEKLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2510.17681" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/Andrew0613/PICABench" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 58</div>
                                    <div class="link-item stars-item">‚≠ê 16</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">PICABench and PICAEval evaluate physical realism in image editing by assessing eight sub-dimensions and using VLM-as-a-judge with human annotations, highlighting the need for physics-based solutions.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Image%20editing">Image editing</a> has achieved remarkable progress recently. Modern editing
models could already follow complex instructions to manipulate the original
content. However, beyond completing the editing instructions, the accompanying
physical effects are the key to the generation realism. For example, removing
an object should also remove its shadow, reflections, and interactions with
nearby objects. Unfortunately, existing models and benchmarks mainly focus on
instruction completion but overlook these physical effects. So, at this moment,
how far are we from physically realistic <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=image%20editing">image editing</a>? To answer this, we
introduce <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=PICABench">PICABench</a>, which systematically evaluates <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=physical%20realism">physical realism</a> across
eight sub-dimension (spanning optics, mechanics, and state transitions) for
most of the common editing operations (add, remove, attribute change, etc). We
further propose the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=PICAEval">PICAEval</a>, a reliable evaluation protocol that uses
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=VLM-as-a-judge">VLM-as-a-judge</a> with per-case, region-level human annotations and questions.
Beyond benchmarking, we also explore effective solutions by learning physics
from videos and construct a training dataset <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=PICA-100K">PICA-100K</a>. After evaluating most
of the mainstream models, we observe that <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=physical%20realism">physical realism</a> remains a
challenging problem with large rooms to explore. We hope that our benchmark and
proposed solutions can serve as a foundation for future work moving from naive
content editing toward physically consistent realism.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="MONTHLY" data-date="2025-10-06" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2510.05096" target="_blank" onclick="event.stopPropagation()">Paper2Video: Automatic Video Generation from Scientific Papers</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-10-06</div>
                                        <div class="tags"><span class="tag">MONTHLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2510.05096" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/showlab/Paper2Video" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 106</div>
                                    <div class="link-item stars-item">‚≠ê 1.22k</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">PaperTalker is a multi-agent framework that automates academic presentation video generation by integrating slide generation, layout refinement, subtitling, speech synthesis, and talking-head rendering, outperforming existing methods.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Academic presentation videos have become an essential medium for research
communication, yet producing them remains highly labor-intensive, often
requiring hours of slide design, recording, and editing for a short 2 to 10
minutes video. Unlike natural video, presentation video generation involves
distinctive challenges: inputs from research papers, dense multi-modal
information (text, figures, tables), and the need to coordinate multiple
aligned channels such as slides, subtitles, speech, and human talker. To
address these challenges, we introduce PaperTalker, the first benchmark of 101
research papers paired with author-created presentation videos, slides, and
speaker metadata. We further design four tailored evaluation metrics--Meta
Similarity, PresentArena, PresentQuiz, and IP Memory--to measure how videos
convey the paper's information to the audience. Building on this foundation, we
propose PaperTalker, the first <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=multi-agent%20framework">multi-agent framework</a> for academic presentation
video generation. It integrates <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=slide%20generation">slide generation</a> with effective layout
refinement by a novel effective <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=tree%20search%20visual%20choice">tree search visual choice</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=cursor%20grounding">cursor grounding</a>,
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=subtitling">subtitling</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=speech%20synthesis">speech synthesis</a>, and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=talking-head%20rendering">talking-head rendering</a>, while parallelizing
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=slide-wise%20generation">slide-wise generation</a> for efficiency. Experiments on Paper2Video demonstrate
that the presentation videos produced by our approach are more faithful and
informative than existing baselines, establishing a practical step toward
automated and ready-to-use academic video generation. Our dataset, agent, and
code are available at https://github.com/showlab/Paper2Video.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="TRENDING" data-date="2025-05-20" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2505.14059" target="_blank" onclick="event.stopPropagation()">Dolphin: Document Image Parsing via Heterogeneous Anchor Prompting</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-05-20</div>
                                        <div class="tags"><span class="tag">TRENDING</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2505.14059" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/bytedance/dolphin" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 2</div>
                                    <div class="link-item stars-item">‚≠ê 7.57k</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">Dolphin, a multimodal document image parsing model, uses heterogeneous anchor prompting to achieve state-of-the-art performance on diverse page-level and element-level tasks through an efficient analyze-then-parse paradigm.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Document image parsing is challenging due to its complexly intertwined
elements such as text paragraphs, figures, formulas, and tables. Current
approaches either assemble specialized expert models or directly generate
page-level content autoregressively, facing integration overhead, efficiency
bottlenecks, and layout structure degradation despite their decent performance.
To address these limitations, we present Dolphin
(\textbf{Document Image Parsing via Heterogeneous
Anchor Prompting}), a novel <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=multimodal%20document%20image%20parsing">multimodal document image parsing</a> model
following an <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=analyze-then-parse">analyze-then-parse</a> paradigm. In the first stage, Dolphin generates
a sequence of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=layout%20elements">layout elements</a> in reading order. These heterogeneous elements,
serving as anchors and coupled with task-specific prompts, are fed back to
Dolphin for <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=parallel%20content%20parsing">parallel content parsing</a> in the second stage. To train Dolphin, we
construct a large-scale dataset of over 30 million samples, covering
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=multi-granularity%20parsing%20tasks">multi-granularity parsing tasks</a>. Through comprehensive evaluations on both
prevalent benchmarks and self-constructed ones, Dolphin achieves
state-of-the-art performance across diverse page-level and element-level
settings, while ensuring superior efficiency through its lightweight
architecture and parallel parsing mechanism. The code and pre-trained models
are publicly available at https://github.com/ByteDance/Dolphin
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="WEEKLY" data-date="2025-10-21" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2510.18855" target="_blank" onclick="event.stopPropagation()">Every Step Evolves: Scaling Reinforcement Learning for Trillion-Scale
  Thinking Model</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-10-21</div>
                                        <div class="tags"><span class="tag">WEEKLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2510.18855" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/inclusionAI/Ring-V2" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 53</div>
                                    <div class="link-item stars-item">‚≠ê 55</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">Ring-1T, a trillion-parameter open-source thinking model, addresses training challenges with IcePop, C3PO++, and ASystem, achieving top results across benchmarks and democratizing large-scale reasoning intelligence.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                We present Ring-1T, the first open-source, state-of-the-art thinking model
with a trillion-scale parameter. It features 1 trillion total parameters and
activates approximately 50 billion per token. Training such models at a
trillion-parameter scale introduces unprecedented challenges, including
train-inference misalignment, inefficiencies in <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=rollout%20processing">rollout processing</a>, and
bottlenecks in the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=RL%20system">RL system</a>. To address these, we pioneer three interconnected
innovations: (1) <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=IcePop">IcePop</a> stabilizes RL training via token-level discrepancy
masking and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=clipping">clipping</a>, resolving instability from training-inference mismatches;
(2) <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=C3PO%2B%2B">C3PO++</a> improves <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=resource%20utilization">resource utilization</a> for long rollouts under a token budget
by dynamically partitioning them, thereby obtaining high time efficiency; and
(3) <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=ASystem">ASystem</a>, a high-performance RL framework designed to overcome the systemic
bottlenecks that impede <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=trillion-parameter%20model">trillion-parameter model</a> training. Ring-1T delivers
breakthrough results across critical benchmarks: 93.4 on <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=AIME-2025">AIME-2025</a>, 86.72 on
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=HMMT-2025">HMMT-2025</a>, 2088 on <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=CodeForces">CodeForces</a>, and 55.94 on <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=ARC-AGI-v1">ARC-AGI-v1</a>. Notably, it attains a
silver medal-level result on the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=IMO-2025">IMO-2025</a>, underscoring its exceptional
reasoning capabilities. By releasing the complete 1T parameter <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=MoE">MoE</a> model to the
community, we provide the research community with direct access to cutting-edge
reasoning capabilities. This contribution marks a significant milestone in
democratizing large-scale reasoning intelligence and establishes a new baseline
for open-source model performance.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="TRENDING" data-date="2025-10-17" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2510.15742" target="_blank" onclick="event.stopPropagation()">Scaling Instruction-Based Video Editing with a High-Quality Synthetic
  Dataset</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-10-17</div>
                                        <div class="tags"><span class="tag">TRENDING</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2510.15742" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/EzioBy/Ditto" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 46</div>
                                    <div class="link-item stars-item">‚≠ê 273</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">Ditto framework addresses data scarcity in instruction-based video editing by generating a large dataset and using a curriculum learning strategy to train Editto, achieving superior instruction-following ability.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Instruction-based video editing promises to democratize content creation, yet
its progress is severely hampered by the scarcity of large-scale, high-quality
training data. We introduce Ditto, a holistic framework designed to tackle this
fundamental challenge. At its heart, Ditto features a novel data generation
pipeline that fuses the creative diversity of a leading <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=image%20editor">image editor</a> with an
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=in-context%20video%20generator">in-context video generator</a>, overcoming the limited scope of existing models. To
make this process viable, our framework resolves the prohibitive cost-quality
trade-off by employing an efficient, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=distilled%20model%20architecture">distilled model architecture</a> augmented by
a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=temporal%20enhancer">temporal enhancer</a>, which simultaneously reduces computational overhead and
improves <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=temporal%20coherence">temporal coherence</a>. Finally, to achieve full scalability, this entire
pipeline is driven by an <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=intelligent%20agent">intelligent agent</a> that crafts diverse instructions and
rigorously filters the output, ensuring quality control at scale. Using this
framework, we invested over 12,000 GPU-days to build Ditto-1M, a new dataset of
one million high-fidelity video editing examples. We trained our model, Editto,
on Ditto-1M with a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=curriculum%20learning%20strategy">curriculum learning strategy</a>. The results demonstrate
superior <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=instruction-following%20ability">instruction-following ability</a> and establish a new state-of-the-art in
instruction-based video editing.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="WEEKLY" data-date="2025-10-20" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2510.17800" target="_blank" onclick="event.stopPropagation()">Glyph: Scaling Context Windows via Visual-Text Compression</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-10-20</div>
                                        <div class="tags"><span class="tag">WEEKLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2510.17800" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <div class="link-item" style="background: #f5f5f5; color: #999;">No GitHub</div>
                                    <div class="link-item upvote-item">üî• 53</div>
                                    <div class="link-item stars-item">‚≠ê 0</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">Glyph compresses long textual inputs into images using vision-language models, achieving significant token compression and improved performance in long-context tasks.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Large%20language%20models">Large language models</a> (LLMs) increasingly rely on <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=long-context%20modeling">long-context modeling</a> for
tasks such as <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=document%20understanding">document understanding</a>, code analysis, and multi-step reasoning.
However, scaling context windows to the million-token level brings prohibitive
computational and memory costs, limiting the practicality of long-context LLMs.
In this work, we take a different perspective-visual context scaling-to tackle
this challenge. Instead of extending <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=token-based%20sequences">token-based sequences</a>, we propose <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Glyph">Glyph</a>, a
framework that renders long texts into images and processes them with
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=vision-language%20models">vision-language models</a> (VLMs). This approach substantially compresses textual
input while preserving semantic information, and we further design an
LLM-driven <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=genetic%20search">genetic search</a> to identify optimal visual rendering configurations
for balancing accuracy and compression. Through extensive experiments, we
demonstrate that our method achieves 3-4x <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=token%20compression">token compression</a> while maintaining
accuracy comparable to leading LLMs such as Qwen3-8B on various long-context
benchmarks. This compression also leads to around 4x faster <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=prefilling">prefilling</a> and
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=decoding">decoding</a>, and approximately 2x faster <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=SFT%20training">SFT training</a>. Furthermore, under extreme
compression, a 128K-context VLM could scale to handle 1M-token-level text
tasks. In addition, the rendered text data benefits real-world multimodal
tasks, such as <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=document%20understanding">document understanding</a>. Our code and model are released at
https://github.com/thu-coai/<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Glyph">Glyph</a>.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="MONTHLY" data-date="2025-10-06" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2510.04849" target="_blank" onclick="event.stopPropagation()">When Models Lie, We Learn: Multilingual Span-Level Hallucination
  Detection with PsiloQA</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-10-06</div>
                                        <div class="tags"><span class="tag">MONTHLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2510.04849" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/s-nlp/PsiloQA" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 106</div>
                                    <div class="link-item stars-item">‚≠ê 10</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">PsiloQA, a multilingual dataset with span-level hallucinations, enhances hallucination detection in large language models across 14 languages using an automated pipeline and encoder-based models.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Hallucination%20detection">Hallucination detection</a> remains a fundamental challenge for the safe and
reliable deployment of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=large%20language%20models">large language models</a> (LLMs), especially in applications
requiring factual accuracy. Existing hallucination benchmarks often operate at
the sequence level and are limited to English, lacking the fine-grained,
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=multilingual">multilingual</a> supervision needed for a comprehensive evaluation. In this work,
we introduce <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=PsiloQA">PsiloQA</a>, a large-scale, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=multilingual">multilingual</a> dataset annotated with
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=span-level%20hallucinations">span-level hallucinations</a> across 14 languages. <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=PsiloQA">PsiloQA</a> is constructed through
an automated three-stage pipeline: generating question-answer pairs from
Wikipedia using <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=GPT-4o">GPT-4o</a>, eliciting potentially hallucinated answers from diverse
LLMs in a no-context setting, and automatically annotating hallucinated spans
using <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=GPT-4o">GPT-4o</a> by comparing against golden answers and retrieved context. We
evaluate a wide range of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=hallucination%20detection">hallucination detection</a> methods -- including
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=uncertainty%20quantification">uncertainty quantification</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=LLM-based%20tagging">LLM-based tagging</a>, and fine-tuned <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=encoder%20models">encoder models</a> --
and show that encoder-based models achieve the strongest performance across
languages. Furthermore, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=PsiloQA">PsiloQA</a> demonstrates effective cross-lingual
generalization and supports robust <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=knowledge%20transfer">knowledge transfer</a> to other benchmarks, all
while being significantly more cost-efficient than human-annotated datasets.
Our dataset and results advance the development of scalable, fine-grained
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=hallucination%20detection">hallucination detection</a> in <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=multilingual">multilingual</a> settings.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="WEEKLY" data-date="2025-10-16" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2510.15019" target="_blank" onclick="event.stopPropagation()">NANO3D: A Training-Free Approach for Efficient 3D Editing Without Masks</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-10-16</div>
                                        <div class="tags"><span class="tag">WEEKLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2510.15019" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/JAMESYJL/Nano3D/tree/main" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 53</div>
                                    <div class="link-item stars-item">‚≠ê 56</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">Nano3D is a training-free framework that integrates FlowEdit and TRELLIS for precise 3D object editing, using front-view renderings and region-aware merging strategies to maintain structural fidelity and visual quality.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                3D object editing is essential for interactive content creation in gaming,
animation, and robotics, yet current approaches remain inefficient,
inconsistent, and often fail to preserve unedited regions. Most methods rely on
editing multi-view renderings followed by reconstruction, which introduces
artifacts and limits practicality. To address these challenges, we propose
Nano3D, a training-free framework for precise and coherent 3D object editing
without masks. Nano3D integrates <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=FlowEdit">FlowEdit</a> into <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=TRELLIS">TRELLIS</a> to perform localized
edits guided by <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=front-view%20renderings">front-view renderings</a>, and further introduces region-aware
merging strategies, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Voxel%2FSlat-Merge">Voxel/Slat-Merge</a>, which adaptively preserve structural
fidelity by ensuring consistency between edited and unedited areas. Experiments
demonstrate that Nano3D achieves superior <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=3D%20consistency">3D consistency</a> and visual quality
compared with existing methods. Based on this framework, we construct the first
large-scale <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=3D%20editing%20datasets">3D editing datasets</a> Nano3D-Edit-100k, which contains over 100,000
high-quality 3D editing pairs. This work addresses long-standing challenges in
both algorithm design and data availability, significantly improving the
generality and reliability of 3D editing, and laying the groundwork for the
development of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=feed-forward%203D%20editing%20models">feed-forward 3D editing models</a>. Project
Page:https://jamesyjl.github.io/Nano3D
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="WEEKLY" data-date="2025-10-19" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2510.16880" target="_blank" onclick="event.stopPropagation()">Chem-R: Learning to Reason as a Chemist</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-10-19</div>
                                        <div class="tags"><span class="tag">WEEKLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2510.16880" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/davidweidawang/Chem-R" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 50</div>
                                    <div class="link-item stars-item">‚≠ê 10</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">Chem-R, a three-phase trained Chemical Reasoning model, achieves superior performance on chemical tasks by integrating core knowledge, expert reasoning, and multi-task optimization.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Although large language models (LLMs) have significant potential to advance
chemical discovery, current LLMs lack core chemical knowledge, produce
unreliable reasoning trajectories, and exhibit suboptimal performance across
diverse chemical tasks. To address these challenges, we propose <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Chem-R">Chem-R</a>, a
generalizable Chemical Reasoning model designed to emulate the deliberative
processes of chemists. <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Chem-R">Chem-R</a> is trained through a three-phase framework that
progressively builds advanced reasoning capabilities, including: 1) Chemical
Foundation Training, which establishes core chemical knowledge. 2) Chemical
Reasoning Protocol Distillation, incorporating structured, expert-like
reasoning traces to guide systematic and reliable problem solving. 3)
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Multi-task%20Group%20Relative%20Policy%20Optimization">Multi-task Group Relative Policy Optimization</a> that optimizes the model for
balanced performance across diverse molecular- and reaction-level tasks. This
structured pipeline enables <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Chem-R">Chem-R</a> to achieve <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=state-of-the-art%20performance">state-of-the-art performance</a> on
comprehensive benchmarks, surpassing leading large language models, including
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Gemini-2.5-Pro">Gemini-2.5-Pro</a> and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=DeepSeek-R1">DeepSeek-R1</a>, by up to 46% on <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=molecular%20tasks">molecular tasks</a> and 66% on
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=reaction%20tasks">reaction tasks</a>. Meanwhile, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Chem-R">Chem-R</a> also consistently outperforms the existing
chemical foundation models across both molecular and reaction level tasks.
These results highlight <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Chem-R">Chem-R</a>'s <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=robust%20generalization">robust generalization</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=interpretability">interpretability</a>, and
potential as a foundation for next-generation AI-driven chemical discovery.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="WEEKLY" data-date="2025-10-20" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2510.17269" target="_blank" onclick="event.stopPropagation()">FineVision: Open Data Is All You Need</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-10-20</div>
                                        <div class="tags"><span class="tag">WEEKLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2510.17269" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <div class="link-item" style="background: #f5f5f5; color: #999;">No GitHub</div>
                                    <div class="link-item upvote-item">üî• 47</div>
                                    <div class="link-item stars-item">‚≠ê 0</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">FineVision, a large-scale and curated dataset, enhances vision-language models through rigorous data collection, de-duplication, and human oversight, leading to improved performance.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                The advancement of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=vision-language%20models">vision-language models</a> (VLMs) is hampered by a fragmented
landscape of inconsistent and contaminated public datasets. We introduce
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=FineVision">FineVision</a>, a meticulously collected, curated, and unified corpus of 24 million
samples - the largest open resource of its kind. We unify more than 200 sources
into 185 subsets via a semi-automated, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=human-in-the-loop">human-in-the-loop</a> pipeline: automation
performs bulk ingestion and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=schema%20mapping">schema mapping</a>, while reviewers audit mappings and
spot-check outputs to verify faithful consumption of annotations, appropriate
formatting and diversity, and safety; issues trigger targeted fixes and
re-runs. The workflow further applies rigorous <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=de-duplication">de-duplication</a> within and across
sources and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=decontamination">decontamination</a> against 66 public benchmarks. <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=FineVision">FineVision</a> also
encompasses <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=agentic%2FGUI%20tasks">agentic/GUI tasks</a> with a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=unified%20action%20space">unified action space</a>; reviewers validate
schemas and inspect a sample of trajectories to confirm executable fidelity.
Models trained on <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=FineVision">FineVision</a> consistently outperform those trained on existing
open mixtures across a broad evaluation suite, underscoring the benefits of
scale, data hygiene, and balanced automation with human oversight. We release
the corpus and curation tools to accelerate data-centric VLM research.
                            </div>
                        </div>
                    </div>
                </div>
                </div>
                        </div>
                    </section>
                </div>
            </div>
        </main>

        <!-- Footer -->
        <footer class="footer">
            <div class="container">
                <p>&copy; 2025 Daily Digest. Curated with ‚ù§Ô∏è for the curious minds.</p>
            </div>
        </footer>
        <script src="static/script.js"></script>
    </body>
    </html>
    