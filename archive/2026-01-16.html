
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <title>Daily Digest</title>
        <link rel="stylesheet" href="static/style.css">
    </head>
    <body>
        <!-- Header -->
        <header class="header">
            <div class="container">
                <nav class="nav">
                    <div class="logo">üì∞ Daily Digest</div>
                    <ul class="nav-links">
                        <li><a href="#astronomy">Astronomy</a></li>
                        <li><a href="#earth">Earth</a></li>
                        <li><a href="#tarot">Tarot</a></li>
                        <li><a href="#tech">Tech News</a></li>
                        <li><a href="#papers">Papers</a></li>
                    </ul>
                    <div class="date-badge">Fri, Jan 16, 2026</div>
                </nav>
            </div>
        </header>

        <!-- Main Content -->
        <main class="main-content">
            <div class="container">
                <!-- Hero Section -->
                <section class="hero">
                    <h1>Your Daily Tech & Science Digest</h1>
                    <p>Stay updated with the latest in astronomy, earth sciences, technology news, and cutting-edge research papers</p>
                </section>

                <!-- Content Grid -->
                <div class="content-grid">
                    <!-- APOD, Earth Observatory, and Tarot Row -->
                    <div class="three-column">
                        <!-- APOD Section -->
                        <section id="astronomy" class="card apod-card">
                            <div class="card-media" id="apod-media"></div>
                            <div class="card-content-wrapper">
                                <div class="card-header">
                                    <div class="card-icon">‚ú®</div>
                                    <div>
                                        <h2 class="card-title">Astronomy Picture</h2>
                                        <p class="card-subtitle">NASA's daily cosmic wonder</p>
                                    </div>
                                </div>
                                <div class="card-content" id="apod-content">
                                    
        <div style='display: flex; flex-direction: column; gap: 20px;'>
            <div style='display: none;'>
                <img src="https://apod.nasa.gov/apod/image/2601/2025-12-29-1656_Plato_Lmorr1024.jpg" alt="APOD" class="apod-media-element">
            </div>
            <div>
                <h3>Plato and the Lunar Alps</h3>
                <p><b> Explanation: </b> 

The dark-floored, 95 kilometer wide crater Plato and sunlit peaks of the
<a href="https://lroc.im-ldi.com/images/714">lunar Alps</a>
(Montes Alpes) are highlighted in this
<a href="https://app.astrobin.com/u/Luigi_morrone_1979?i=okquoj#gallery">this
sharp telescopic snapshot</a> of the Moon's surface.

While the <a href="http://en.wikipedia.org/wiki/Alps">Alps</a>
of planet Earth were uplifted over millions of
years as continental plates slowly collided, the lunar Alps were likely
formed by a sudden collision that created the giant
<a href="http://www.lpi.usra.edu/lunar/missions/orbiter/lunar_orbiter/impact_basin/">impact basin</a>
known as the <a href="https://apod.nasa.gov/apod/ap001228.html">Mare Imbrium</a> or Sea of Rains.

The mare's generally smooth, lava-flooded floor is seen
below the bordering mountain range.

The prominent straight feature cutting through the mountains
is the <a href="https://lroc.im-ldi.com/images/55">lunar Alpine Valley</a> (Vallis Alpes).

Joining the Mare Imbrium and northern Mare Frigoris (Sea of Cold)
the valley extends toward the upper right, about 160 kilometers long
and up to 10 kilometers wide.

Of course, the large, bright
<a href="https://en.wikipedia.org/wiki/Mont_Blanc_(Moon)">lunar alpine</a>
mountain below and right of Plato crater is named
<a href="https://science.nasa.gov/earth/earth-observatory/mt-blanc-3620/">Mont Blanc</a>.

Lacking an atmosphere, not to mention <a href="https://apod.nasa.gov/apod/ap050224.html">snow</a>,
the lunar Alps are probably not an ideal location for a winter
<a href="https://apod.nasa.gov/apod/ap010713.html">vacation</a>.

Still, a 150 pound skier
<a href="http://www.exploratorium.edu/ronh/weight/index.html">would
weigh</a> a mere 25 pounds
<a href="https://www.nasa.gov/feature/artemis/">on the Moon</a>.

<br><b> Tomorrow's picture: </b>pixels in space</p>
            </div>
        </div>
        
                                </div>
                            </div>
                        </section>

                        <!-- Earth Observatory -->
                        <section id="earth" class="card eo-card">
                            <div class="card-media" id="eo-media"></div>
                            <div class="card-content-wrapper">
                                <div class="card-header">
                                    <div class="card-icon">üåç</div>
                                    <div>
                                        <h2 class="card-title">Earth Observatory</h2>
                                        <p class="card-subtitle">Our planet from above</p>
                                    </div>
                                </div>
                                <div class="card-content" id="eo-content">
                                    
        <div style='display: flex; flex-direction: column; gap: 20px;'>
            <div style='display: none;'>
                <img src='https://assets.science.nasa.gov/dynamicimage/assets/science/esd/eo/images/iotd/2026/clouds-swimming-over-lago-argentino/ISS074-E-8940_th.jpg?fit=clip&crop=faces%2Cfocalpoint&w=300' alt='Earth Observatory' class='eo-media-element'>
            </div>
            <div>
                <h3>Clouds Swimming over Lago Argentino</h3>
                <p>A collection of fish-shaped clouds hovered above the glacial lake in Patagonia in December 2025. <a href='https://science.nasa.gov/earth/earth-observatory/clouds-swimming-over-lago-argentino/' target='_blank'>[Read more]</a></p>
                <div style='background: #f0f8f0; padding: 15px; border-radius: 8px; border-left: 4px solid #2d5016; margin-top: 20px;'>
                    <p>
                        <strong>ü§ñ AI Summary:</strong> No summary available
                    </p>
                </div>
            </div>
        </div>
        
                                </div>
                            </div>
                        </section>

                        <!-- Tarot Section -->
                        <section id="tarot" class="card tarot-card">
                            <div class="card-media" id="tarot-media"></div>
                            <div class="card-content-wrapper">
                                <div class="card-header">
                                    <div class="card-icon">üîÆ</div>
                                    <div>
                                        <h2 class="card-title">Daily Tarot</h2>
                                        <p class="card-subtitle">Your mystical guidance</p>
                                    </div>
                                </div>
                                <div class="card-content" id="tarot-content">
                                    
        <div style='display: flex; flex-direction: column; gap: 20px;'>
            <div style='display: none;'>
                <div class='tarot-card-container tarot-media-element'>
                    <div class='tarot-card' onclick='this.style.transform = this.style.transform.includes("rotateY(180deg)") ? "rotateY(0deg)" : "rotateY(180deg)"'>
                        <div>
                            <div style='font-size: 2.5rem; color: #d4af37; text-align: center; line-height: 1.2;'>
                                üîÆ<br>
                                <span style='font-size: 0.8rem; letter-spacing: 2px; font-weight: normal;'>DAILY TAROT</span><br>
                                <span style='font-size: 0.6rem; opacity: 0.8;'>Click to Reveal</span>
                            </div>
                        </div>
                        <div>
                            <img src='https://raw.githubusercontent.com/Haus226/daily-email/refs/heads/main/tarot_cards/Pictorial_Key_to_the_Tarot_Cups_09.jpg' alt='Nine of Cups' />
                        </div>
                    </div>
                </div>
            </div>
            <div>
                <h3>Nine of Cups</h3>                
                <div style='background: linear-gradient(135deg, #ffeaa7, #fdcb6e); padding: 5px; border-radius: 12px; border-left: 4px solid #e17055; margin-bottom: 20px; box-shadow: 0 4px 15px rgba(0,0,0,0.1);'>
                    <div style='font-weight: bold; color: #2d3436; margin-bottom: 5px; font-size: 1.2rem; text-transform: uppercase; letter-spacing: 1px;'>‚ú® Core Meaning</div>
                    <p style='margin: 0; color: #2d3436; line-height: 1.5; font-size: 1.0rem;'>Concord, contentment, physical bien-√™tre; also victory, success, advantage; satisfaction for the Querent or person for whom the consultation is made.</p>
                </div>
                <div style='background: linear-gradient(135deg, #ddd6fe, #c4b5fd); padding: 5px; border-radius: 12px; border-left: 4px solid #8b5cf6; box-shadow: 0 4px 15px rgba(0,0,0,0.1);'>
                    <div style='font-weight: bold; color: #2d3436; margin-bottom: 5px; font-size: 1.4rem; text-transform: uppercase; letter-spacing: 1px;'>üîç Daily Guidance</div>
                    <p style='margin: 0; color: #2d3436; line-height: 1.6; font-size: 1.3rem; font-weight: 500;'>Trust in your inner wisdom today.</p>
                </div>
    
            </div>
        </div>
        
                                </div>
                            </div>
                        </section>
                        
                        <!-- Navigation Arrows -->
                        <button class="card-nav prev" onclick="prevCard()" aria-label="Previous card">‚Äπ</button>
                        <button class="card-nav next" onclick="nextCard()" aria-label="Next card">‚Ä∫</button>
                    </div>

                    <!-- Hacker News Section -->
                    <section id="tech" class="card hn-card featured-section">
                        <div class="card-header">
                            <div class="card-icon">üî•</div>
                            <div>
                                <h2 class="card-title">Hacker News Top 10</h2>
                                <p class="card-subtitle">What's trending in tech</p>
                            </div>
                        </div>
                        <div class="card-content">
                            <ol class='hn-list'><li><a href='https://www.culpium.com/p/exclusiveapple-is-fighting-for-tsmc' target='_blank'>Apple is fighting for TSMC capacity as Nvidia takes center stage</a></li><li><a href='https://kyutai.org/blog/2026-01-13-pocket-tts' target='_blank'>Pocket TTS: A high quality TTS that gives your CPU a voice</a></li><li><a href='https://hackernoon.com/the-long-now-of-the-web-inside-the-internet-archives-fight-against-forgetting' target='_blank'>Inside The Internet Archive's Infrastructure</a></li><li><a href='https://shellbox.dev/' target='_blank'>Linux boxes via SSH: suspended when disconected</a></li><li><a href='https://briarproject.org/manual/fa/' target='_blank'>Briar keeps Iran connected via Bluetooth and Wi-Fi when the internet goes dark</a></li><li><a href='https://news.ycombinator.com/item?id=46635345' target='_blank'>Ask HN: How can we solve the loneliness epidemic?</a></li><li><a href='https://github.com/juicedata/juicefs' target='_blank'>JuiceFS is a distributed POSIX file system built on top of Redis and S3</a></li><li><a href='https://buttondown.com/hillelwayne/archive/my-gripes-with-prolog/' target='_blank'>My Gripes with Prolog</a></li><li><a href='https://github.com/syncguy/go-legacy-winxp/tree/winxp-compat' target='_blank'>Go-legacy-winxp: Compile Golang 1.24 code for Windows XP</a></li><li><a href='https://frontierai.substack.com/p/data-is-your-only-moat' target='_blank'>Data is the only moat</a></li></ol>
                        </div>
                    </section>

                    <!-- Hugging Face Papers -->
                    <section id="papers" class="card hf-card-container featured-section">
                        <div class="card-header">
                            <div class="card-icon">üìö</div>
                            <div>
                                <h2 class="card-title">Latest Research Papers</h2>
                                <p class="card-subtitle">Cutting-edge AI & ML research</p>
                            </div>
                        </div>
                        
                        <!-- Paper Filters -->
                        <div class="paper-filters" id="hf-filters">
                            <button class="filter-btn active" data-tag="ALL" onclick="filterPapers('ALL')">All Papers</button>
                            <button class="filter-btn" data-tag="DAILY" onclick="filterPapers('DAILY')">Daily</button>
                            <button class="filter-btn" data-tag="WEEKLY" onclick="filterPapers('WEEKLY')">Weekly</button>
                            <button class="filter-btn" data-tag="MONTHLY" onclick="filterPapers('MONTHLY')">Monthly</button>
                            <button class="filter-btn" data-tag="TRENDING" onclick="filterPapers('TRENDING')">Trending</button>
                        </div>

                        <!-- Papers Content -->
                        <div class="card-content">
                            
        <div id="hf-grid" class="papers-grid">
        
                <div class="paper-card hf-card flip-card" data-tags="DAILY WEEKLY TRENDING MONTHLY" data-date="2026-01-12" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2601.07348" target="_blank" onclick="event.stopPropagation()">Controlled Self-Evolution for Algorithmic Code Optimization</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2026-01-12</div>
                                        <div class="tags"><span class="tag">DAILY</span><span class="tag">MONTHLY</span><span class="tag">TRENDING</span><span class="tag">WEEKLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2601.07348" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/QuantaAlpha/EvoControl" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 95</div>
                                    <div class="link-item stars-item">‚≠ê 79</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">Controlled Self-Evolution method improves code generation through diversified initialization, feedback-guided genetic evolution, and hierarchical memory to enhance exploration efficiency and solution quality.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Self-evolution%20methods">Self-evolution methods</a> enhance code generation through iterative "generate-verify-refine" cycles, yet existing approaches suffer from low <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=exploration%20efficiency">exploration efficiency</a>, failing to discover solutions with superior complexity within limited budgets. This inefficiency stems from <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=initialization%20bias">initialization bias</a> trapping evolution in poor solution regions, uncontrolled <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=stochastic%20operations">stochastic operations</a> lacking <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=feedback%20guidance">feedback guidance</a>, and insufficient experience utilization across tasks. To address these bottlenecks, we propose Controlled Self-Evolution (CSE), which consists of three key components. Diversified Planning Initialization generates structurally distinct algorithmic strategies for broad solution space coverage. <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Genetic%20Evolution">Genetic Evolution</a> replaces <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=stochastic%20operations">stochastic operations</a> with feedback-guided mechanisms, enabling <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=targeted%20mutation">targeted mutation</a> and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=compositional%20crossover">compositional crossover</a>. <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Hierarchical%20Evolution%20Memory">Hierarchical Evolution Memory</a> captures both successful and failed experiences at inter-task and intra-task levels. Experiments on <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=EffiBench-X">EffiBench-X</a> demonstrate that CSE consistently outperforms all baselines across various <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=LLM%20backbones">LLM backbones</a>. Furthermore, CSE achieves higher efficiency from early generations and maintains continuous improvement throughout evolution. Our code is publicly available at https://github.com/QuantaAlpha/EvoControl.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="TRENDING" data-date="2026-01-12" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2601.07372" target="_blank" onclick="event.stopPropagation()">Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2026-01-12</div>
                                        <div class="tags"><span class="tag">TRENDING</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2601.07372" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/deepseek-ai/Engram" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 15</div>
                                    <div class="link-item stars-item">‚≠ê 2.45k</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">Conditional memory via Engram module enhances Transformer models by enabling efficient knowledge lookup and improving reasoning capabilities through optimized sparsity allocation.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                While <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Mixture-of-Experts">Mixture-of-Experts</a> (MoE) scales capacity via <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=conditional%20computation">conditional computation</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Transformers">Transformers</a> lack a native primitive for <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=knowledge%20lookup">knowledge lookup</a>, forcing them to inefficiently simulate retrieval through computation. To address this, we introduce conditional memory as a complementary sparsity axis, instantiated via <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Engram">Engram</a>, a module that modernizes classic <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=N-gram%20embedding">N-gram embedding</a> for O(1) lookup. By formulating the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Sparsity%20Allocation">Sparsity Allocation</a> problem, we uncover a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=U-shaped%20scaling%20law">U-shaped scaling law</a> that optimizes the trade-off between <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=neural%20computation">neural computation</a> (MoE) and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=static%20memory">static memory</a> (<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Engram">Engram</a>). Guided by this law, we scale <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Engram">Engram</a> to 27B parameters, achieving superior performance over a strictly iso-parameter and iso-FLOPs MoE baseline. Most notably, while the memory module is expected to aid knowledge retrieval (e.g., MMLU +3.4; CMMLU +4.0), we observe even larger gains in general reasoning (e.g., BBH +5.0; ARC-Challenge +3.7) and code/math domains~(HumanEval +3.0; MATH +2.4). Mechanistic analyses reveal that <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Engram">Engram</a> relieves the backbone's early layers from static reconstruction, effectively deepening the network for complex reasoning. Furthermore, by delegating local dependencies to lookups, it frees up <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=attention%20capacity">attention capacity</a> for global context, substantially boosting <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=long-context%20retrieval">long-context retrieval</a> (e.g., Multi-Query NIAH: 84.2 to 97.0). Finally, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Engram">Engram</a> establishes infrastructure-aware efficiency: its <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=deterministic%20addressing">deterministic addressing</a> enables <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=runtime%20prefetching">runtime prefetching</a> from host memory, incurring negligible overhead. We envision conditional memory as an indispensable modeling primitive for next-generation sparse models.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="MONTHLY" data-date="2025-12-31" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2512.24880" target="_blank" onclick="event.stopPropagation()">mHC: Manifold-Constrained Hyper-Connections</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-12-31</div>
                                        <div class="tags"><span class="tag">MONTHLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2512.24880" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <div class="link-item" style="background: #f5f5f5; color: #999;">No GitHub</div>
                                    <div class="link-item upvote-item">üî• 252</div>
                                    <div class="link-item stars-item">‚≠ê 0</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">Manifold-Constrained Hyper-Connections (mHC) stabilize and scale residual connection architectures by restoring identity mapping properties through manifold projection and infrastructure optimization.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Recently, studies exemplified by <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Hyper-Connections%20(HC)">Hyper-Connections (HC)</a> have extended the ubiquitous residual connection paradigm established over the past decade by expanding the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=residual%20stream%20width">residual stream width</a> and diversifying <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=connectivity%20patterns">connectivity patterns</a>. While yielding substantial performance gains, this diversification fundamentally compromises the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=identity%20mapping%20property">identity mapping property</a> intrinsic to the residual connection, which causes severe <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=training%20instability">training instability</a> and restricted <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=scalability">scalability</a>, and additionally incurs notable <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=memory%20access%20overhead">memory access overhead</a>. To address these challenges, we propose <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Manifold-Constrained%20Hyper-Connections%20(mHC)">Manifold-Constrained Hyper-Connections (mHC)</a>, a general framework that projects the residual connection space of HC onto a specific manifold to restore the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=identity%20mapping%20property">identity mapping property</a>, while incorporating rigorous <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=infrastructure%20optimization">infrastructure optimization</a> to ensure efficiency. Empirical experiments demonstrate that mHC is effective for training at scale, offering tangible performance improvements and superior <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=scalability">scalability</a>. We anticipate that mHC, as a flexible and practical extension of HC, will contribute to a deeper understanding of topological architecture design and suggest promising directions for the evolution of foundational models.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="WEEKLY MONTHLY" data-date="2026-01-11" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2601.06943" target="_blank" onclick="event.stopPropagation()">Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2026-01-11</div>
                                        <div class="tags"><span class="tag">MONTHLY</span><span class="tag">WEEKLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2601.06943" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/QuantaAlpha/VideoDR-Benchmark" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 199</div>
                                    <div class="link-item stars-item">‚≠ê 80</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">VideoDR benchmark enables video question answering by combining cross-frame visual extraction, web retrieval, and multi-hop reasoning in open-domain settings.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                In real-world <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=video%20question%20answering">video question answering</a> scenarios, videos often provide only localized visual cues, while verifiable answers are distributed across the open web; models therefore need to jointly perform cross-frame clue extraction, iterative retrieval, and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=multi-hop%20reasoning">multi-hop reasoning</a>-based verification. To bridge this gap, we construct the first video deep research benchmark, VideoDR. VideoDR centers on video-conditioned open-domain <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=video%20question%20answering">video question answering</a>, requiring <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=cross-frame%20visual%20anchor%20extraction">cross-frame visual anchor extraction</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=interactive%20web%20retrieval">interactive web retrieval</a>, and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=multi-hop%20reasoning">multi-hop reasoning</a> over joint video-web evidence; through rigorous human annotation and quality control, we obtain high-quality video deep research samples spanning six semantic domains. We evaluate multiple closed-source and open-source <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=multimodal%20large%20language%20models">multimodal large language models</a> under both the Workflow and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Agentic%20paradigm">Agentic paradigm</a>s, and the results show that Agentic is not consistently superior to Workflow: its gains depend on a model's ability to maintain the initial video anchors over long retrieval chains. Further analysis indicates that <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=goal%20drift">goal drift</a> and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=long-horizon%20consistency">long-horizon consistency</a> are the core bottlenecks. In sum, VideoDR provides a systematic benchmark for studying video agents in open-web settings and reveals the key challenges for next-generation video deep research agents.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="TRENDING" data-date="2025-11-14" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2511.11793" target="_blank" onclick="event.stopPropagation()">MiroThinker: Pushing the Performance Boundaries of Open-Source Research Agents via Model, Context, and Interactive Scaling</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-11-14</div>
                                        <div class="tags"><span class="tag">TRENDING</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2511.11793" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/MiroMindAI/MiroThinker" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 183</div>
                                    <div class="link-item stars-item">‚≠ê 4.99k</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">(No summary available)</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                We present MiroThinker v1.0, an open-source research agent designed to advance tool-augmented reasoning and information-seeking capabilities. Unlike previous agents that only scale up model size or context length, MiroThinker explores interaction scaling at the model level, systematically training the model to handle deeper and more frequent agent-environment interactions as a third dimension of performance improvement. Unlike LLM test-time scaling, which operates in isolation and risks degradation with longer reasoning chains, interactive scaling leverages environment feedback and external information acquisition to correct errors and refine trajectories. Through reinforcement learning, the model achieves efficient interaction scaling: with a 256K context window, it can perform up to 600 tool calls per task, enabling sustained multi-turn reasoning and complex real-world research workflows. Across four representative benchmarks-GAIA, HLE, BrowseComp, and BrowseComp-ZH-the 72B variant achieves up to 81.9%, 37.7%, 47.1%, and 55.6% accuracy respectively, surpassing previous open-source agents and approaching commercial counterparts such as GPT-5-high. Our analysis reveals that MiroThinker benefits from interactive scaling consistently: research performance improves predictably as the model engages in deeper and more frequent agent-environment interactions, demonstrating that interaction depth exhibits scaling behaviors analogous to model size and context length. These findings establish interaction scaling as a third critical dimension for building next-generation open research agents, complementing model capacity and context windows.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="DAILY WEEKLY TRENDING" data-date="2026-01-14" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2601.09688" target="_blank" onclick="event.stopPropagation()">DeepResearchEval: An Automated Framework for Deep Research Task Construction and Agentic Evaluation</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2026-01-14</div>
                                        <div class="tags"><span class="tag">DAILY</span><span class="tag">TRENDING</span><span class="tag">WEEKLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2601.09688" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/Infinity-AILab/DeepResearchEval" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 92</div>
                                    <div class="link-item stars-item">‚≠ê 67</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">DeepResearchEval presents an automated framework for creating complex research tasks and evaluating them through agent-based methods that adapt to task specifics and verify facts without relying on citations.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Deep research systems are widely used for multi-step web research, analysis, and cross-source synthesis, yet their evaluation remains challenging. Existing benchmarks often require annotation-intensive task construction, rely on static evaluation dimensions, or fail to reliably verify facts when citations are missing. To bridge these gaps, we introduce DeepResearchEval, an <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=automated%20framework">automated framework</a> for <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=deep%20research%20task%20construction">deep research task construction</a> and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=agentic%20evaluation">agentic evaluation</a>. For task construction, we propose a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=persona-driven%20pipeline">persona-driven pipeline</a> generating realistic, complex research tasks anchored in diverse user profiles, applying a two-stage filter <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Task%20Qualification">Task Qualification</a> and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Search%20Necessity">Search Necessity</a> to retain only tasks requiring <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=multi-source%20evidence%20integration">multi-source evidence integration</a> and external retrieval. For evaluation, we propose an agentic pipeline with two components: an <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Adaptive%20Point-wise%20Quality%20Evaluation">Adaptive Point-wise Quality Evaluation</a> that dynamically derives task-specific evaluation dimensions, criteria, and weights conditioned on each generated task, and an <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Active%20Fact-Checking">Active Fact-Checking</a> that autonomously extracts and verifies report statements via <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=web%20search">web search</a>, even when citations are missing.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="MONTHLY" data-date="2026-01-08" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2601.05242" target="_blank" onclick="event.stopPropagation()">GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2026-01-08</div>
                                        <div class="tags"><span class="tag">MONTHLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2601.05242" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/NVlabs/GDPO" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 186</div>
                                    <div class="link-item stars-item">‚≠ê 249</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">Multi-reward reinforcement learning suffers from reward normalization collapse in GRPO, which GDPO addresses by decoupling reward normalization for improved training stability and performance across reasoning tasks.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Reinforcement%20learning">Reinforcement learning</a> (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Group%20Relative%20Policy%20Optimization">Group Relative Policy Optimization</a> (GRPO) under <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=multi-reward%20setting">multi-reward setting</a> without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=advantage%20values">advantage values</a>, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Group%20reward-Decoupled%20Normalization%20Policy%20Optimization">Group reward-Decoupled Normalization Policy Optimization</a> (GDPO), a new <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=policy%20optimization">policy optimization</a> method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=training%20stability">training stability</a>. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=multi-reward%20reinforcement%20learning">multi-reward reinforcement learning</a> optimization.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="DAILY WEEKLY" data-date="2026-01-14" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2601.09259" target="_blank" onclick="event.stopPropagation()">MAXS: Meta-Adaptive Exploration with LLM Agents</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2026-01-14</div>
                                        <div class="tags"><span class="tag">DAILY</span><span class="tag">WEEKLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2601.09259" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/exoskeletonzj/MAXS" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 81</div>
                                    <div class="link-item stars-item">‚≠ê 5</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">MAXS is a meta-adaptive reasoning framework for LLM agents that improves multi-tool reasoning through lookahead strategies and trajectory convergence mechanisms, balancing global effectiveness and computational efficiency.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Large Language Model (LLM) Agents exhibit inherent reasoning abilities through the collaboration of multiple tools. However, during agent inference, existing methods often suffer from (i) locally myopic generation, due to the absence of lookahead, and (ii) trajectory instability, where minor early errors can escalate into divergent reasoning paths. These issues make it difficult to balance global effectiveness and computational efficiency. To address these two issues, we propose meta-adaptive exploration with <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=LLM%20agents">LLM agents</a> https://github.com/exoskeletonzj/MAXS, a meta-adaptive reasoning framework based on <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=LLM%20Agents">LLM Agents</a> that flexibly integrates <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=tool%20execution">tool execution</a> and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=reasoning%20planning">reasoning planning</a>. MAXS employs a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=lookahead%20strategy">lookahead strategy</a> to extend reasoning paths a few steps ahead, estimating the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=advantage%20value">advantage value</a> of tool usage, and combines <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=step%20consistency%20variance">step consistency variance</a> and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=inter-step%20trend%20slopes">inter-step trend slopes</a> to jointly select stable, consistent, and high-value reasoning steps. Additionally, we introduce a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=trajectory%20convergence">trajectory convergence</a> mechanism that controls computational cost by halting further rollouts once path consistency is achieved, enabling a balance between resource efficiency and global effectiveness in <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=multi-tool%20reasoning">multi-tool reasoning</a>. We conduct extensive empirical studies across three base models (MiMo-VL-7B, Qwen2.5-VL-7B, Qwen2.5-VL-32B) and five datasets, demonstrating that MAXS consistently outperforms existing methods in both performance and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=inference%20efficiency">inference efficiency</a>. Further analysis confirms the effectiveness of our <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=lookahead%20strategy">lookahead strategy</a> and tool usage.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="WEEKLY MONTHLY" data-date="2026-01-10" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2601.06521" target="_blank" onclick="event.stopPropagation()">BabyVision: Visual Reasoning Beyond Language</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2026-01-10</div>
                                        <div class="tags"><span class="tag">MONTHLY</span><span class="tag">WEEKLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2601.06521" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/UniPat-AI/BabyVision" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 177</div>
                                    <div class="link-item stars-item">‚≠ê 127</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">Current multimodal large language models exhibit significant gaps in fundamental visual understanding compared to human children, as demonstrated by the BabyVision benchmark.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                While humans develop <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=core%20visual%20skills">core visual skills</a> long before acquiring language, contemporary <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Multimodal%20LLMs">Multimodal LLMs</a> (MLLMs) still rely heavily on linguistic priors to compensate for their fragile visual understanding. We uncovered a crucial fact: state-of-the-art MLLMs consistently fail on basic visual tasks that humans, even 3-year-olds, can solve effortlessly. To systematically investigate this gap, we introduce BabyVision, a benchmark designed to assess core visual abilities independent of linguistic knowledge for MLLMs. BabyVision spans a wide range of tasks, with 388 items divided into 22 subclasses across four key categories. Empirical results and human evaluation reveal that leading MLLMs perform significantly below human baselines. Gemini3-Pro-Preview scores 49.7, lagging behind 6-year-old humans and falling well behind the average adult score of 94.1. These results show despite excelling in knowledge-heavy evaluations, current MLLMs still lack fundamental <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=visual%20primitives">visual primitives</a>. Progress in BabyVision represents a step toward human-level <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=visual%20perception">visual perception</a> and reasoning capabilities. We also explore solving <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=visual%20reasoning">visual reasoning</a> with generation models by proposing BabyVision-Gen and automatic evaluation toolkit. Our code and benchmark data are released at https://github.com/UniPat-AI/BabyVision for reproduction.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="TRENDING MONTHLY" data-date="2026-01-06" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2601.03233" target="_blank" onclick="event.stopPropagation()">LTX-2: Efficient Joint Audio-Visual Foundation Model</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2026-01-06</div>
                                        <div class="tags"><span class="tag">MONTHLY</span><span class="tag">TRENDING</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2601.03233" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/Lightricks/LTX-2" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 118</div>
                                    <div class="link-item stars-item">‚≠ê 2.52k</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">LTX-2 is an open-source audiovisual diffusion model that generates synchronized video and audio content using a dual-stream transformer architecture with cross-modal attention and classifier-free guidance.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Recent <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=text-to-video%20diffusion%20models">text-to-video diffusion models</a> can generate compelling video sequences, yet they remain silent -- missing the semantic, emotional, and atmospheric cues that audio provides. We introduce LTX-2, an open-source foundational model capable of generating high-quality, temporally synchronized <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=audiovisual%20content">audiovisual content</a> in a unified manner. LTX-2 consists of an asymmetric <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=dual-stream%20transformer">dual-stream transformer</a> with a 14B-parameter video stream and a 5B-parameter audio stream, coupled through bidirectional audio-video <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=cross-attention%20layers">cross-attention layers</a> with <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=temporal%20positional%20embeddings">temporal positional embeddings</a> and cross-modality <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=AdaLN">AdaLN</a> for shared timestep conditioning. This architecture enables efficient training and inference of a unified audiovisual model while allocating more capacity for video generation than audio generation. We employ a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=multilingual%20text%20encoder">multilingual text encoder</a> for broader prompt understanding and introduce a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=modality-aware%20classifier-free%20guidance">modality-aware classifier-free guidance</a> (modality-CFG) mechanism for improved audiovisual alignment and controllability. Beyond generating speech, LTX-2 produces rich, coherent audio tracks that follow the characters, environment, style, and emotion of each scene -- complete with natural background and foley elements. In our evaluations, the model achieves state-of-the-art audiovisual quality and prompt adherence among open-source systems, while delivering results comparable to proprietary models at a fraction of their computational cost and inference time. All model weights and code are publicly released.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="MONTHLY WEEKLY" data-date="2026-01-08" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2601.05432" target="_blank" onclick="event.stopPropagation()">Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2026-01-08</div>
                                        <div class="tags"><span class="tag">MONTHLY</span><span class="tag">WEEKLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2601.05432" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/AMAP-ML/Thinking-with-Map" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 152</div>
                                    <div class="link-item stars-item">‚≠ê 130</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">Large vision-language models are enhanced for image geolocalization by incorporating map-based reasoning and agent-in-the-map loop optimization, achieving superior accuracy compared to existing models.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                The image <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=geolocalization">geolocalization</a> task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=vision-language%20model">vision-language model</a> (LVLM) approaches leverage world knowledge, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=chain-of-thought%20reasoning">chain-of-thought reasoning</a>, and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=agentic%20capabilities">agentic capabilities</a>, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model Thinking with Map ability and formulate it as an <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=agent-in-the-map%20loop">agent-in-the-map loop</a>. We develop a two-stage optimization scheme for it, including <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=agentic%20reinforcement%20learning">agentic reinforcement learning</a> (RL) followed by <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=parallel%20test-time%20scaling">parallel test-time scaling</a> (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=geolocalization">geolocalization</a>. To evaluate our method on up-to-date and in-the-wild images, we further present <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=MAPBench">MAPBench</a>, a comprehensive <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=geolocalization">geolocalization</a> training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Acc%40500m">Acc@500m</a> from 8.0\% to 22.1\% compared to Gemini-3-Pro with Google Search/Map grounded mode.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="DAILY WEEKLY" data-date="2026-01-14" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2601.09274" target="_blank" onclick="event.stopPropagation()">A^3-Bench: Benchmarking Memory-Driven Scientific Reasoning via Anchor and Attractor Activation</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2026-01-14</div>
                                        <div class="tags"><span class="tag">DAILY</span><span class="tag">WEEKLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2601.09274" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/exoskeletonzj/A3-Bench" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 74</div>
                                    <div class="link-item stars-item">‚≠ê 0</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">(No summary available)</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Scientific reasoning relies not only on logical inference but also on activating prior knowledge and experiential structures. Memory can efficiently reuse knowledge and enhance reasoning consistency and stability. However, existing benchmarks mainly evaluate final answers or step-by-step coherence, overlooking the memory-driven mechanisms that underlie human reasoning, which involves activating anchors and attractors, then integrating them into multi-step inference. To address this gap, we propose A^3-Bench~ https://a3-bench.github.io, a benchmark designed to evaluate scientific reasoning through dual-scale memory-driven activation, grounded in Anchor and Attractor Activation. First, we annotate 2,198 science reasoning problems across domains using the SAPM process(subject, anchor &amp; attractor, problem, and memory developing). Second, we introduce a dual-scale memory evaluation framework utilizing anchors and attractors, along with the AAUI(Anchor--Attractor Utilization Index) metric to measure memory activation rates. Finally, through experiments with various base models and paradigms, we validate A^3-Bench and analyze how memory activation impacts reasoning performance, providing insights into memory-driven scientific reasoning.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="WEEKLY MONTHLY" data-date="2026-01-06" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2601.03017" target="_blank" onclick="event.stopPropagation()">MMFormalizer: Multimodal Autoformalization in the Wild</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2026-01-06</div>
                                        <div class="tags"><span class="tag">MONTHLY</span><span class="tag">WEEKLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2601.03017" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <div class="link-item" style="background: #f5f5f5; color: #999;">No GitHub</div>
                                    <div class="link-item upvote-item">üî• 101</div>
                                    <div class="link-item stars-item">‚≠ê 0</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">MMFormalizer enables multimodal autoformalization by integrating visual perception with formal mathematical reasoning, supporting complex physical domains from classical mechanics to quantum mechanics.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Autoformalization">Autoformalization</a>, which translates natural language mathematics into formal statements to enable machine reasoning, faces fundamental challenges in the wild due to the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=multimodal">multimodal</a> nature of the physical world, where physics requires inferring hidden constraints (e.g., mass or energy) from visual elements. To address this, we propose MMFormalizer, which extends <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=autoformalization">autoformalization</a> beyond text by integrating adaptive grounding with entities from real-world mathematical and physical domains. MMFormalizer recursively constructs formal propositions from <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=perceptually%20grounded%20primitives">perceptually grounded primitives</a> through <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=recursive%20grounding">recursive grounding</a> and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=axiom%20composition">axiom composition</a>, with <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=adaptive%20recursive%20termination">adaptive recursive termination</a> ensuring that every abstraction is supported by visual evidence and anchored in dimensional or <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=axiomatic%20grounding">axiomatic grounding</a>. We evaluate MMFormalizer on a new benchmark, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=PhyX-AF">PhyX-AF</a>, comprising 115 curated samples from <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=MathVerse">MathVerse</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=PhyX">PhyX</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Synthetic%20Geometry">Synthetic Geometry</a>, and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Analytic%20Geometry">Analytic Geometry</a>, covering diverse <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=multimodal">multimodal</a> <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=autoformalization">autoformalization</a> tasks. Results show that frontier models such as <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=GPT-5">GPT-5</a> and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Gemini-3-Pro">Gemini-3-Pro</a> achieve the highest compile and semantic accuracy, with <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=GPT-5">GPT-5</a> excelling in physical reasoning, while geometry remains the most challenging domain. Overall, MMFormalizer provides a scalable framework for unified <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=multimodal">multimodal</a> <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=autoformalization">autoformalization</a>, bridging perception and formal reasoning. To the best of our knowledge, this is the first <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=multimodal">multimodal</a> <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=autoformalization">autoformalization</a> method capable of handling <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=classical%20mechanics">classical mechanics</a> (derived from the Hamiltonian), as well as <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=relativity">relativity</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=quantum%20mechanics">quantum mechanics</a>, and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=thermodynamics">thermodynamics</a>. More details are available on our project page: MMFormalizer.github.io
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="TRENDING" data-date="2025-03-14" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2503.11576" target="_blank" onclick="event.stopPropagation()">SmolDocling: An ultra-compact vision-language model for end-to-end
  multi-modal document conversion</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-03-14</div>
                                        <div class="tags"><span class="tag">TRENDING</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2503.11576" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/docling-project/docling" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 134</div>
                                    <div class="link-item stars-item">‚≠ê 50.1k</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">SmolDocling is a compact vision-language model that performs end-to-end document conversion with robust performance across various document types using 256M parameters and a new markup format.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                We introduce SmolDocling, an ultra-compact <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=vision-language%20model">vision-language model</a> targeting
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=end-to-end%20document%20conversion">end-to-end document conversion</a>. Our model comprehensively processes entire
pages by generating <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=DocTags">DocTags</a>, a new <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=universal%20markup%20format">universal markup format</a> that captures all
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=page%20elements">page elements</a> in their full context with location. Unlike existing approaches
that rely on <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=large%20foundational%20models">large foundational models</a>, or <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=ensemble%20solutions">ensemble solutions</a> that rely on
handcrafted pipelines of multiple <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=specialized%20models">specialized models</a>, SmolDocling offers an
end-to-end conversion for accurately capturing content, structure and spatial
location of document elements in a 256M parameters <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=vision-language%20model">vision-language model</a>.
SmolDocling exhibits robust performance in correctly reproducing document
features such as <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=code%20listings">code listings</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=tables">tables</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=equations">equations</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=charts">charts</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=lists">lists</a>, and more
across a diverse range of document types including business documents, academic
papers, technical reports, patents, and forms -- significantly extending beyond
the commonly observed focus on scientific papers. Additionally, we contribute
novel <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=publicly%20sourced%20datasets">publicly sourced datasets</a> for <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=charts">charts</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=tables">tables</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=equations">equations</a>, and code
recognition. Experimental results demonstrate that SmolDocling competes with
other <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Vision%20Language%20Models">Vision Language Models</a> that are up to 27 times larger in size, while
reducing <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=computational%20requirements">computational requirements</a> substantially. The model is currently
available, datasets will be publicly available soon.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="MONTHLY" data-date="2025-12-31" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2512.24618" target="_blank" onclick="event.stopPropagation()">Youtu-LLM: Unlocking the Native Agentic Potential for Lightweight Large Language Models</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-12-31</div>
                                        <div class="tags"><span class="tag">MONTHLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2512.24618" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/TencentCloudADP/youtu-tip" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 136</div>
                                    <div class="link-item stars-item">‚≠ê 503</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">Youtu-LLM is a lightweight language model optimized for computational efficiency and agentic intelligence through a compact architecture, STEM-focused training curriculum, and scalable mid-training strategies for planning and reasoning tasks.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                We introduce Youtu-LLM, a lightweight yet powerful language model that harmonizes high computational efficiency with native agentic intelligence. Unlike typical small models that rely on distillation, Youtu-LLM (1.96B) is pre-trained from scratch to systematically cultivate reasoning and planning capabilities. The key technical advancements are as follows: (1) Compact Architecture with Long-Context Support: Built on a dense <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Multi-Latent%20Attention%20(MLA)%20architecture">Multi-Latent Attention (MLA) architecture</a> with a novel <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=STEM-oriented%20vocabulary">STEM-oriented vocabulary</a>, Youtu-LLM supports a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=128k%20context%20window">128k context window</a>. This design enables robust <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=long-context%20reasoning">long-context reasoning</a> and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=state%20tracking">state tracking</a> within a minimal memory footprint, making it ideal for long-horizon agent and reasoning tasks. (2) Principled "Commonsense-STEM-Agent" Curriculum: We curated a massive corpus of approximately 11T tokens and implemented a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=multi-stage%20training%20strategy">multi-stage training strategy</a>. By progressively shifting the pre-training data distribution from general commonsense to complex STEM and agentic tasks, we ensure the model acquires deep cognitive abilities rather than superficial alignment. (3) Scalable <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Agentic%20Mid-training">Agentic Mid-training</a>: Specifically for the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=agentic%20mid-training">agentic mid-training</a>, we employ diverse <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=data%20construction%20schemes">data construction schemes</a> to synthesize rich and varied trajectories across math, coding, and tool-use domains. This high-quality data enables the model to internalize <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=planning%20and%20reflection%20behaviors">planning and reflection behaviors</a> effectively. Extensive evaluations show that Youtu-LLM sets a new state-of-the-art for sub-2B LLMs. On general benchmarks, it achieves competitive performance against larger models, while on agent-specific tasks, it significantly surpasses existing SOTA baselines, demonstrating that lightweight models can possess strong intrinsic <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=agentic%20capabilities">agentic capabilities</a>.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="WEEKLY" data-date="2026-01-11" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2601.06789" target="_blank" onclick="event.stopPropagation()">MemGovern: Enhancing Code Agents through Learning from Governed Human Experiences</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2026-01-11</div>
                                        <div class="tags"><span class="tag">WEEKLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2601.06789" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/QuantaAlpha/MemGovern" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 73</div>
                                    <div class="link-item stars-item">‚≠ê 36</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">MemGovern framework transforms unstructured GitHub data into structured experiential memory for autonomous software engineering agents, improving bug resolution rates through enhanced experience retrieval.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                While <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=autonomous%20software%20engineering">autonomous software engineering</a> (SWE) agents are reshaping programming paradigms, they currently suffer from a "closed-world" limitation: they attempt to fix bugs from scratch or solely using local context, ignoring the immense historical human experience available on platforms like <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=GitHub">GitHub</a>. Accessing this <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=open-world%20experience">open-world experience</a> is hindered by the unstructured and fragmented nature of real-world issue-tracking data. In this paper, we introduce MemGovern, a framework designed to govern and transform raw <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=GitHub">GitHub</a> data into actionable experiential memory for agents. MemGovern employs <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=experience%20governance">experience governance</a> to convert human experience into agent-friendly <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=experience%20cards">experience cards</a> and introduces an <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=agentic%20experience%20search">agentic experience search</a> strategy that enables logic-driven retrieval of human expertise. By producing 135K governed <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=experience%20cards">experience cards</a>, MemGovern achieves a significant performance boost, improving resolution rates on the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=SWE-bench%20Verified">SWE-bench Verified</a> by 4.65%. As a plug-in approach, MemGovern provides a solution for agent-friendly memory infrastructure.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="DAILY" data-date="2026-01-14" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2601.09088" target="_blank" onclick="event.stopPropagation()">Distribution-Aligned Sequence Distillation for Superior Long-CoT Reasoning</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2026-01-14</div>
                                        <div class="tags"><span class="tag">DAILY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2601.09088" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/D2I-ai/dasd-thinking" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 45</div>
                                    <div class="link-item stars-item">‚≠ê 16</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">A lightweight open-source reasoning model achieves state-of-the-art performance through enhanced sequence-level distillation that addresses limitations in current teacher-student knowledge transfer methods.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                In this report, we introduce DASD-4B-Thinking, a lightweight yet highly capable, fully open-source reasoning model. It achieves SOTA performance among open-source models of comparable scale across challenging benchmarks in mathematics, scientific reasoning, and code generation -- even outperforming several larger models. We begin by critically reexamining a widely adopted distillation paradigm in the community: <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=SFT">SFT</a> on teacher-generated responses, also known as <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=sequence-level%20distillation">sequence-level distillation</a>. Although a series of recent works following this scheme have demonstrated remarkable efficiency and strong empirical performance, they are primarily grounded in the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=SFT">SFT</a> perspective. Consequently, these approaches focus predominantly on designing <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=heuristic%20rules">heuristic rules</a> for <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=SFT">SFT</a> data filtering, while largely overlooking the core principle of distillation itself -- enabling the student model to learn the teacher's full <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=output%20distribution">output distribution</a> so as to inherit its <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=generalization%20capability">generalization capability</a>. Specifically, we identify three critical limitations in current practice: i) Inadequate representation of the teacher's sequence-level distribution; ii) Misalignment between the teacher's <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=output%20distribution">output distribution</a> and the student's learning capacity; and iii) <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Exposure%20bias">Exposure bias</a> arising from <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=teacher-forced%20training">teacher-forced training</a> versus <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=autoregressive%20inference">autoregressive inference</a>. In summary, these shortcomings reflect a systemic absence of explicit teacher-student interaction throughout the distillation process, leaving the essence of distillation underexploited. To address these issues, we propose several methodological innovations that collectively form an enhanced <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=sequence-level%20distillation">sequence-level distillation</a> training pipeline. Remarkably, DASD-4B-Thinking obtains competitive results using only 448K training samples -- an order of magnitude fewer than those employed by most existing open-source efforts. To support community research, we publicly release our models and the training dataset.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="TRENDING" data-date="2026-01-05" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2601.02553" target="_blank" onclick="event.stopPropagation()">SimpleMem: Efficient Lifelong Memory for LLM Agents</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2026-01-05</div>
                                        <div class="tags"><span class="tag">TRENDING</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2601.02553" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/aiming-lab/SimpleMem" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 28</div>
                                    <div class="link-item stars-item">‚≠ê 1.13k</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">(No summary available)</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                To support reliable long-term interaction in complex environments, LLM agents require memory systems that efficiently manage historical experiences. Existing approaches either retain full interaction histories via passive context extension, leading to substantial redundancy, or rely on iterative reasoning to filter noise, incurring high token costs. To address this challenge, we introduce SimpleMem, an efficient memory framework based on semantic lossless compression. We propose a three-stage pipeline designed to maximize information density and token utilization: (1) Semantic Structured Compression, which applies entropy-aware filtering to distill unstructured interactions into compact, multi-view indexed memory units; (2) Recursive Memory Consolidation, an asynchronous process that integrates related units into higher-level abstract representations to reduce redundancy; and (3) Adaptive Query-Aware Retrieval, which dynamically adjusts retrieval scope based on query complexity to construct precise context efficiently. Experiments on benchmark datasets show that our method consistently outperforms baseline approaches in accuracy, retrieval efficiency, and inference cost, achieving an average F1 improvement of 26.4% while reducing inference-time token consumption by up to 30-fold, demonstrating a superior balance between performance and efficiency. Code is available at https://github.com/aiming-lab/SimpleMem.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="MONTHLY" data-date="2026-01-01" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2601.00393" target="_blank" onclick="event.stopPropagation()">NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2026-01-01</div>
                                        <div class="tags"><span class="tag">MONTHLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2601.00393" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/IamCreateAI/NeoVerse" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 116</div>
                                    <div class="link-item stars-item">‚≠ê 295</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">NeoVerse is a scalable 4D world model that enables pose-free reconstruction and novel-trajectory video generation from monocular videos with state-of-the-art performance.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                In this paper, we propose NeoVerse, a versatile <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=4D%20world%20model">4D world model</a> that is capable of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=4D%20reconstruction">4D reconstruction</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=novel-trajectory%20video%20generation">novel-trajectory video generation</a>, and rich downstream applications. We first identify a common limitation of scalability in current <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=4D%20world%20model">4D world model</a>ing methods, caused either by expensive and specialized multi-view 4D data or by cumbersome training pre-processing. In contrast, our NeoVerse is built upon a core philosophy that makes the full pipeline scalable to diverse in-the-wild <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=monocular%20videos">monocular videos</a>. Specifically, NeoVerse features <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=pose-free">pose-free</a> <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=feed-forward">feed-forward</a> <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=4D%20reconstruction">4D reconstruction</a>, online monocular <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=degradation%20pattern%20simulation">degradation pattern simulation</a>, and other well-aligned techniques. These designs empower NeoVerse with versatility and generalization to various domains. Meanwhile, NeoVerse achieves state-of-the-art performance in standard reconstruction and generation benchmarks. Our project page is available at https://neoverse-4d.github.io
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="WEEKLY" data-date="2026-01-09" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2601.05593" target="_blank" onclick="event.stopPropagation()">PaCoRe: Learning to Scale Test-Time Compute with Parallel Coordinated Reasoning</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2026-01-09</div>
                                        <div class="tags"><span class="tag">WEEKLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2601.05593" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/stepfun-ai/PaCoRe" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 72</div>
                                    <div class="link-item stars-item">‚≠ê 268</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">Parallel Coordinated Reasoning enables large-scale test-time compute scaling beyond sequential reasoning limitations through parallel exploration and message-passing architecture.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                We introduce Parallel Coordinated Reasoning (PaCoRe), a training-and-inference framework designed to overcome a central limitation of contemporary language models: their inability to scale <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=test-time%20compute">test-time compute</a> (TTC) far beyond <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=sequential%20reasoning">sequential reasoning</a> under a fixed context window. PaCoRe departs from the traditional sequential paradigm by driving TTC through massive <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=parallel%20exploration">parallel exploration</a> coordinated via a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=message-passing%20architecture">message-passing architecture</a> in multiple rounds. Each round launches many parallel reasoning trajectories, compacts their findings into context-bounded messages, and synthesizes these messages to guide the next round and ultimately produce the final answer. Trained end-to-end with large-scale, outcome-based <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=reinforcement%20learning">reinforcement learning</a>, the model masters the synthesis abilities required by PaCoRe and scales to <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=multi-million-token">multi-million-token</a> effective TTC without exceeding context limits. The approach yields strong improvements across diverse domains, and notably pushes reasoning beyond frontier systems in mathematics: an 8B model reaches 94.5% on <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=HMMT%202025">HMMT 2025</a>, surpassing <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=GPT-5">GPT-5</a>'s 93.2% by scaling effective TTC to roughly two million tokens. We open-source model checkpoints, training data, and the full inference pipeline to accelerate follow-up work.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="DAILY" data-date="2026-01-14" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2601.09708" target="_blank" onclick="event.stopPropagation()">Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2026-01-14</div>
                                        <div class="tags"><span class="tag">DAILY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2601.09708" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <div class="link-item" style="background: #f5f5f5; color: #999;">No GitHub</div>
                                    <div class="link-item upvote-item">üî• 38</div>
                                    <div class="link-item stars-item">‚≠ê 0</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">Fast-ThinkAct is an efficient vision-language-action framework that reduces inference latency by 89.3% through compact latent reasoning while maintaining long-horizon planning and few-shot adaptation capabilities.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Vision-Language-Action">Vision-Language-Action</a> (VLA) tasks require reasoning over complex visual scenes and executing adaptive actions in dynamic environments. While recent studies on reasoning VLAs show that explicit <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=chain-of-thought">chain-of-thought</a> (CoT) can improve generalization, they suffer from high <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=inference%20latency">inference latency</a> due to lengthy reasoning traces. We propose Fast-ThinkAct, an efficient reasoning framework that achieves compact yet performant planning through verbalizable <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=latent%20reasoning">latent reasoning</a>. Fast-ThinkAct learns to reason efficiently with latent CoTs by distilling from a teacher, driven by a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=preference-guided%20objective">preference-guided objective</a> to align manipulation trajectories that transfers both linguistic and visual planning capabilities for <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=embodied%20control">embodied control</a>. This enables reasoning-enhanced <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=policy%20learning">policy learning</a> that effectively connects compact reasoning to action execution. Extensive experiments across diverse embodied manipulation and reasoning benchmarks demonstrate that Fast-ThinkAct achieves strong performance with up to 89.3\% reduced <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=inference%20latency">inference latency</a> over state-of-the-art reasoning VLAs, while maintaining effective long-horizon planning, few-shot adaptation, and failure recovery.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="TRENDING" data-date="2024-03-20" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2403.13372" target="_blank" onclick="event.stopPropagation()">LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2024-03-20</div>
                                        <div class="tags"><span class="tag">TRENDING</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2403.13372" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/hiyouga/LLaMA-Factory" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 176</div>
                                    <div class="link-item stars-item">‚≠ê 65.8k</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">LlamaFactory is a unified framework enabling efficient fine-tuning of large language models across various tasks using a web-based user interface.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Efficient%20fine-tuning">Efficient fine-tuning</a> is vital for adapting <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=large%20language%20models">large language models</a> (LLMs) to
downstream tasks. However, it requires non-trivial efforts to implement these
methods on different models. We present <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=LlamaFactory">LlamaFactory</a>, a unified framework that
integrates a suite of cutting-edge efficient training methods. It allows users
to flexibly customize the fine-tuning of 100+ LLMs without the need for coding
through the built-in web UI <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=LlamaBoard">LlamaBoard</a>. We empirically validate the efficiency
and effectiveness of our framework on <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=language%20modeling">language modeling</a> and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=text%20generation">text generation</a>
tasks. It has been released at https://github.com/hiyouga/<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=LLaMA">LLaMA</a>-Factory and
already received over 13,000 stars and 1,600 forks.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="MONTHLY" data-date="2025-12-31" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2512.24615" target="_blank" onclick="event.stopPropagation()">Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-12-31</div>
                                        <div class="tags"><span class="tag">MONTHLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2512.24615" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/TencentCloudADP/youtu-agent" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 112</div>
                                    <div class="link-item stars-item">‚≠ê 4.24k</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">(No summary available)</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Existing Large Language Model (LLM) agent frameworks face two significant challenges: high configuration costs and static capabilities. Building a high-quality agent often requires extensive manual effort in tool integration and prompt engineering, while deployed agents struggle to adapt to dynamic environments without expensive fine-tuning. To address these issues, we propose Youtu-Agent, a modular framework designed for the automated generation and continuous evolution of LLM agents. Youtu-Agent features a structured configuration system that decouples execution environments, toolkits, and context management, enabling flexible reuse and automated synthesis. We introduce two generation paradigms: a Workflow mode for standard tasks and a Meta-Agent mode for complex, non-standard requirements, capable of automatically generating tool code, prompts, and configurations. Furthermore, Youtu-Agent establishes a hybrid policy optimization system: (1) an Agent Practice module that enables agents to accumulate experience and improve performance through in-context optimization without parameter updates; and (2) an Agent RL module that integrates with distributed training frameworks to enable scalable and stable reinforcement learning of any Youtu-Agents in an end-to-end, large-scale manner. Experiments demonstrate that Youtu-Agent achieves state-of-the-art performance on WebWalkerQA (71.47\%) and GAIA (72.8\%) using open-weight models. Our automated generation pipeline achieves over 81\% tool synthesis success rate, while the Practice module improves performance on AIME 2024/2025 by +2.7\% and +5.4\% respectively. Moreover, our Agent RL training achieves 40\% speedup with steady performance improvement on 7B LLMs, enhancing coding/reasoning and searching capabilities respectively up to 35\% and 21\% on Maths and general/multi-hop QA benchmarks.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="DAILY" data-date="2026-01-14" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2601.09136" target="_blank" onclick="event.stopPropagation()">SkinFlow: Efficient Information Transmission for Open Dermatological Diagnosis via Dynamic Visual Encoding and Staged RL</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2026-01-14</div>
                                        <div class="tags"><span class="tag">DAILY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2601.09136" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <div class="link-item" style="background: #f5f5f5; color: #999;">No GitHub</div>
                                    <div class="link-item upvote-item">üî• 36</div>
                                    <div class="link-item stars-item">‚≠ê 0</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">SkinFlow introduces a novel framework for dermatological vision-language modeling that improves diagnostic accuracy through optimized visual information transmission efficiency rather than parameter scaling alone.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                General-purpose <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Large%20Vision-Language%20Models">Large Vision-Language Models</a> (LVLMs), despite their massive scale, often falter in dermatology due to "<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=diffuse%20attention">diffuse attention</a>" - the inability to disentangle subtle pathological lesions from background noise. In this paper, we challenge the assumption that parameter scaling is the only path to medical precision. We introduce SkinFlow, a framework that treats diagnosis as an optimization of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=visual%20information%20transmission%20efficiency">visual information transmission efficiency</a>. Our approach utilizes a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Virtual-Width%20Dynamic%20Vision%20Encoder">Virtual-Width Dynamic Vision Encoder</a> (DVE) to "unfold" complex pathological manifolds without physical parameter expansion, coupled with a two-stage <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Reinforcement%20Learning">Reinforcement Learning</a> strategy. This strategy sequentially aligns explicit medical descriptions (Stage I) and reconstructs implicit diagnostic textures (Stage II) within a constrained semantic space. Furthermore, we propose a clinically grounded evaluation protocol that prioritizes diagnostic safety and hierarchical relevance over rigid label matching. Empirical results are compelling: our 7B model establishes a new state-of-the-art on the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Fitzpatrick17k%20benchmark">Fitzpatrick17k benchmark</a>, achieving a +12.06% gain in <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Top-1%20accuracy">Top-1 accuracy</a> and a +28.57% boost in <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Top-6%20accuracy">Top-6 accuracy</a> over the massive general-purpose models (e.g., Qwen3VL-235B and GPT-5.2). These findings demonstrate that optimizing geometric capacity and information flow yields superior <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=diagnostic%20reasoning">diagnostic reasoning</a> compared to raw parameter scaling.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="WEEKLY" data-date="2026-01-13" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2601.08828" target="_blank" onclick="event.stopPropagation()">Motion Attribution for Video Generation</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2026-01-13</div>
                                        <div class="tags"><span class="tag">WEEKLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2601.08828" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <div class="link-item" style="background: #f5f5f5; color: #999;">No GitHub</div>
                                    <div class="link-item upvote-item">üî• 65</div>
                                    <div class="link-item stars-item">‚≠ê 0</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">Motive is a gradient-based data attribution framework that identifies influential video clips for motion improvement in text-to-video models through motion-weighted loss masking.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Despite the rapid progress of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=video%20generation%20models">video generation models</a>, the role of data in influencing motion is poorly understood. We present Motive (MOTIon attribution for Video gEneration), a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=motion-centric">motion-centric</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=gradient-based">gradient-based</a> <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=data%20attribution">data attribution</a> framework that scales to modern, large, high-quality video datasets and models. We use this to study which <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=fine-tuning">fine-tuning</a> clips improve or degrade <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=temporal%20dynamics">temporal dynamics</a>. Motive isolates <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=temporal%20dynamics">temporal dynamics</a> from static appearance via <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=motion-weighted%20loss%20masks">motion-weighted loss masks</a>, yielding efficient and scalable motion-specific influence computation. On <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=text-to-video%20models">text-to-video models</a>, Motive identifies clips that strongly affect motion and guides data curation that improves temporal consistency and physical plausibility. With Motive-selected high-influence data, our method improves both <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=motion%20smoothness">motion smoothness</a> and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=dynamic%20degree">dynamic degree</a> on <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=VBench">VBench</a>, achieving a 74.1% human preference win rate compared with the pretrained base model. To our knowledge, this is the first framework to attribute motion rather than visual appearance in video generative models and to use it to curate <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=fine-tuning">fine-tuning</a> data.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="TRENDING" data-date="2024-06-11" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2406.07155" target="_blank" onclick="event.stopPropagation()">Scaling Large-Language-Model-based Multi-Agent Collaboration</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2024-06-11</div>
                                        <div class="tags"><span class="tag">TRENDING</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2406.07155" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/OpenBMB/ChatDev/tree/macnet" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 3</div>
                                    <div class="link-item stars-item">‚≠ê 28.7k</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">Multi-agent collaboration networks enhance collective intelligence, outperforming baselines across various topologies and showing emergent abilities earlier than neural scaling laws suggest.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Pioneering advancements in large language model-powered agents have
underscored the design pattern of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=multi-agent%20collaboration">multi-agent collaboration</a>, demonstrating that
collective intelligence can surpass the capabilities of each individual.
Inspired by the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=neural%20scaling%20law">neural scaling law</a>, which posits that increasing neurons leads
to emergent abilities, this study investigates whether a similar principle
applies to increasing agents in <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=multi-agent%20collaboration">multi-agent collaboration</a>. Technically, we
propose <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=multi-agent%20collaboration">multi-agent collaboration</a> networks (<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=MacNet">MacNet</a>), which utilize directed
acyclic graphs to organize agents and streamline their interactive reasoning
via <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=topological%20ordering">topological ordering</a>, with solutions derived from their <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=dialogues">dialogues</a>.
Extensive experiments show that <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=MacNet">MacNet</a> consistently outperforms baseline
models, enabling effective agent collaboration across various network
topologies and supporting cooperation among more than a thousand agents.
Notably, we observed a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=small-world%20collaboration%20phenomenon">small-world collaboration phenomenon</a>, where topologies
resembling small-world properties achieved superior performance. Additionally,
we identified a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=collaborative%20scaling%20law">collaborative scaling law</a>, indicating that normalized solution
quality follows a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=logistic%20growth%20pattern">logistic growth pattern</a> as scaling agents, with collaborative
emergence occurring much earlier than previously observed instances of neural
emergence. The code and data will be available at
https://github.com/OpenBMB/ChatDev.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="DAILY" data-date="2026-01-13" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2601.09028" target="_blank" onclick="event.stopPropagation()">OpenDecoder: Open Large Language Model Decoding to Incorporate Document Quality in RAG</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2026-01-13</div>
                                        <div class="tags"><span class="tag">DAILY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2601.09028" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <div class="link-item" style="background: #f5f5f5; color: #999;">No GitHub</div>
                                    <div class="link-item upvote-item">üî• 25</div>
                                    <div class="link-item stars-item">‚≠ê 0</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">OpenDecoder enhances retrieval-augmented generation by explicitly evaluating retrieved information quality through relevance, ranking, and query performance prediction scores, improving robustness to noisy context.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                The development of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=large%20language%20models">large language models</a> (LLMs) has achieved superior performance in a range of downstream tasks, including LLM-based <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=retrieval-augmented%20generation">retrieval-augmented generation</a> (RAG). The quality of generated content heavily relies on the usefulness of the retrieved information and the capacity of LLMs' internal information processing mechanism to incorporate it in answer generation. It is generally assumed that the retrieved information is relevant to the question. However, the retrieved information may have a variable degree of relevance and usefulness, depending on the question and the document collection. It is important to take into account the relevance of the retrieved information in answer generation. In this paper, we propose OpenDecoder, a new approach that leverages explicit evaluation of the retrieved information as quality indicator features for generation. We aim to build a RAG model that is more robust to varying levels of noisy context. Three types of explicit evaluation information are considered: <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=relevance%20score">relevance score</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=ranking%20score">ranking score</a>, and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=QPP">QPP</a> (<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=query%20performance%20prediction">query performance prediction</a>) score. The experimental results on five benchmark datasets demonstrate the effectiveness and better robustness of OpenDecoder by outperforming various baseline methods. Importantly, this paradigm is flexible to be integrated with the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=post-training">post-training</a> of LLMs for any purposes and incorporated with any type of external indicators.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="MONTHLY" data-date="2025-12-30" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2512.23959" target="_blank" onclick="event.stopPropagation()">Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-12-30</div>
                                        <div class="tags"><span class="tag">MONTHLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2512.23959" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/Encyclomen/HGMem" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 101</div>
                                    <div class="link-item stars-item">‚≠ê 88</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">(No summary available)</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Multi-step retrieval-augmented generation (RAG) has become a widely adopted strategy for enhancing large language models (LLMs) on tasks that demand global comprehension and intensive reasoning. Many RAG systems incorporate a working memory module to consolidate retrieved information. However, existing memory designs function primarily as passive storage that accumulates isolated facts for the purpose of condensing the lengthy inputs and generating new sub-queries through deduction. This static nature overlooks the crucial high-order correlations among primitive facts, the compositions of which can often provide stronger guidance for subsequent steps. Therefore, their representational strength and impact on multi-step reasoning and knowledge evolution are limited, resulting in fragmented reasoning and weak global sense-making capacity in extended contexts. We introduce HGMem, a hypergraph-based memory mechanism that extends the concept of memory beyond simple storage into a dynamic, expressive structure for complex reasoning and global understanding. In our approach, memory is represented as a hypergraph whose hyperedges correspond to distinct memory units, enabling the progressive formation of higher-order interactions within memory. This mechanism connects facts and thoughts around the focal problem, evolving into an integrated and situated knowledge structure that provides strong propositions for deeper reasoning in subsequent steps. We evaluate HGMem on several challenging datasets designed for global sense-making. Extensive experiments and in-depth analyses show that our method consistently improves multi-step RAG and substantially outperforms strong baseline systems across diverse tasks.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="WEEKLY" data-date="2026-01-11" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2601.07022" target="_blank" onclick="event.stopPropagation()">Solar Open Technical Report</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2026-01-11</div>
                                        <div class="tags"><span class="tag">WEEKLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2601.07022" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <div class="link-item" style="background: #f5f5f5; color: #999;">No GitHub</div>
                                    <div class="link-item upvote-item">üî• 57</div>
                                    <div class="link-item stars-item">‚≠ê 0</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">Solar Open presents a 102B-parameter bilingual Mixture-of-Experts language model that addresses data scarcity in underserved languages through synthetic data generation, progressive curriculum coordination, and scalable reinforcement learning optimization.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                We introduce Solar Open, a 102B-parameter bilingual <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Mixture-of-Experts">Mixture-of-Experts</a> <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=language%20model">language model</a> for <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=underserved%20languages">underserved languages</a>. Solar Open demonstrates a systematic methodology for building competitive LLMs by addressing three interconnected challenges. First, to train effectively despite data scarcity for <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=underserved%20languages">underserved languages</a>, we synthesize 4.5T tokens of high-quality, domain-specific, and RL-oriented data. Second, we coordinate this data through a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=progressive%20curriculum">progressive curriculum</a> jointly optimizing composition, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=quality%20thresholds">quality thresholds</a>, and domain coverage across 20 trillion tokens. Third, to enable reasoning capabilities through scalable RL, we apply our proposed framework <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=SnapPO">SnapPO</a> for efficient optimization. Across benchmarks in English and Korean, Solar Open achieves competitive performance, demonstrating the effectiveness of this methodology for underserved language AI development.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="TRENDING" data-date="2024-06-13" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2406.08979" target="_blank" onclick="event.stopPropagation()">Multi-Agent Software Development through Cross-Team Collaboration</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2024-06-13</div>
                                        <div class="tags"><span class="tag">TRENDING</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2406.08979" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/OpenBMB/ChatDev" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• -</div>
                                    <div class="link-item stars-item">‚≠ê 28.7k</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">Cross-Team Collaboration improves software quality by enabling multiple LLM agent teams to propose and communicate decisions.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                The latest breakthroughs in <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Large%20Language%20Models">Large Language Models</a> (<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=LLMs">LLMs</a>), eg., <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=ChatDev">ChatDev</a>, have
catalyzed profound transformations, particularly through multi-agent
collaboration for software <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=development">development</a>. LLM agents can collaborate in teams
like humans, and follow the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=waterfall%20model">waterfall model</a> to sequentially work on
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=requirements%20analysis">requirements analysis</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=development">development</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=review">review</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=testing">testing</a>, and other phases to
perform <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=autonomous%20software%20generation">autonomous software generation</a>. However, for an agent team, each phase
in a single <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=development">development</a> process yields only one possible outcome. This results
in the completion of only one <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=development">development</a> chain, thereby losing the opportunity
to explore multiple potential decision paths within the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=solution%20space">solution space</a>.
Consequently, this may lead to obtaining suboptimal results. To address this
challenge, we introduce <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Cross-Team%20Collaboration">Cross-Team Collaboration</a> (<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=CTC">CTC</a>), a scalable multi-team
framework that enables orchestrated teams to jointly propose various decisions
and communicate with their insights in a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=cross-team%20collaboration">cross-team collaboration</a> environment
for superior <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=content%20generation">content generation</a>. Experimental results in software <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=development">development</a>
reveal a notable increase in quality compared to state-of-the-art baselines,
underscoring the efficacy of our framework. The significant improvements in
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=story%20generation">story generation</a> demonstrate the promising generalization ability of our
framework across various domains. We anticipate that our work will guide LLM
agents towards a cross-team paradigm and contribute to their significant growth
in but not limited to software <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=development">development</a>. The code and data will be available
at https://github.com/OpenBMB/<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=ChatDev">ChatDev</a>.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="DAILY" data-date="2026-01-14" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2601.09575" target="_blank" onclick="event.stopPropagation()">OpenVoxel: Training-Free Grouping and Captioning Voxels for Open-Vocabulary 3D Scene Understanding</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2026-01-14</div>
                                        <div class="tags"><span class="tag">DAILY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2601.09575" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <div class="link-item" style="background: #f5f5f5; color: #999;">No GitHub</div>
                                    <div class="link-item upvote-item">üî• 22</div>
                                    <div class="link-item stars-item">‚≠ê 0</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">OpenVoxel enables open-vocabulary 3D scene understanding through training-free grouping and captioning of sparse voxels using Vision Language Models and Multi-modal Large Language Models.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                We propose OpenVoxel, a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=training-free%20algorithm">training-free algorithm</a> for grouping and captioning <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=sparse%20voxels">sparse voxels</a> for the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=open-vocabulary%203D%20scene%20understanding">open-vocabulary 3D scene understanding</a> tasks. Given the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=sparse%20voxel%20rasterization">sparse voxel rasterization</a> (SVR) model obtained from multi-view images of a 3D scene, our OpenVoxel is able to produce meaningful groups that describe different objects in the scene. Also, by leveraging powerful <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Vision%20Language%20Models">Vision Language Models</a> (VLMs) and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Multi-modal%20Large%20Language%20Models">Multi-modal Large Language Models</a> (MLLMs), our OpenVoxel successfully build an informative scene map by captioning each group, enabling further 3D scene understanding tasks such as <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=open-vocabulary%20segmentation">open-vocabulary segmentation</a> (OVS) or <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=referring%20expression%20segmentation">referring expression segmentation</a> (RES). Unlike previous methods, our method is training-free and does not introduce embeddings from a CLIP/BERT text encoder. Instead, we directly proceed with text-to-text search using MLLMs. Through extensive experiments, our method demonstrates superior performance compared to recent studies, particularly in complex <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=referring%20expression%20segmentation">referring expression segmentation</a> (RES) tasks. The code will be open.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="MONTHLY" data-date="2025-12-31" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2512.24873" target="_blank" onclick="event.stopPropagation()">Let It Flow: Agentic Crafting on Rock and Roll, Building the ROME Model within an Open Agentic Learning Ecosystem</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-12-31</div>
                                        <div class="tags"><span class="tag">MONTHLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2512.24873" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/alibaba/ROLL" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 100</div>
                                    <div class="link-item stars-item">‚≠ê 2.65k</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">The Agentic Learning Ecosystem (ALE) introduces a principled infrastructure for agent development, combining post-training optimization, sandbox environments, and policy alignment to enhance long-horizon training stability and performance in real-world tasks.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Agentic crafting requires LLMs to operate in real-world environments over multiple turns by taking actions, observing outcomes, and iteratively refining artifacts. Despite its importance, the open-source community lacks a principled, end-to-end ecosystem to streamline agent development. We introduce the Agentic Learning Ecosystem (ALE), a foundational infrastructure that optimizes the production pipeline for agent LLMs. ALE consists of three components: ROLL, a post-training framework for weight optimization; ROCK, a sandbox environment manager for trajectory generation; and iFlow CLI, an agent framework for efficient context engineering. We release ROME (ROME is Obviously an Agentic Model), an open-source agent grounded by ALE and trained on over one million trajectories. Our approach includes <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=data%20composition%20protocols">data composition protocols</a> for synthesizing complex behaviors and a novel policy optimization algorithm, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Interaction-based%20Policy%20Alignment%20(IPA)">Interaction-based Policy Alignment (IPA)</a>, which assigns credit over <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=semantic%20interaction%20chunks">semantic interaction chunks</a> rather than individual tokens to improve long-horizon training stability. Empirically, we evaluate ROME within a structured setting and introduce <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Terminal%20Bench%20Pro">Terminal Bench Pro</a>, a benchmark with improved scale and contamination control. ROME demonstrates strong performance across benchmarks like <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=SWE-bench%20Verified">SWE-bench Verified</a> and Terminal Bench, proving the effectiveness of the ALE infrastructure.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="TRENDING" data-date="2023-09-12" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2309.06180" target="_blank" onclick="event.stopPropagation()">Efficient Memory Management for Large Language Model Serving with
  PagedAttention</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2023-09-12</div>
                                        <div class="tags"><span class="tag">TRENDING</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2309.06180" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/vllm-project/vllm" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 29</div>
                                    <div class="link-item stars-item">‚≠ê 67.6k</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">PagedAttention algorithm and vLLM system enhance the throughput of large language models by efficiently managing memory and reducing waste in the key-value cache.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                High throughput serving of large language models (LLMs) requires batching
sufficiently many requests at a time. However, existing systems struggle
because the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=key-value%20cache">key-value cache</a> (<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=KV%20cache">KV cache</a>) memory for each request is huge and
grows and shrinks dynamically. When managed inefficiently, this memory can be
significantly wasted by fragmentation and redundant duplication, limiting the
batch size. To address this problem, we propose <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=PagedAttention">PagedAttention</a>, an attention
algorithm inspired by the classical <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=virtual%20memory">virtual memory</a> and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=paging%20techniques">paging techniques</a> in
operating systems. On top of it, we build <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=vLLM">vLLM</a>, an LLM serving system that
achieves (1) near-zero waste in <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=KV%20cache">KV cache</a> memory and (2) flexible sharing of KV
cache within and across requests to further reduce memory usage. Our
evaluations show that <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=vLLM">vLLM</a> improves the throughput of popular LLMs by
2-4times with the same level of latency compared to the state-of-the-art
systems, such as <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=FasterTransformer">FasterTransformer</a> and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Orca">Orca</a>. The improvement is more pronounced
with longer sequences, larger models, and more complex decoding algorithms.
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=vLLM">vLLM</a>'s source code is publicly available at
https://github.com/<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=vllm">vllm</a>-project/<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=vllm">vllm</a>
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="WEEKLY" data-date="2026-01-08" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2601.04745" target="_blank" onclick="event.stopPropagation()">KnowMe-Bench: Benchmarking Person Understanding for Lifelong Digital Companions</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2026-01-08</div>
                                        <div class="tags"><span class="tag">WEEKLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2601.04745" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/QuantaAlpha/KnowMeBench" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 50</div>
                                    <div class="link-item stars-item">‚≠ê 92</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">Long-horizon memory benchmarks based on autobiographical narratives evaluate models' ability to infer stable motivations and decision principles through evidence-linked questions spanning factual recall, subjective state attribution, and principle-level reasoning.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Existing long-horizon <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=memory%20benchmarks">memory benchmarks</a> mostly use multi-turn dialogues or synthetic user histories, which makes retrieval performance an imperfect proxy for person understanding. We present \BenchName, a publicly releasable benchmark built from long-form <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=autobiographical%20narratives">autobiographical narratives</a>, where actions, context, and inner thoughts provide dense evidence for inferring stable motivations and decision principles. \BenchName~reconstructs each narrative into a flashback-aware, time-anchored stream and evaluates models with evidence-linked questions spanning <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=factual%20recall">factual recall</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=subjective%20state%20attribution">subjective state attribution</a>, and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=principle-level%20reasoning">principle-level reasoning</a>. Across diverse narrative sources, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=retrieval-augmented%20systems">retrieval-augmented systems</a> mainly improve factual accuracy, while errors persist on temporally grounded explanations and higher-level inferences, highlighting the need for memory mechanisms beyond retrieval. Our data is in KnowMeBench{https://github.com/QuantaAlpha/KnowMeBench}.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="WEEKLY" data-date="2026-01-06" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2601.03319" target="_blank" onclick="event.stopPropagation()">CaricatureGS: Exaggerating 3D Gaussian Splatting Faces With Gaussian Curvature</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2026-01-06</div>
                                        <div class="tags"><span class="tag">WEEKLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2601.03319" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <div class="link-item" style="background: #f5f5f5; color: #999;">No GitHub</div>
                                    <div class="link-item upvote-item">üî• 50</div>
                                    <div class="link-item stars-item">‚≠ê 0</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">A photorealistic 3D caricaturization framework combines Gaussian curvature-based surface exaggeration with 3D Gaussian Splatting to create controllable, realistic avatars with improved fidelity and real-time deformation capabilities.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                A photorealistic and controllable 3D caricaturization framework for faces is introduced. We start with an intrinsic Gaussian curvature-based surface exaggeration technique, which, when coupled with texture, tends to produce over-smoothed renders. To address this, we resort to <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=3D%20Gaussian%20Splatting">3D Gaussian Splatting</a> (3DGS), which has recently been shown to produce realistic free-viewpoint avatars. Given a multiview sequence, we extract a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=FLAME%20mesh">FLAME mesh</a>, solve a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=curvature-weighted%20Poisson%20equation">curvature-weighted Poisson equation</a>, and obtain its exaggerated form. However, directly deforming the Gaussians yields poor results, necessitating the synthesis of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=pseudo-ground-truth%20caricature%20images">pseudo-ground-truth caricature images</a> by warping each frame to its exaggerated 2D representation using <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=local%20affine%20transformations">local affine transformations</a>. We then devise a training scheme that alternates real and synthesized supervision, enabling a single Gaussian collection to represent both natural and exaggerated avatars. This scheme improves fidelity, supports local edits, and allows continuous control over the intensity of the caricature. In order to achieve <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=real-time%20deformations">real-time deformations</a>, an efficient interpolation between the original and exaggerated surfaces is introduced. We further analyze and show that it has a bounded deviation from <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=closed-form%20solutions">closed-form solutions</a>. In both quantitative and qualitative evaluations, our results outperform prior work, delivering photorealistic, geometry-controlled caricature avatars.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="TRENDING" data-date="2025-11-17" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2511.12884" target="_blank" onclick="event.stopPropagation()">Agent READMEs: An Empirical Study of Context Files for Agentic Coding</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-11-17</div>
                                        <div class="tags"><span class="tag">TRENDING</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2511.12884" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/openai/agents.md" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 20</div>
                                    <div class="link-item stars-item">‚≠ê 15.1k</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">(No summary available)</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Agentic coding tools receive goals written in natural language as input, break them down into specific tasks, and write or execute the actual code with minimal human intervention. Central to this process are agent context files ("READMEs for agents") that provide persistent, project-level instructions. In this paper, we conduct the first large-scale empirical study of 2,303 agent context files from 1,925 repositories to characterize their structure, maintenance, and content. We find that these files are not static documentation but complex, difficult-to-read artifacts that evolve like configuration code, maintained through frequent, small additions. Our content analysis of 16 instruction types shows that developers prioritize functional context, such as build and run commands (62.3%), implementation details (69.9%), and architecture (67.7%). We also identify a significant gap: non-functional requirements like security (14.5%) and performance (14.5%) are rarely specified. These findings indicate that while developers use context files to make agents functional, they provide few guardrails to ensure that agent-written code is secure or performant, highlighting the need for improved tooling and practices.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="MONTHLY" data-date="2026-01-06" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2601.03252" target="_blank" onclick="event.stopPropagation()">InfiniDepth: Arbitrary-Resolution and Fine-Grained Depth Estimation with Neural Implicit Fields</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2026-01-06</div>
                                        <div class="tags"><span class="tag">MONTHLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2601.03252" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <div class="link-item" style="background: #f5f5f5; color: #999;">No GitHub</div>
                                    <div class="link-item upvote-item">üî• 95</div>
                                    <div class="link-item stars-item">‚≠ê 0</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">InfiniDepth represents depth as neural implicit fields using a local implicit decoder, enabling continuous 2D coordinate querying for arbitrary-resolution depth estimation and superior performance in fine-detail regions.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Existing depth estimation methods are fundamentally limited to predicting depth on discrete image grids. Such representations restrict their scalability to arbitrary output resolutions and hinder the geometric detail recovery. This paper introduces InfiniDepth, which represents depth as <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=neural%20implicit%20fields">neural implicit fields</a>. Through a simple yet effective <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=local%20implicit%20decoder">local implicit decoder</a>, we can query depth at <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=continuous%202D%20coordinates">continuous 2D coordinates</a>, enabling arbitrary-resolution and fine-grained depth estimation. To better assess our method's capabilities, we curate a high-quality <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=4K%20synthetic%20benchmark">4K synthetic benchmark</a> from five different games, spanning diverse scenes with rich geometric and appearance details. Extensive experiments demonstrate that InfiniDepth achieves state-of-the-art performance on both synthetic and real-world benchmarks across relative and metric depth estimation tasks, particularly excelling in fine-detail regions. It also benefits the task of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=novel%20view%20synthesis">novel view synthesis</a> under large <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=viewpoint%20shifts">viewpoint shifts</a>, producing high-quality results with fewer holes and artifacts.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="DAILY" data-date="2026-01-13" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2601.08605" target="_blank" onclick="event.stopPropagation()">ExpSeek: Self-Triggered Experience Seeking for Web Agents</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2026-01-13</div>
                                        <div class="tags"><span class="tag">DAILY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2601.08605" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <div class="link-item" style="background: #f5f5f5; color: #999;">No GitHub</div>
                                    <div class="link-item upvote-item">üî• 15</div>
                                    <div class="link-item stars-item">‚≠ê 0</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">ExpSeek enables web agents to proactively seek experience during interaction by using entropy-based timing and tailored content, achieving significant performance improvements across multiple benchmarks.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Experience%20intervention">Experience intervention</a> in <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=web%20agents">web agents</a> emerges as a promising technical paradigm, enhancing agent interaction capabilities by providing valuable insights from accumulated experiences. However, existing methods predominantly inject experience passively as global context before task execution, struggling to adapt to dynamically changing contextual observations during agent-environment interaction. We propose ExpSeek, which shifts experience toward <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=step-level%20proactive%20seeking">step-level proactive seeking</a>: (1) estimating step-level <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=entropy%20thresholds">entropy thresholds</a> to determine intervention timing using the model's <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=intrinsic%20signals">intrinsic signals</a>; (2) designing step-level tailor-designed experience content. Experiments on <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Qwen3-8B">Qwen3-8B</a> and 32B models across four challenging <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=web%20agent%20benchmarks">web agent benchmarks</a> demonstrate that ExpSeek achieves absolute improvements of 9.3% and 7.5%, respectively. Our experiments validate the feasibility and advantages of entropy as a self-triggering signal, reveal that even a 4B small-scale experience model can significantly boost the performance of larger agent models.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="TRENDING" data-date="2026-01-10" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2601.06487" target="_blank" onclick="event.stopPropagation()">ArenaRL: Scaling RL for Open-Ended Agents via Tournament-based Relative Ranking</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2026-01-10</div>
                                        <div class="tags"><span class="tag">TRENDING</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2601.06487" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/Alibaba-NLP/qqr" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 41</div>
                                    <div class="link-item stars-item">‚≠ê 88</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">Reinforcement learning for large language model agents suffers from discrimination collapse in open-ended tasks due to pointwise scalar scoring, which ArenaRL addresses through relative ranking and pairwise evaluation mechanisms.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Reinforcement%20learning">Reinforcement learning</a> has substantially improved the performance of LLM agents on tasks with verifiable outcomes, but it still struggles on open-ended agent tasks with vast solution spaces (e.g., complex travel planning). Due to the absence of objective ground-truth for these tasks, current RL algorithms largely rely on <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=reward%20models">reward models</a> that assign <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=scalar%20scores">scalar scores</a> to individual responses. We contend that such pointwise scoring suffers from an inherent <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=discrimination%20collapse">discrimination collapse</a>: the reward model struggles to distinguish subtle advantages among different trajectories, resulting in scores within a group being compressed into a narrow range. Consequently, the effective reward signal becomes dominated by noise from the reward model, leading to optimization stagnation. To address this, we propose ArenaRL, a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=reinforcement%20learning">reinforcement learning</a> paradigm that shifts from pointwise scalar scoring to <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=intra-group%20relative%20ranking">intra-group relative ranking</a>. ArenaRL introduces a process-aware <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=pairwise%20evaluation">pairwise evaluation</a> mechanism, employing <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=multi-level%20rubrics">multi-level rubrics</a> to assign fine-grained relative scores to trajectories. Additionally, we construct an intra-group <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=adversarial%20arena">adversarial arena</a> and devise a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=tournament-based%20ranking">tournament-based ranking</a> scheme to obtain stable advantage signals. Empirical results confirm that the built seeded single-elimination scheme achieves nearly equivalent <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=advantage%20estimation">advantage estimation</a> accuracy to full pairwise comparisons with O(N^2) complexity, while operating with only O(N) complexity, striking an optimal balance between <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=efficiency">efficiency</a> and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=precision">precision</a>. Furthermore, to address the lack of full-cycle <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=benchmarks">benchmarks</a> for open-ended agents, we build Open-Travel and Open-DeepResearch, two high-quality <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=benchmarks">benchmarks</a> featuring a comprehensive pipeline covering <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=SFT">SFT</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=RL%20training">RL training</a>, and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=multi-dimensional%20evaluation">multi-dimensional evaluation</a>. Extensive experiments show that ArenaRL substantially outperforms standard RL baselines, enabling LLM agents to generate more robust solutions for complex real-world tasks.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="WEEKLY" data-date="2026-01-09" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2601.06002" target="_blank" onclick="event.stopPropagation()">The Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2026-01-09</div>
                                        <div class="tags"><span class="tag">WEEKLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2601.06002" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <div class="link-item" style="background: #f5f5f5; color: #999;">No GitHub</div>
                                    <div class="link-item upvote-item">üî• 48</div>
                                    <div class="link-item stars-item">‚≠ê 0</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">Large language models struggle with long chain-of-thought reasoning due to unstable structural patterns, but a molecular-inspired approach using effective semantic isomers and distribution-transfer-graph methods improves training stability and performance.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Large%20language%20models">Large language models</a> (LLMs) often fail to learn effective long <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=chain-of-thought">chain-of-thought</a> (<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Long%20CoT">Long CoT</a>) reasoning from human or non-Long-CoT LLMs imitation. To understand this, we propose that effective and learnable <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Long%20CoT">Long CoT</a> trajectories feature stable <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=molecular-like%20structures">molecular-like structures</a> in unified view, which are formed by three interaction types: Deep-Reasoning (covalent-like), <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Self-Reflection">Self-Reflection</a> (hydrogen-bond-like), and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Self-Exploration">Self-Exploration</a> (van der Waals-like). Analysis of distilled trajectories reveals these structures emerge from <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Long%20CoT">Long CoT</a> <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=fine-tuning">fine-tuning</a>, not keyword imitation. We introduce Effective <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Semantic%20Isomers">Semantic Isomers</a> and show that only bonds promoting fast <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=entropy%20convergence">entropy convergence</a> support stable <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Long%20CoT">Long CoT</a> learning, while structural competition impairs training. Drawing on these findings, we present Mole-Syn, a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=distribution-transfer-graph">distribution-transfer-graph</a> method that guides synthesis of effective <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Long%20CoT">Long CoT</a> structures, boosting performance and RL stability across benchmarks.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="DAILY" data-date="2026-01-14" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2601.09465" target="_blank" onclick="event.stopPropagation()">EvoFSM: Controllable Self-Evolution for Deep Research with Finite State Machines</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2026-01-14</div>
                                        <div class="tags"><span class="tag">DAILY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2601.09465" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <div class="link-item" style="background: #f5f5f5; color: #999;">No GitHub</div>
                                    <div class="link-item upvote-item">üî• 12</div>
                                    <div class="link-item stars-item">‚≠ê 0</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">EvoFSM is a structured self-evolving framework for LLM agents that uses finite state machines to improve adaptability while maintaining control through constrained optimization and memory mechanisms.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                While <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=LLM-based%20agents">LLM-based agents</a> have shown promise for deep research, most existing approaches rely on fixed workflows that struggle to adapt to real-world, open-ended queries. Recent work therefore explores <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=self-evolution">self-evolution</a> by allowing agents to rewrite their own code or prompts to improve problem-solving ability, but un<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=constrained%20optimization">constrained optimization</a> often triggers instability, hallucinations, and instruction drift. We propose EvoFSM, a structured self-evolving framework that achieves both adaptability and control by evolving an explicit <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Finite%20State%20Machine">Finite State Machine</a> (FSM) instead of relying on free-form rewriting. EvoFSM decouples the optimization space into macroscopic Flow (state-transition logic) and microscopic Skill (state-specific behaviors), enabling targeted improvements under clear behavioral boundaries. Guided by a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=critic%20mechanism">critic mechanism</a>, EvoFSM refines the FSM through a small set of constrained operations, and further incorporates a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=self-evolving%20memory">self-evolving memory</a> that distills successful trajectories as reusable priors and failure patterns as constraints for future queries. Extensive evaluations on five <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=multi-hop%20QA">multi-hop QA</a> benchmarks demonstrate the effectiveness of EvoFSM. In particular, EvoFSM reaches 58.0% accuracy on the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=DeepSearch%20benchmark">DeepSearch benchmark</a>. Additional results on <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=interactive%20decision-making">interactive decision-making</a> tasks further validate its generalization.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="MONTHLY" data-date="2026-01-05" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2601.02151" target="_blank" onclick="event.stopPropagation()">Entropy-Adaptive Fine-Tuning: Resolving Confident Conflicts to Mitigate Forgetting</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2026-01-05</div>
                                        <div class="tags"><span class="tag">MONTHLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2601.02151" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/PRIS-CV/EAFT" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 95</div>
                                    <div class="link-item stars-item">‚≠ê 58</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">Entropy-Adaptive Fine-Tuning addresses catastrophic forgetting in supervised fine-tuning by using token-level entropy to distinguish uncertainty from knowledge conflict, enabling better preservation of general capabilities.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Supervised%20Fine-Tuning">Supervised Fine-Tuning</a> (SFT) is the standard paradigm for domain adaptation, yet it frequently incurs the cost of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=catastrophic%20forgetting">catastrophic forgetting</a>. In sharp contrast, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=on-policy%20Reinforcement%20Learning">on-policy Reinforcement Learning</a> (RL) effectively preserves general capabilities. We investigate this discrepancy and identify a fundamental <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=distributional%20gap">distributional gap</a>: while RL aligns with the model's internal belief, SFT forces the model to fit external supervision. This mismatch often manifests as "<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Confident%20Conflicts">Confident Conflicts</a>" tokens characterized by low probability but low entropy. In these instances, the model is highly confident in its own prediction but is forced to learn a divergent ground truth, triggering destructive <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=gradient%20updates">gradient updates</a>. To address this, we propose Entropy-Adaptive Fine-Tuning (EAFT). Unlike methods relying solely on prediction probability, EAFT utilizes <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=token-level%20entropy">token-level entropy</a> as a gating mechanism to distinguish between <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=epistemic%20uncertainty">epistemic uncertainty</a> and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=knowledge%20conflict">knowledge conflict</a>. This allows the model to learn from uncertain samples while suppressing gradients on conflicting data. Extensive experiments on Qwen and GLM series (ranging from 4B to 32B parameters) across mathematical, medical, and agentic domains confirm our hypothesis. EAFT consistently matches the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=downstream%20performance">downstream performance</a> of standard SFT while significantly mitigating the degradation of general capabilities.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="TRENDING" data-date="2025-10-16" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2510.14528" target="_blank" onclick="event.stopPropagation()">PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B Ultra-Compact Vision-Language Model</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-10-16</div>
                                        <div class="tags"><span class="tag">TRENDING</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2510.14528" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/PaddlePaddle/PaddleOCR" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 111</div>
                                    <div class="link-item stars-item">‚≠ê 68.1k</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">PaddleOCR-VL, a vision-language model combining NaViT-style dynamic resolution and ERNIE, achieves state-of-the-art performance in document parsing and element recognition with high efficiency.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                In this report, we propose PaddleOCR-VL, a SOTA and resource-efficient model tailored for document parsing. Its core component is PaddleOCR-VL-0.9B, a compact yet powerful <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=vision-language%20model">vision-language model</a> (<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=VLM">VLM</a>) that integrates a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=NaViT-style%20dynamic%20resolution%20visual%20encoder">NaViT-style dynamic resolution visual encoder</a> with the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=ERNIE">ERNIE</a>-4.5-0.3B language model to enable accurate <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=element%20recognition">element recognition</a>. This innovative model efficiently supports 109 languages and excels in recognizing complex elements (e.g., text, tables, formulas, and charts), while maintaining minimal resource consumption. Through comprehensive evaluations on widely used public benchmarks and in-house benchmarks, PaddleOCR-VL achieves SOTA performance in both <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=page-level%20document%20parsing">page-level document parsing</a> and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=element-level%20recognition">element-level recognition</a>. It significantly outperforms existing solutions, exhibits strong competitiveness against top-tier <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=VLM">VLM</a>s, and delivers fast inference speeds. These strengths make it highly suitable for practical deployment in real-world scenarios. Code is available at https://github.com/PaddlePaddle/PaddleOCR .
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="DAILY" data-date="2026-01-07" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2601.03928" target="_blank" onclick="event.stopPropagation()">FocusUI: Efficient UI Grounding via Position-Preserving Visual Token Selection</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2026-01-07</div>
                                        <div class="tags"><span class="tag">DAILY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2601.03928" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <div class="link-item" style="background: #f5f5f5; color: #999;">No GitHub</div>
                                    <div class="link-item upvote-item">üî• 12</div>
                                    <div class="link-item stars-item">‚≠ê 0</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">FocusUI is an efficient UI grounding framework that reduces computational overhead by selecting relevant visual tokens while preserving positional continuity through a novel PosPad strategy.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Vision-Language%20Models">Vision-Language Models</a> (VLMs) have shown remarkable performance in User Interface (UI) grounding tasks, driven by their ability to process increasingly high-resolution screenshots. However, screenshots are tokenized into thousands of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=visual%20tokens">visual tokens</a> (e.g., about 4700 for 2K resolution), incurring significant computational overhead and diluting attention. In contrast, humans typically focus on regions of interest when interacting with UI. In this work, we pioneer the task of efficient <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=UI%20grounding">UI grounding</a>. Guided by practical analysis of the task's characteristics and challenges, we propose FocusUI, an efficient <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=UI%20grounding">UI grounding</a> framework that selects patches most relevant to the instruction while preserving <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=positional%20continuity">positional continuity</a> for precise grounding. FocusUI addresses two key challenges: (1) Eliminating redundant tokens in visual encoding. We construct <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=patch-level%20supervision">patch-level supervision</a> by fusing an <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=instruction-conditioned%20score">instruction-conditioned score</a> with a rule-based <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=UI-graph%20score">UI-graph score</a> that down-weights large homogeneous regions to select distinct and instruction-relevant <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=visual%20tokens">visual tokens</a>. (2) Preserving <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=positional%20continuity">positional continuity</a> during visual token selection. We find that general <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=visual%20token%20pruning">visual token pruning</a> methods suffer from severe accuracy degradation on <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=UI%20grounding">UI grounding</a> tasks due to broken positional information. We introduce a novel <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=PosPad%20strategy">PosPad strategy</a>, which compresses each contiguous sequence of dropped <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=visual%20tokens">visual tokens</a> into a single special marker placed at the sequence's last index to preserve <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=positional%20continuity">positional continuity</a>. Comprehensive experiments on four grounding benchmarks demonstrate that FocusUI surpasses GUI-specific baselines. On the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=ScreenSpot-Pro%20benchmark">ScreenSpot-Pro benchmark</a>, FocusUI-7B achieves a performance improvement of 3.7% over GUI-Actor-7B. Even with only 30% visual token retention, FocusUI-7B drops by only 3.2% while achieving up to 1.44x faster inference and 17% lower peak GPU memory.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="TRENDING" data-date="2019-06-14" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/1906.08172" target="_blank" onclick="event.stopPropagation()">MediaPipe: A Framework for Building Perception Pipelines</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2019-06-14</div>
                                        <div class="tags"><span class="tag">TRENDING</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/1906.08172" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/google-ai-edge/mediapipe" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 2</div>
                                    <div class="link-item stars-item">‚≠ê 33.2k</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">MediaPipe framework facilitates the development of perception applications by providing tools for combining components, prototyping, and measuring performance across platforms.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Building applications that perceive the world around them is challenging. A
developer needs to (a) select and develop corresponding machine learning
algorithms and models, (b) build a series of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=prototypes">prototypes</a> and demos, (c) balance
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=resource%20consumption">resource consumption</a> against the quality of the solutions, and finally (d)
identify and mitigate problematic cases. The MediaPipe framework addresses all
of these challenges. A developer can use MediaPipe to build <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=prototypes">prototypes</a> by
combining existing <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=perception%20components">perception components</a>, to advance them to polished
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=cross-platform%20applications">cross-platform applications</a> and measure <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=system%20performance">system performance</a> and resource
consumption on target platforms. We show that these features enable a developer
to focus on the algorithm or model development and use MediaPipe as an
environment for iteratively improving their application with results
reproducible across different devices and platforms. MediaPipe will be
open-sourced at https://github.com/google/mediapipe.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="DAILY" data-date="2026-01-10" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2601.06596" target="_blank" onclick="event.stopPropagation()">Are LLMs Vulnerable to Preference-Undermining Attacks (PUA)? A Factorial Analysis Methodology for Diagnosing the Trade-off between Preference Alignment and Real-World Validity</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2026-01-10</div>
                                        <div class="tags"><span class="tag">DAILY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2601.06596" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <div class="link-item" style="background: #f5f5f5; color: #999;">No GitHub</div>
                                    <div class="link-item upvote-item">üî• 11</div>
                                    <div class="link-item stars-item">‚≠ê 0</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">Research examines how large language models can be manipulated through preference-undermining attacks that exploit alignment objectives, revealing model vulnerabilities and proposing a factorial evaluation method for diagnosing alignment risks.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Large%20Language%20Model">Large Language Model</a> (LLM) training often optimizes for <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=preference%20alignment">preference alignment</a>, rewarding outputs that are perceived as helpful and interaction-friendly. However, this <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=preference-oriented%20objective">preference-oriented objective</a> can be exploited: manipulative prompts can steer responses toward user-appeasing agreement and away from <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=truth-oriented%20correction">truth-oriented correction</a>. In this work, we investigate whether aligned models are vulnerable to <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Preference-Undermining%20Attacks">Preference-Undermining Attacks</a> (PUA), a class of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=manipulative%20prompting">manipulative prompting</a> strategies designed to exploit the model's desire to please user preferences at the expense of truthfulness. We propose a diagnostic methodology that provides a finer-grained and more directive analysis than aggregate benchmark scores, using a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=factorial%20evaluation%20framework">factorial evaluation framework</a> to decompose prompt-induced shifts into interpretable effects of system objectives (truth- vs. preference-oriented) and PUA-style dialogue factors (directive control, personal derogation, conditional approval, reality denial) within a controlled 2 times 2^4 design. Surprisingly, more advanced models are sometimes more susceptible to manipulative prompts. Beyond the dominant reality-denial factor, we observe model-specific sign reversals and interactions with PUA-style factors, suggesting tailored defenses rather than uniform robustness. These findings offer a novel, reproducible factorial evaluation methodology that provides finer-grained diagnostics for <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=post-training%20processes">post-training processes</a> like <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=RLHF">RLHF</a>, enabling better trade-offs in the product iteration of LLMs by offering a more nuanced understanding of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=preference%20alignment">preference alignment</a> risks and the impact of manipulative prompts.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="DAILY" data-date="2026-01-13" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2601.09012" target="_blank" onclick="event.stopPropagation()">TranslateGemma Technical Report</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2026-01-13</div>
                                        <div class="tags"><span class="tag">DAILY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2601.09012" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <div class="link-item" style="background: #f5f5f5; color: #999;">No GitHub</div>
                                    <div class="link-item upvote-item">üî• 10</div>
                                    <div class="link-item stars-item">‚≠ê 0</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">TranslateGemma enhances Gemma 3's multilingual capabilities through two-stage fine-tuning with synthetic and human-translated data, achieving superior translation quality with improved efficiency.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                We present TranslateGemma, a suite of open <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=machine%20translation">machine translation</a> models based on the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Gemma%203">Gemma 3</a> foundation models. To enhance the inherent multilingual capabilities of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Gemma%203">Gemma 3</a> for the translation task, we employ a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=two-stage%20fine-tuning">two-stage fine-tuning</a> process. First, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=supervised%20fine-tuning">supervised fine-tuning</a> is performed using a rich mixture of high-quality large-scale synthetic parallel data generated via state-of-the-art models and human-translated parallel data. This is followed by a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=reinforcement%20learning">reinforcement learning</a> phase, where we optimize translation quality using an ensemble of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=reward%20models">reward models</a>, including <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=MetricX-QE">MetricX-QE</a> and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=AutoMQM">AutoMQM</a>, targeting translation quality. We demonstrate the effectiveness of TranslateGemma with human evaluation on the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=WMT25">WMT25</a> test set across 10 language pairs and with automatic evaluation on the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=WMT24%2B%2B">WMT24++</a> benchmark across 55 language pairs. Automatic metrics show consistent and substantial gains over the baseline <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Gemma%203">Gemma 3</a> models across all sizes. Notably, smaller TranslateGemma models often achieve performance comparable to larger baseline models, offering improved efficiency. We also show that TranslateGemma models retain strong multimodal capabilities, with enhanced performance on the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Vistra">Vistra</a> image translation benchmark. The release of the open TranslateGemma models aims to provide the research community with powerful and adaptable tools for <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=machine%20translation">machine translation</a>.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="DAILY" data-date="2026-01-13" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2601.08955" target="_blank" onclick="event.stopPropagation()">Imagine-then-Plan: Agent Learning from Adaptive Lookahead with World Models</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2026-01-13</div>
                                        <div class="tags"><span class="tag">DAILY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2601.08955" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <div class="link-item" style="background: #f5f5f5; color: #999;">No GitHub</div>
                                    <div class="link-item upvote-item">üî• 9</div>
                                    <div class="link-item stars-item">‚≠ê 0</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">Imagine-then-Plan framework enables agent learning through adaptive lookahead imagination, combining imagined trajectories with current observations to guide policy learning in complex task scenarios.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Recent advances in <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=world%20models">world models</a> have shown promise for modeling future dynamics of environmental states, enabling agents to reason and act without accessing real environments. Current methods mainly perform single-step or fixed-horizon rollouts, leaving their potential for complex task planning under-exploited. We propose Imagine-then-Plan (ITP), a unified framework for agent learning via <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=lookahead%20imagination">lookahead imagination</a>, where an agent's policy model interacts with the learned world model, yielding multi-step ``imagined'' trajectories. Since the imagination horizon may vary by tasks and stages, we introduce a novel <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=adaptive%20lookahead%20mechanism">adaptive lookahead mechanism</a> by trading off the ultimate goal and task progress. The resulting imagined trajectories provide rich signals about future consequences, such as achieved progress and potential conflicts, which are fused with current observations, formulating a partially observable and imaginable Markov decision process to guide <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=policy%20learning">policy learning</a>. We instantiate ITP with both training-free and reinforcement-trained variants. Extensive experiments across representative <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=agent%20benchmarks">agent benchmarks</a> demonstrate that ITP significantly outperforms competitive baselines. Further analyses validate that our adaptive lookahead largely enhances agents' reasoning capability, providing valuable insights into addressing broader, complex tasks.
                            </div>
                        </div>
                    </div>
                </div>
                </div>
                        </div>
                    </section>
                </div>
            </div>
        </main>

        <!-- Footer -->
        <footer class="footer">
            <div class="container">
                <p>&copy; 2025 Daily Digest. Curated with ‚ù§Ô∏è for the curious minds.</p>
            </div>
        </footer>
        <script src="static/script.js"></script>
    </body>
    </html>
    