
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <title>Daily Digest</title>
        <link rel="stylesheet" href="static/style.css">
    </head>
    <body>
        <!-- Header -->
        <header class="header">
            <div class="container">
                <nav class="nav">
                    <div class="logo">üì∞ Daily Digest</div>
                    <ul class="nav-links">
                        <li><a href="#astronomy">Astronomy</a></li>
                        <li><a href="#earth">Earth</a></li>
                        <li><a href="#tarot">Tarot</a></li>
                        <li><a href="#tech">Tech News</a></li>
                        <li><a href="#papers">Papers</a></li>
                    </ul>
                    <div class="date-badge">Tue, Aug 12, 2025</div>
                </nav>
            </div>
        </header>

        <!-- Main Content -->
        <main class="main-content">
            <div class="container">
                <!-- Hero Section -->
                <section class="hero">
                    <h1>Your Daily Tech & Science Digest</h1>
                    <p>Stay updated with the latest in astronomy, earth sciences, technology news, and cutting-edge research papers</p>
                </section>

                <!-- Content Grid -->
                <div class="content-grid">
                    <!-- APOD, Earth Observatory, and Tarot Row -->
                    <div class="three-column">
                        <!-- APOD Section -->
                        <section id="astronomy" class="card apod-card">
                            <div class="card-media" id="apod-media"></div>
                            <div class="card-content-wrapper">
                                <div class="card-header">
                                    <div class="card-icon">‚ú®</div>
                                    <div>
                                        <h2 class="card-title">Astronomy Picture</h2>
                                        <p class="card-subtitle">NASA's daily cosmic wonder</p>
                                    </div>
                                </div>
                                <div class="card-content" id="apod-content">
                                    
        <div style='display: flex; flex-direction: column; gap: 20px;'>
            <div style='display: none;'>
                <video controls class="apod-media-element"><source src="https://apod.nasa.gov/apod/image/2508/SunPlumes_psp.mp4"></video>
            </div>
            <div>
                <h3>Closest Ever Images Near the Sun</h3>
                <p><b> Explanation: </b> 
Everybody sees the Sun. 

Nobody's been there. 

Starting in 2018, though, NASA 
<a href="https://apod.nasa.gov/apod/ap180815.html">launch</a>ed the robotic 
<a href="https://parkersolarprobe.jhuapl.edu/">Parker Solar Probe</a> 
(PSP) to investigate regions near to the 
<a href="https://science.nasa.gov/sun/">Sun</a> for the first time. 

The <a href="https://svs.gsfc.nasa.gov/14865">featured time-lapse video</a> shows the view looking sideways from 
<a href="https://www.nature.com/articles/d41586-019-03665-3">behind PSP</a>'s Sun shield in December during the 
<a href="https://science.nasa.gov/science-research/heliophysics/nasas-parker-solar-probe-snaps-closest-ever-images-to-sun/">closest approach</a> of any human-made spacecraft to the Sun, 
<a href="https://en.wikipedia.org/wiki/Parker_Solar_Probe#Timeline">looping down</a> to only about five solar diameters above the 
<a href="https://apod.nasa.gov/apod/ap111106.html">Sun's hot surface</a>.  

The PSP's Wide Field Imager for Solar Probe 
(<a href="https://wispr.nrl.navy.mil/">WISPR</a>) cameras took these images over seven hours, but they are digitally compressed here into about 5 seconds. 

The <a href="https://apod.nasa.gov/apod/ap230516.html">solar corona</a>, 
including colliding 
<a href="https://www.nasa.gov/image-article/what-coronal-mass-ejection-or-cme/">coronal mass ejections</a> (CMEs), is visible here in 
<a href="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSQXmwoxMT4Txn3iwvV4PBzOc2O-WIVJ8wE-Q&amp;s">unprecedented detail</a>, with stars 
<a href="https://apod.nasa.gov/apod/ap240219.html">passing</a> far in the background. 

The Sun is not only Earth's dominant 
<a href="https://www.energy.gov/energy-sources?nrg_redirect=267706">energy source</a>, but its variable 
<a href="https://apod.nasa.gov/apod/ap000318.html">solar wind</a> also compresses Earth's atmosphere, triggers auroras, affects power grids, and can even 
<a href="https://hesperia.gsfc.nasa.gov/rhessi3/mission/science/the-impact-of-flares/index.html">damage orbiting communication satellites</a>.



<br><b> Tomorrow's picture: </b><a href="https://apod.nasa.gov/apod/ap250812.html">sky flow</a>
<p></p>
            </div>
        </div>
        
                                </div>
                            </div>
                        </section>

                        <!-- Earth Observatory -->
                        <section id="earth" class="card eo-card">
                            <div class="card-media" id="eo-media"></div>
                            <div class="card-content-wrapper">
                                <div class="card-header">
                                    <div class="card-icon">üåç</div>
                                    <div>
                                        <h2 class="card-title">Earth Observatory</h2>
                                        <p class="card-subtitle">Our planet from above</p>
                                    </div>
                                </div>
                                <div class="card-content" id="eo-content">
                                    
        <div style='display: flex; flex-direction: column; gap: 20px;'>
            <div style='display: none;'>
                <img src='https://eoimages.gsfc.nasa.gov/images/imagerecords/154000/154658/giffordfire_amo_20250808_th.jpg' alt='Earth Observatory' class='eo-media-element'>
            </div>
            <div>
                <h3>Sprawling Gifford Fire Scorches California</h3>
                <p>The fast-growing blaze charred more than 100,000 acres in the span of a week. <a href='https://earthobservatory.nasa.gov/images/154658/sprawling-gifford-fire-scorches-california' target='_blank'>[Read more]</a></p>
                <div style='background: #f0f8f0; padding: 15px; border-radius: 8px; border-left: 4px solid #2d5016; margin-top: 20px;'>
                    <p>
                        <strong>ü§ñ AI Summary:</strong> As of August 11, 2025, California experienced 5,362 wildfires burning 374,000 acres, exceeding the five-year average of 4,931 fires and 302,509 acres. The Gifford fire, ignited August 1 northwest of Santa Barbara, was the largest contributor, burning 119,214 acres by August 11 and classified as a megafire. Fueled by dry, gusty conditions and decades-unburned dense brush in Los Padres National Forest, it rapidly spread through mountainous terrain, damaging two structures and prompting evacuations near Santa Maria and San Luis Obispo. Satellite imagery captured extensive smoke over the Central Valley. Fire behavior analysts attributed its vigorous growth to increased atmospheric mixing height (rising from 8,000 to 15,000 feet), enhancing smoke column height and fire intensity. Despite its scale, the Gifford fire remained smaller than California‚Äôs top 20 historical fires, all exceeding 192,000 acres. Containment reached 33% with over 4,000 personnel deploying ground and aerial resources.
                    </p>
                </div>
            </div>
        </div>
        
                                </div>
                            </div>
                        </section>

                        <!-- Tarot Section -->
                        <section id="tarot" class="card tarot-card">
                            <div class="card-media" id="tarot-media"></div>
                            <div class="card-content-wrapper">
                                <div class="card-header">
                                    <div class="card-icon">üîÆ</div>
                                    <div>
                                        <h2 class="card-title">Daily Tarot</h2>
                                        <p class="card-subtitle">Your mystical guidance</p>
                                    </div>
                                </div>
                                <div class="card-content" id="tarot-content">
                                    
        <div style='display: flex; flex-direction: column; gap: 20px;'>
            <div style='display: none;'>
                <div class='tarot-card-container tarot-media-element'>
                    <div class='tarot-card' onclick='this.style.transform = this.style.transform.includes("rotateY(180deg)") ? "rotateY(0deg)" : "rotateY(180deg)"'>
                        <div>
                            <div style='font-size: 2.5rem; color: #d4af37; text-align: center; line-height: 1.2;'>
                                üîÆ<br>
                                <span style='font-size: 0.8rem; letter-spacing: 2px; font-weight: normal;'>DAILY TAROT</span><br>
                                <span style='font-size: 0.6rem; opacity: 0.8;'>Click to Reveal</span>
                            </div>
                        </div>
                        <div>
                            <img src='https://raw.githubusercontent.com/Haus226/daily-email/refs/heads/main/tarot_cards/Pictorial_Key_to_the_Tarot_18_The_Moon.jpg' alt='The Moon' />
                        </div>
                    </div>
                </div>
            </div>
            <div>
                <h3>The Moon</h3>                
                <div style='background: linear-gradient(135deg, #ffeaa7, #fdcb6e); padding: 5px; border-radius: 12px; border-left: 4px solid #e17055; margin-bottom: 20px; box-shadow: 0 4px 15px rgba(0,0,0,0.1);'>
                    <div style='font-weight: bold; color: #2d3436; margin-bottom: 5px; font-size: 1.2rem; text-transform: uppercase; letter-spacing: 1px;'>‚ú® Core Meaning</div>
                    <p style='margin: 0; color: #2d3436; line-height: 1.5; font-size: 1.0rem;'>Hidden enemies, danger, calumny, darkness, terror, deception, occult forces, error.</p>
                </div>
                <div style='background: linear-gradient(135deg, #ddd6fe, #c4b5fd); padding: 5px; border-radius: 12px; border-left: 4px solid #8b5cf6; box-shadow: 0 4px 15px rgba(0,0,0,0.1);'>
                    <div style='font-weight: bold; color: #2d3436; margin-bottom: 5px; font-size: 1.4rem; text-transform: uppercase; letter-spacing: 1px;'>üîç Daily Guidance</div>
                    <p style='margin: 0; color: #2d3436; line-height: 1.6; font-size: 1.3rem; font-weight: 500;'>"Trust your inner light, even in darkness, and face your fears to find your path."</p>
                </div>
    
            </div>
        </div>
        
                                </div>
                            </div>
                        </section>
                        
                        <!-- Navigation Arrows -->
                        <button class="card-nav prev" onclick="prevCard()" aria-label="Previous card">‚Äπ</button>
                        <button class="card-nav next" onclick="nextCard()" aria-label="Next card">‚Ä∫</button>
                    </div>

                    <!-- Hacker News Section -->
                    <section id="tech" class="card hn-card featured-section">
                        <div class="card-header">
                            <div class="card-icon">üî•</div>
                            <div>
                                <h2 class="card-title">Hacker News Top 10</h2>
                                <p class="card-subtitle">What's trending in tech</p>
                            </div>
                        </div>
                        <div class="card-content">
                            <ol class='hn-list'><li><a href='https://www.bbc.com/news/articles/cjr11qqvvwlo' target='_blank'>Wikipedia loses challenge against Online Safety Act</a></li><li><a href='https://www.al3rez.com/todo-txt-journey' target='_blank'>I tried every todo app and ended up with a .txt file</a></li><li><a href='https://www.arxiv.org/pdf/2508.06471' target='_blank'>GLM-4.5: Agentic, Reasoning, and Coding (Arc) Foundation Models [pdf]</a></li><li><a href='https://www.theverge.com/news/757461/microsoft-github-thomas-dohmke-resignation-coreai-team-transition' target='_blank'>GitHub is no longer independent at Microsoft after CEO resignation</a></li><li><a href='https://github.com/danterolle/tilf' target='_blank'>Show HN: I built an offline, open‚Äësource desktop Pixel Art Editor in Python</a></li><li><a href='https://dwyer.co.za/static/claude-code-is-all-you-need.html' target='_blank'>Claude Code is all you need</a></li><li><a href='https://github.com/AdoPi/wlgblock' target='_blank'>Show HN: Play Pok√©mon to unlock your Wayland session</a></li><li><a href='https://www.openssh.com/pq.html' target='_blank'>OpenSSH Post-Quantum Cryptography</a></li><li><a href='https://planetscale.com/blog/announcing-neki' target='_blank'>Neki ‚Äì sharded Postgres by the team behind Vitess</a></li><li><a href='https://kmicinski.com/functional-programming/2025/08/01/loops/' target='_blank'>Why tail-recursive functions are loops</a></li></ol>
                        </div>
                    </section>

                    <!-- Hugging Face Papers -->
                    <section id="papers" class="card hf-card-container featured-section">
                        <div class="card-header">
                            <div class="card-icon">üìö</div>
                            <div>
                                <h2 class="card-title">Latest Research Papers</h2>
                                <p class="card-subtitle">Cutting-edge AI & ML research</p>
                            </div>
                        </div>
                        
                        <!-- Paper Filters -->
                        <div class="paper-filters" id="hf-filters">
                            <button class="filter-btn active" data-tag="ALL" onclick="filterPapers('ALL')">All Papers</button>
                            <button class="filter-btn" data-tag="DAILY" onclick="filterPapers('DAILY')">Daily</button>
                            <button class="filter-btn" data-tag="WEEKLY" onclick="filterPapers('WEEKLY')">Weekly</button>
                            <button class="filter-btn" data-tag="MONTHLY" onclick="filterPapers('MONTHLY')">Monthly</button>
                            <button class="filter-btn" data-tag="TRENDING" onclick="filterPapers('TRENDING')">Trending</button>
                        </div>

                        <!-- Papers Content -->
                        <div class="card-content">
                            
        <div id="hf-grid" class="papers-grid">
        
                <div class="paper-card hf-card flip-card" data-tags="DAILY WEEKLY" data-date="2025-08-07" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2508.05614" target="_blank" onclick="event.stopPropagation()">OmniEAR: Benchmarking Agent Reasoning in Embodied Tasks</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-08-07</div>
                                        <div class="tags"><span class="tag">DAILY</span><span class="tag">WEEKLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2508.05614" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/ZJU-REAL/OmniEmbodied" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 8</div>
                                    <div class="link-item stars-item">‚≠ê 24</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">OmniEAR evaluates language models' embodied reasoning capabilities in physical interactions, tool usage, and multi-agent coordination, revealing performance degradation under constraints and highlighting architectural limitations.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Large language models excel at abstract reasoning but their capacity for
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=embodied%20agent%20reasoning">embodied agent reasoning</a> remains largely unexplored. We present <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=OmniEAR">OmniEAR</a>, a
comprehensive framework for evaluating how language models reason about
physical interactions, tool usage, and multi-agent coordination in embodied
tasks. Unlike existing benchmarks that provide predefined tool sets or explicit
collaboration directives, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=OmniEAR">OmniEAR</a> requires agents to dynamically acquire
capabilities and autonomously determine coordination strategies based on task
demands. Through <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=text-based%20environment%20representation">text-based environment representation</a>, we model continuous
physical properties and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=complex%20spatial%20relationships">complex spatial relationships</a> across 1,500 scenarios
spanning household and industrial domains. Our systematic evaluation reveals
severe performance degradation when models must reason from constraints: while
achieving 85-96% success with explicit instructions, performance drops to
56-85% for <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=tool%20reasoning">tool reasoning</a> and 63-85% for <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=implicit%20collaboration">implicit collaboration</a>, with compound
tasks showing over 50% failure rates. Surprisingly, complete environmental
information degrades coordination performance, indicating models cannot filter
task-relevant constraints. <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Fine-tuning">Fine-tuning</a> improves single-agent tasks dramatically
(0.6% to 76.3%) but yields minimal multi-agent gains (1.5% to 5.5%), exposing
fundamental architectural limitations. These findings demonstrate that embodied
reasoning poses fundamentally different challenges than current models can
address, establishing <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=OmniEAR">OmniEAR</a> as a rigorous benchmark for evaluating and
advancing <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=embodied%20AI%20systems">embodied AI systems</a>. Our code and data are included in the
supplementary materials and will be open-sourced upon acceptance.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="WEEKLY TRENDING MONTHLY" data-date="2025-08-08" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2508.06471" target="_blank" onclick="event.stopPropagation()">GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-08-08</div>
                                        <div class="tags"><span class="tag">MONTHLY</span><span class="tag">TRENDING</span><span class="tag">WEEKLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2508.06471" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/zai-org/GLM-4.5" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 78</div>
                                    <div class="link-item stars-item">‚≠ê 1.82k</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">GLM-4.5, a Mixture-of-Experts large language model with 355B parameters, achieves strong performance across agentic, reasoning, and coding tasks using multi-stage training and reinforcement learning.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                We present GLM-4.5, an open-source <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Mixture-of-Experts">Mixture-of-Experts</a> (MoE) large language
model with 355B total parameters and 32B activated parameters, featuring a
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=hybrid%20reasoning%20method">hybrid reasoning method</a> that supports both thinking and direct response modes.
Through <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=multi-stage%20training">multi-stage training</a> on 23T tokens and comprehensive post-training with
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=expert%20model%20iteration">expert model iteration</a> and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=reinforcement%20learning">reinforcement learning</a>, GLM-4.5 achieves strong
performance across agentic, reasoning, and coding (ARC) tasks, scoring 70.1% on
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=TAU-Bench">TAU-Bench</a>, 91.0% on <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=AIME%2024">AIME 24</a>, and 64.2% on <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=SWE-bench%20Verified">SWE-bench Verified</a>. With much fewer
parameters than several competitors, GLM-4.5 ranks 3rd overall among all
evaluated models and 2nd on <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=agentic%20benchmarks">agentic benchmarks</a>. We release both GLM-4.5 (355B
parameters) and a compact version, GLM-4.5-Air (106B parameters), to advance
research in reasoning and agentic AI systems. Code, models, and more
information are available at https://github.com/zai-org/GLM-4.5.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="DAILY" data-date="2025-08-08" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2508.06601" target="_blank" onclick="event.stopPropagation()">Deep Ignorance: Filtering Pretraining Data Builds Tamper-Resistant
  Safeguards into Open-Weight LLMs</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-08-08</div>
                                        <div class="tags"><span class="tag">DAILY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2508.06601" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <div class="link-item" style="background: #f5f5f5; color: #999;">No GitHub</div>
                                    <div class="link-item upvote-item">üî• -</div>
                                    <div class="link-item stars-item">‚≠ê 0</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">Data filtering during pretraining enhances LLM resistance to adversarial fine-tuning attacks without degrading unrelated capabilities, offering a promising defense mechanism for open-weight AI systems.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Open-weight%20AI%20systems">Open-weight AI systems</a> offer unique benefits, including enhanced
transparency, open research, and decentralized access. However, they are
vulnerable to <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=tampering%20attacks">tampering attacks</a> which can efficiently elicit harmful behaviors
by modifying weights or activations. Currently, there is not yet a robust
science of open-weight model risk management. Existing safety fine-tuning
methods and other post-training techniques have struggled to make LLMs
resistant to more than a few dozen steps of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=adversarial%20fine-tuning">adversarial fine-tuning</a>. In this
paper, we investigate whether filtering text about dual-use topics from
training data can prevent unwanted capabilities and serve as a more
tamper-resistant safeguard. We introduce a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=multi-stage%20pipeline">multi-stage pipeline</a> for scalable
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=data%20filtering">data filtering</a> and show that it offers a tractable and effective method for
minimizing <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=biothreat%20proxy%20knowledge">biothreat proxy knowledge</a> in LLMs. We pretrain multiple
6.9B-parameter models from scratch and find that they exhibit substantial
resistance to <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=adversarial%20fine-tuning">adversarial fine-tuning</a> attacks on up to 10,000 steps and 300M
tokens of biothreat-related text -- outperforming existing post-training
baselines by over an order of magnitude -- with no observed degradation to
unrelated capabilities. However, while filtered models lack internalized
dangerous knowledge, we find that they can still leverage such information when
it is provided in context (e.g., via search tool augmentation), demonstrating a
need for a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=defense-in-depth">defense-in-depth</a> approach. Overall, these findings help to establish
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=pretraining">pretraining</a> data curation as a promising layer of defense for open-weight AI
systems.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="WEEKLY TRENDING" data-date="2025-08-06" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2508.04825" target="_blank" onclick="event.stopPropagation()">Voost: A Unified and Scalable Diffusion Transformer for Bidirectional
  Virtual Try-On and Try-Off</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-08-06</div>
                                        <div class="tags"><span class="tag">TRENDING</span><span class="tag">WEEKLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2508.04825" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/nxnai/Voost" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 35</div>
                                    <div class="link-item stars-item">‚≠ê 58</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">Voost, a unified diffusion transformer framework, jointly learns virtual try-on and try-off, enhancing garment-body correspondence and achieving state-of-the-art results across benchmarks.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Virtual%20try-on">Virtual try-on</a> aims to synthesize a realistic image of a person wearing a
target garment, but accurately modeling <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=garment-body%20correspondence">garment-body correspondence</a> remains a
persistent challenge, especially under pose and appearance variation. In this
paper, we propose Voost - a unified and scalable framework that jointly learns
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=virtual%20try-on">virtual try-on</a> and try-off with a single <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=diffusion%20transformer">diffusion transformer</a>. By modeling
both tasks jointly, Voost enables each garment-person pair to supervise both
directions and supports flexible conditioning over generation direction and
garment category, enhancing garment-body relational reasoning without
task-specific networks, auxiliary losses, or additional labels. In addition, we
introduce two inference-time techniques: <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=attention%20temperature%20scaling">attention temperature scaling</a> for
robustness to resolution or mask variation, and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=self-corrective%20sampling">self-corrective sampling</a> that
leverages <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=bidirectional%20consistency">bidirectional consistency</a> between tasks. Extensive experiments
demonstrate that Voost achieves state-of-the-art results on both try-on and
try-off benchmarks, consistently outperforming strong baselines in alignment
accuracy, visual fidelity, and generalization.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="MONTHLY" data-date="2025-08-02" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2508.01191" target="_blank" onclick="event.stopPropagation()">Is Chain-of-Thought Reasoning of LLMs a Mirage? A Data Distribution Lens</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-08-02</div>
                                        <div class="tags"><span class="tag">MONTHLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2508.01191" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/ChengshuaiZhao0/DataAlchemy" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 191</div>
                                    <div class="link-item stars-item">‚≠ê 126</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">CoT reasoning in LLMs is found to be limited by the distribution discrepancy between training and test data, suggesting it is not a robust form of reasoning.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Chain-of-Thought">Chain-of-Thought</a> (CoT) prompting has been shown to improve Large Language
Model (LLM) performance on various tasks. With this approach, LLMs appear to
produce human-like reasoning steps before providing answers (a.k.a., CoT
reasoning), which often leads to the perception that they engage in deliberate
inferential processes. However, some initial findings suggest that CoT
reasoning may be more superficial than it appears, motivating us to explore
further. In this paper, we study <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=CoT%20reasoning">CoT reasoning</a> via a data distribution lens and
investigate if <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=CoT%20reasoning">CoT reasoning</a> reflects a structured <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=inductive%20bias">inductive bias</a> learned from
in-distribution data, allowing the model to conditionally generate reasoning
paths that approximate those seen during training. Thus, its effectiveness is
fundamentally bounded by the degree of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=distribution%20discrepancy">distribution discrepancy</a> between the
training data and the test queries. With this lens, we dissect <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=CoT%20reasoning">CoT reasoning</a>
via three dimensions: task, length, and format. To investigate each dimension,
we design <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=DataAlchemy">DataAlchemy</a>, an isolated and controlled environment to train LLMs
from scratch and systematically probe them under various distribution
conditions. Our results reveal that <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=CoT%20reasoning">CoT reasoning</a> is a brittle mirage that
vanishes when it is pushed beyond training distributions. This work offers a
deeper understanding of why and when <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=CoT%20reasoning">CoT reasoning</a> fails, emphasizing the
ongoing challenge of achieving genuine and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=generalizable%20reasoning">generalizable reasoning</a>.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="WEEKLY" data-date="2025-08-07" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2508.05731" target="_blank" onclick="event.stopPropagation()">InfiGUI-G1: Advancing GUI Grounding with Adaptive Exploration Policy
  Optimization</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-08-07</div>
                                        <div class="tags"><span class="tag">WEEKLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2508.05731" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/InfiXAI/InfiGUI-G1" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 17</div>
                                    <div class="link-item stars-item">‚≠ê 9</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">Adaptive Exploration Policy Optimization (AEPO) enhances semantic alignment in Multimodal Large Language Models (MLLMs) for GUI interaction, improving performance on benchmarks by up to 9.0% compared to RLVR.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                The emergence of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Multimodal%20Large%20Language%20Models">Multimodal Large Language Models</a> (MLLMs) has propelled the
development of autonomous agents that operate on Graphical User Interfaces
(<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=GUIs">GUIs</a>) using pure visual input. A fundamental challenge is robustly grounding
natural language instructions. This requires a precise spatial alignment, which
accurately locates the coordinates of each element, and, more critically, a
correct semantic alignment, which matches the instructions to the functionally
appropriate UI element. Although <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Reinforcement%20Learning%20with%20Verifiable%20Rewards">Reinforcement Learning with Verifiable Rewards</a>
(RLVR) has proven to be effective at improving spatial alignment for these
MLLMs, we find that inefficient exploration bottlenecks semantic alignment,
which prevent models from learning difficult semantic associations. To address
this exploration problem, we present <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Adaptive%20Exploration%20Policy%20Optimization">Adaptive Exploration Policy Optimization</a>
(AEPO), a new policy optimization framework. AEPO employs a multi-answer
generation strategy to enforce broader exploration, which is then guided by a
theoretically grounded <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Adaptive%20Exploration%20Reward">Adaptive Exploration Reward</a> (AER) function derived from
first principles of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=efficiency%20eta%3DU%2FC">efficiency eta=U/C</a>. Our AEPO-trained models, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=InfiGUI-G1-3B">InfiGUI-G1-3B</a>
and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=InfiGUI-G1-7B">InfiGUI-G1-7B</a>, establish new state-of-the-art results across multiple
challenging GUI grounding benchmarks, achieving significant relative
improvements of up to 9.0% against the naive RLVR baseline on benchmarks
designed to test generalization and semantic understanding. Resources are
available at https://github.com/InfiXAI/InfiGUI-G1.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="TRENDING" data-date="2025-08-05" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2508.03680" target="_blank" onclick="event.stopPropagation()">Agent Lightning: Train ANY AI Agents with Reinforcement Learning</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-08-05</div>
                                        <div class="tags"><span class="tag">TRENDING</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2508.03680" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/microsoft/agent-lightning" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 50</div>
                                    <div class="link-item stars-item">‚≠ê 528</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">Agent Lightning is a flexible RL framework for training LLMs in various agents, using a hierarchical RL algorithm and decoupling execution from training to handle complex interactions.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                We present Agent Lightning, a flexible and extensible framework that enables
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Reinforcement%20Learning">Reinforcement Learning</a> (RL)-based training of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Large%20Language%20Models">Large Language Models</a> (LLMs) for
any AI agent. Unlike existing methods that tightly couple RL training with
agent or rely on sequence concatenation with masking, Agent Lightning achieves
complete decoupling between agent execution and training, allowing seamless
integration with existing agents developed via diverse ways (e.g., using
frameworks like LangChain, OpenAI Agents SDK, AutoGen, and building from
scratch) with almost ZERO code modifications. By formulating agent execution as
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Markov%20decision%20process">Markov decision process</a>, we define an unified data interface and propose a
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=hierarchical%20RL%20algorithm">hierarchical RL algorithm</a>, LightningRL, which contains a credit assignment
module, allowing us to decompose trajectories generated by ANY agents into
training transition. This enables RL to handle complex interaction logic, such
as multi-agent scenarios and dynamic workflows. For the system design, we
introduce a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Training-Agent%20Disaggregation%20architecture">Training-Agent Disaggregation architecture</a>, and brings agent
observability frameworks into agent runtime, providing a standardized agent
finetuning interface. Experiments across <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=text-to-SQL">text-to-SQL</a>, retrieval-augmented
generation, and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=math%20tool-use%20tasks">math tool-use tasks</a> demonstrate stable, continuous
improvements, showcasing the framework's potential for real-world agent
training and deployment.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="MONTHLY TRENDING" data-date="2025-08-04" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2508.02324" target="_blank" onclick="event.stopPropagation()">Qwen-Image Technical Report</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-08-04</div>
                                        <div class="tags"><span class="tag">MONTHLY</span><span class="tag">TRENDING</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2508.02324" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/QwenLM/Qwen-Image" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 169</div>
                                    <div class="link-item stars-item">‚≠ê 2.51k</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">Qwen-Image, an image generation model, advances text rendering and image editing through a comprehensive data pipeline, progressive training, and dual-encoding mechanism.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                We present Qwen-Image, an image generation foundation model in the Qwen
series that achieves significant advances in complex text rendering and precise
image editing. To address the challenges of complex text rendering, we design a
comprehensive <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=data%20pipeline">data pipeline</a> that includes large-scale data collection,
filtering, annotation, synthesis, and balancing. Moreover, we adopt a
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=progressive%20training">progressive training</a> strategy that starts with non-text-to-text rendering,
evolves from simple to complex textual inputs, and gradually scales up to
paragraph-level descriptions. This <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=curriculum%20learning">curriculum learning</a> approach substantially
enhances the model's native text rendering capabilities. As a result,
Qwen-Image not only performs exceptionally well in alphabetic languages such as
English, but also achieves remarkable progress on more challenging logographic
languages like Chinese. To enhance image editing consistency, we introduce an
improved multi-task training paradigm that incorporates not only traditional
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=text-to-image">text-to-image</a> (T2I) and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=text-image-to-image">text-image-to-image</a> (TI2I) tasks but also
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=image-to-image">image-to-image</a> (I2I) reconstruction, effectively aligning the latent
representations between Qwen2.5-VL and MMDiT. Furthermore, we separately feed
the original image into Qwen2.5-VL and the VAE encoder to obtain semantic and
reconstructive representations, respectively. This <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=dual-encoding%20mechanism">dual-encoding mechanism</a>
enables the editing module to strike a balance between preserving semantic
consistency and maintaining <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=visual%20fidelity">visual fidelity</a>. Qwen-Image achieves
state-of-the-art performance, demonstrating its strong capabilities in both
image generation and editing across multiple benchmarks.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="MONTHLY" data-date="2025-08-06" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2508.04026" target="_blank" onclick="event.stopPropagation()">VeriGUI: Verifiable Long-Chain GUI Dataset</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-08-06</div>
                                        <div class="tags"><span class="tag">MONTHLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2508.04026" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/VeriGUI-Team/VeriGUI" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 134</div>
                                    <div class="link-item stars-item">‚≠ê 66</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">VeriGUI is a novel dataset for evaluating GUI agents in long-horizon tasks, emphasizing long-chain complexity and subtask-level verifiability.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Recent studies have delved into constructing autonomous agents capable of
performing complex <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Graphical%20User%20Interface%20(GUI)">Graphical User Interface (GUI)</a>-based computer tasks, with
the potential to revolutionize human-computer interaction. Despite encouraging
results, existing efforts mainly focus on short-term interactions and rely on
outcome-only verification, thereby limiting their scalability in real-world GUI
applications that demand long-horizon task decomposition and execution. In this
work, we introduce VeriGUI, a novel verifiable long-chain GUI dataset designed
to facilitate the development and evaluation of generalist <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=GUI%20agents">GUI agents</a> operating
in realistic computer environments. Our dataset emphasizes two critical
dimensions: (1) <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=long-chain%20complexity">long-chain complexity</a>, with tasks decomposed into a sequence of
interdependent subtasks spanning hundreds of steps, explicitly designed to
allow any subtask to serve as a valid starting point; and (2) subtask-level
verifiability, which enables diverse exploration strategies within each
subtask, while ensuring that each subtask-level goal remains verifiable and
consistent. The dataset consists of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=GUI%20task%20trajectories">GUI task trajectories</a> across both <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=desktop">desktop</a>
and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=web">web</a>, annotated by <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=human%20experts">human experts</a>. Extensive experiments on VeriGUI using
various agents with different foundation models reveal significant performance
gaps in handling <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=long-horizon%20tasks">long-horizon tasks</a>, highlighting the need for more robust
planning and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=decision-making%20capabilities">decision-making capabilities</a> in <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=GUI%20agents">GUI agents</a>.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="WEEKLY" data-date="2025-08-08" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2508.06433" target="_blank" onclick="event.stopPropagation()">Memp: Exploring Agent Procedural Memory</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-08-08</div>
                                        <div class="tags"><span class="tag">WEEKLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2508.06433" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <div class="link-item" style="background: #f5f5f5; color: #999;">No GitHub</div>
                                    <div class="link-item upvote-item">üî• 16</div>
                                    <div class="link-item stars-item">‚≠ê 0</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">Agents equipped with a learnable, updatable procedural memory system, Memp, achieve improved performance and efficiency across tasks by distilling past experiences into detailed instructions and higher-level abstractions.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Large%20Language%20Models">Large Language Models</a> (LLMs) based agents excel at diverse tasks, yet they
suffer from brittle <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=procedural%20memory">procedural memory</a> that is manually engineered or entangled
in static parameters. In this work, we investigate strategies to endow agents
with a learnable, updatable, and lifelong <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=procedural%20memory">procedural memory</a>. We propose <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Memp">Memp</a>
that distills past <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=agent%20trajectories">agent trajectories</a> into both fine-grained, step-by-step
instructions and higher-level, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=script-like%20abstractions">script-like abstractions</a>, and explore the impact
of different strategies for <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Build">Build</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Retrieval">Retrieval</a>, and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Update">Update</a> of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=procedural%20memory">procedural memory</a>.
Coupled with a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=dynamic%20regimen">dynamic regimen</a> that continuously <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=update">update</a>s, corrects, and
deprecates its contents, this repository evolves in lockstep with new
experience. Empirical evaluation on <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=TravelPlanner">TravelPlanner</a> and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=ALFWorld">ALFWorld</a> shows that as
the memory repository is refined, agents achieve steadily higher success rates
and greater efficiency on analogous tasks. Moreover, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=procedural%20memory">procedural memory</a> built
from a stronger model retains its value: migrating the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=procedural%20memory">procedural memory</a> to a
weaker model yields substantial <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=performance%20gains">performance gains</a>.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="TRENDING MONTHLY" data-date="2025-08-07" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2508.05004" target="_blank" onclick="event.stopPropagation()">R-Zero: Self-Evolving Reasoning LLM from Zero Data</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-08-07</div>
                                        <div class="tags"><span class="tag">MONTHLY</span><span class="tag">TRENDING</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2508.05004" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/Chengsong-Huang/R-Zero" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 96</div>
                                    <div class="link-item stars-item">‚≠ê 337</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">R-Zero is a self-evolving framework that autonomously generates and learns from its own training data, improving reasoning capabilities in LLMs without human-curated tasks.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Self-evolving%20Large%20Language%20Models">Self-evolving Large Language Models</a> (<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=LLMs">LLMs</a>) offer a scalable path toward
super-intelligence by autonomously generating, refining, and learning from
their own experiences. However, existing methods for training such models still
rely heavily on vast human-curated tasks and labels, typically via fine-tuning
or reinforcement learning, which poses a fundamental bottleneck to advancing AI
systems toward capabilities beyond human intelligence. To overcome this
limitation, we introduce <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=R-Zero">R-Zero</a>, a fully autonomous framework that generates
its own training data from scratch. Starting from a single base LLM, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=R-Zero">R-Zero</a>
initializes two independent models with distinct roles, a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Challenger">Challenger</a> and a
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Solver">Solver</a>. These models are optimized separately and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=co-evolve">co-evolve</a> through
interaction: the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Challenger">Challenger</a> is rewarded for proposing tasks near the edge of
the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Solver">Solver</a> capability, and the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Solver">Solver</a> is rewarded for solving increasingly
challenging tasks posed by the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Challenger">Challenger</a>. This process yields a targeted,
self-improving curriculum without any pre-existing tasks and labels.
Empirically, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=R-Zero">R-Zero</a> substantially improves reasoning capability across
different backbone <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=LLMs">LLMs</a>, e.g., boosting the Qwen3-4B-Base by +6.49 on
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=math-reasoning%20benchmarks">math-reasoning benchmarks</a> and +7.54 on <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=general-domain%20reasoning%20benchmarks">general-domain reasoning benchmarks</a>.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="WEEKLY" data-date="2025-08-08" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2508.05988" target="_blank" onclick="event.stopPropagation()">Pruning the Unsurprising: Efficient Code Reasoning via First-Token
  Surprisal</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-08-08</div>
                                        <div class="tags"><span class="tag">WEEKLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2508.05988" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/Zengwh02/ASAP" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 12</div>
                                    <div class="link-item stars-item">‚≠ê 3</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">ASAP, a novel coarse-to-fine framework, compresses Chain-of-Thought in code reasoning by preserving core structure and essential steps, reducing costs and improving efficiency.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Recently, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Large%20Reasoning%20Models">Large Reasoning Models</a> (LRMs) have demonstrated remarkable
capabilities in code reasoning by scaling up the length of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Chain-of-Thought">Chain-of-Thought</a>
(CoT). However, excessively long reasoning traces introduce substantial
challenges in terms of training cost, inference latency, and deployment
feasibility. While various <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=CoT%20compression">CoT compression</a> approaches have emerged to address
this challenge, they face inherent trade-offs: token-level methods often
disrupt syntactic and logical coherence, while step-level methods based on
perplexity fail to reliably capture the logically critical reasoning steps. In
this paper, we propose ASAP (Anchor-guided, Surprisal-based Pruning), a novel
coarse-to-fine framework for <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=CoT%20compression">CoT compression</a>. ASAP first performs anchor-guided
pruning to preserve the core reasoning structure, which efficiently reduces the
search space for subsequent processing. It then enables a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=logic-aware%20pruning">logic-aware pruning</a>
by selecting logically essential reasoning steps based on a novel first-token
surprisal metric. Finally, ASAP teaches models to autonomously generate and
leverage these concise CoTs at inference time, enabling efficient reasoning in
coding tasks. Experiments show that ASAP achieves state-of-the-art accuracy
across multiple code generation benchmarks while substantially reducing
training and inference costs. On the challenging <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=LiveCodeBench">LiveCodeBench</a> v4_v5 benchmark,
our approach reduces token generation by 23.5% and inference latency by 43.5%
compared to the strongest baseline, while achieving a competitive accuracy of
36.19% in <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Pass%401">Pass@1</a>. Our results highlight a promising direction for building
powerful and efficient LRMs.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="MONTHLY TRENDING" data-date="2025-08-07" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2508.05629" target="_blank" onclick="event.stopPropagation()">On the Generalization of SFT: A Reinforcement Learning Perspective with
  Reward Rectification</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-08-07</div>
                                        <div class="tags"><span class="tag">MONTHLY</span><span class="tag">TRENDING</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2508.05629" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/yongliang-wu/DFT" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 120</div>
                                    <div class="link-item stars-item">‚≠ê 182</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">Dynamic Fine-Tuning (DFT) improves the generalization of Large Language Models (LLMs) by dynamically rescaling gradients, outperforming standard Supervised Fine-Tuning (SFT) and showing competitive results in offline reinforcement learning.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                We present a simple yet theoretically motivated improvement to Supervised
Fine-Tuning (SFT) for the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Large%20Language%20Model">Large Language Model</a> (LLM), addressing its limited
generalization compared to <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=reinforcement%20learning">reinforcement learning</a> (RL). Through mathematical
analysis, we reveal that standard SFT gradients implicitly encode a problematic
reward structure that may severely restrict the generalization capabilities of
model. To rectify this, we propose <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Dynamic%20Fine-Tuning">Dynamic Fine-Tuning</a> (DFT), stabilizing
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=gradient%20updates">gradient updates</a> for each token by dynamically rescaling the objective function
with the probability of this token. Remarkably, this single-line code change
significantly outperforms standard SFT across multiple challenging benchmarks
and base models, demonstrating greatly improved generalization. Additionally,
our approach shows competitive results in <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=offline%20RL">offline RL</a> settings, offering an
effective yet simpler alternative. This work bridges theoretical insight and
practical solutions, substantially advancing SFT performance. The code will be
available at https://github.com/yongliang-wu/DFT.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="MONTHLY" data-date="2025-08-04" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2508.02193" target="_blank" onclick="event.stopPropagation()">Seed Diffusion: A Large-Scale Diffusion Language Model with High-Speed
  Inference</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-08-04</div>
                                        <div class="tags"><span class="tag">MONTHLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2508.02193" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <div class="link-item" style="background: #f5f5f5; color: #999;">No GitHub</div>
                                    <div class="link-item upvote-item">üî• 117</div>
                                    <div class="link-item stars-item">‚≠ê 0</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">Seed Diffusion Preview, a discrete-state diffusion language model, achieves fast inference speeds through parallel generation, outperforming Mercury and Gemini Diffusion in speed and quality.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                We present <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Seed%20Diffusion%20Preview">Seed Diffusion Preview</a>, a large-scale language model based on
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=discrete-state%20diffusion">discrete-state diffusion</a>, offering remarkably fast inference speed. Thanks to
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=non-sequential">non-sequential</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=parallel%20generation">parallel generation</a>, discrete diffusion models provide a
notable speedup to mitigate the inherent latency of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=token-by-token%20decoding">token-by-token decoding</a>, as
demonstrated recently (e.g., Mercury Coder, Gemini Diffusion). Seed Diffusion
Preview achieves an inference speed of 2,146 token/s over <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=H20%20GPUs">H20 GPUs</a> while
maintaining competitive performance across a sweep of standard code evaluation
benchmarks, significantly faster than contemporary Mercury and Gemini
Diffusion, establishing new state of the art on the speed-quality Pareto
frontier for code models.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="WEEKLY" data-date="2025-08-05" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2508.03616" target="_blank" onclick="event.stopPropagation()">Hidden Dynamics of Massive Activations in Transformer Training</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-08-05</div>
                                        <div class="tags"><span class="tag">WEEKLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2508.03616" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <div class="link-item" style="background: #f5f5f5; color: #999;">No GitHub</div>
                                    <div class="link-item upvote-item">üî• 9</div>
                                    <div class="link-item stars-item">‚≠ê 0</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">The emergence of massive activations in transformer models follows predictable patterns that can be modeled and predicted using architectural specifications, impacting model stability, training duration, and optimization.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Massive%20activations">Massive activations</a> are scalar values in <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=transformer%20hidden%20states">transformer hidden states</a> that
achieve values orders of magnitude larger than typical activations and have
been shown to be critical for model functionality. While prior work has
characterized these phenomena in fully trained models, the temporal dynamics of
their emergence during training remain poorly understood. We present the first
comprehensive analysis of massive activation development throughout transformer
training, using the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Pythia%20model%20family">Pythia model family</a> as our testbed. Through systematic
analysis of various model sizes across multiple training checkpoints, we
demonstrate that massive activation emergence follows predictable mathematical
patterns that can be accurately modeled using an exponentially-modulated
logarithmic function with five key parameters. We develop a machine learning
framework to predict these mathematical parameters from architectural
specifications alone, achieving high accuracy for steady-state behavior and
moderate accuracy for emergence timing and magnitude. These findings enable
architects to predict and potentially control key aspects of massive activation
emergence through design choices, with significant implications for model
stability, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=training%20cycle%20length">training cycle length</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=interpretability">interpretability</a>, and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=optimization">optimization</a>. Our
findings demonstrate that the emergence of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=massive%20activations">massive activations</a> is governed by
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=model%20design">model design</a> and can be anticipated, and potentially controlled, before
training begins.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="WEEKLY" data-date="2025-08-07" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2508.05547" target="_blank" onclick="event.stopPropagation()">Adapting Vision-Language Models Without Labels: A Comprehensive Survey</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-08-07</div>
                                        <div class="tags"><span class="tag">WEEKLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2508.05547" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/tim-learn/Awesome-LabelFree-VLMs" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 6</div>
                                    <div class="link-item stars-item">‚≠ê 16</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">A comprehensive survey of unsupervised adaptation methods for Vision-Language Models (VLMs) categorizes approaches based on the availability of unlabeled visual data and discusses methodologies, benchmarks, and future research directions.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Vision-Language%20Models">Vision-Language Models</a> (VLMs) have demonstrated remarkable generalization
capabilities across a wide range of tasks. However, their performance often
remains suboptimal when directly applied to specific downstream scenarios
without task-specific adaptation. To enhance their utility while preserving
data efficiency, recent research has increasingly focused on unsupervised
adaptation methods that do not rely on labeled data. Despite the growing
interest in this area, there remains a lack of a unified, task-oriented survey
dedicated to unsupervised VLM adaptation. To bridge this gap, we present a
comprehensive and structured overview of the field. We propose a taxonomy based
on the availability and nature of unlabeled visual data, categorizing existing
approaches into four key paradigms: <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Data-Free%20Transfer">Data-Free Transfer</a> (no data), Unsupervised
Domain Transfer (abundant data), <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Episodic%20Test-Time%20Adaptation">Episodic Test-Time Adaptation</a> (batch data),
and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Online%20Test-Time%20Adaptation">Online Test-Time Adaptation</a> (streaming data). Within this framework, we
analyze core methodologies and adaptation strategies associated with each
paradigm, aiming to establish a systematic understanding of the field.
Additionally, we review representative benchmarks across diverse applications
and highlight open challenges and promising directions for future research. An
actively maintained repository of relevant literature is available at
https://github.com/tim-learn/Awesome-LabelFree-VLMs.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="WEEKLY" data-date="2025-08-04" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2508.02831" target="_blank" onclick="event.stopPropagation()">GENIE: Gaussian Encoding for Neural Radiance Fields Interactive Editing</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-08-04</div>
                                        <div class="tags"><span class="tag">WEEKLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2508.02831" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/MikolajZielinski/genie" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 6</div>
                                    <div class="link-item stars-item">‚≠ê 8</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">GENIE combines NeRF's photorealistic rendering with Gaussian Splatting's editable and structured representation, enabling real-time, locality-aware editing and integration with physics-based simulation.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Neural%20Radiance%20Fields">Neural Radiance Fields</a> (NeRF) and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Gaussian%20Splatting">Gaussian Splatting</a> (GS) have recently
transformed 3D scene representation and rendering. NeRF achieves high-fidelity
novel view synthesis by learning <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=volumetric%20representations">volumetric representations</a> through neural
networks, but its <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=implicit%20encoding">implicit encoding</a> makes editing and physical interaction
challenging. In contrast, GS represents scenes as <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=explicit%20collections">explicit collections</a> of
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Gaussian%20primitives">Gaussian primitives</a>, enabling <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=real-time%20rendering">real-time rendering</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=faster%20training">faster training</a>, and more
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=intuitive%20manipulation">intuitive manipulation</a>. This explicit structure has made GS particularly
well-suited for <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=interactive%20editing">interactive editing</a> and integration with physics-based
simulation. In this paper, we introduce GENIE (Gaussian Encoding for Neural
Radiance Fields <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Interactive%20Editing">Interactive Editing</a>), a hybrid model that combines the
photorealistic rendering quality of NeRF with the editable and structured
representation of GS. Instead of using spherical harmonics for appearance
modeling, we assign each Gaussian a trainable <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=feature%20embedding">feature embedding</a>. These
embeddings are used to condition a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=NeRF%20network">NeRF network</a> based on the k nearest
Gaussians to each query point. To make this conditioning efficient, we
introduce <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Ray-Traced%20Gaussian%20Proximity%20Search">Ray-Traced Gaussian Proximity Search</a> (<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=RT-GPS">RT-GPS</a>), a fast nearest
Gaussian search based on a modified ray-tracing pipeline. We also integrate a
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=multi-resolution%20hash%20grid">multi-resolution hash grid</a> to initialize and update Gaussian features.
Together, these components enable real-time, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=locality-aware%20editing">locality-aware editing</a>: as
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Gaussian%20primitives">Gaussian primitives</a> are repositioned or modified, their interpolated influence
is immediately reflected in the rendered output. By combining the strengths of
implicit and explicit representations, GENIE supports intuitive scene
manipulation, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=dynamic%20interaction">dynamic interaction</a>, and compatibility with physical simulation,
bridging the gap between geometry-based editing and neural rendering. The code
can be found under (https://github.com/MikolajZielinski/genie)
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="WEEKLY" data-date="2025-08-07" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2508.05502" target="_blank" onclick="event.stopPropagation()">MELLA: Bridging Linguistic Capability and Cultural Groundedness for
  Low-Resource Language MLLMs</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-08-07</div>
                                        <div class="tags"><span class="tag">WEEKLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2508.05502" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <div class="link-item" style="background: #f5f5f5; color: #999;">No GitHub</div>
                                    <div class="link-item upvote-item">üî• 4</div>
                                    <div class="link-item stars-item">‚≠ê 0</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">MELLA, a multimodal, multilingual dataset, enhances MLLMs in low-resource languages by improving linguistic capability and cultural groundedness through native web alt-text and MLLM-generated captions.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Multimodal%20Large%20Language%20Models">Multimodal Large Language Models</a> (<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=MLLMs">MLLMs</a>) have shown remarkable performance in
high-resource languages. However, their effectiveness diminishes significantly
in the contexts of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=low-resource%20languages">low-resource languages</a>. Current <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=multilingual%20enhancement">multilingual enhancement</a>
methods are often limited to text modality or rely solely on machine
translation. While such approaches help models acquire basic linguistic
capabilities and produce "thin descriptions", they neglect the importance of
multimodal informativeness and cultural groundedness, both of which are crucial
for serving low-resource language users effectively. To bridge this gap, in
this study, we identify two significant objectives for a truly effective MLLM
in low-resource language settings, namely 1) linguistic capability and 2)
cultural groundedness, placing special emphasis on <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=cultural%20awareness">cultural awareness</a>. To
achieve these dual objectives, we propose a dual-source strategy that guides
the collection of data tailored to each goal, sourcing <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=native%20web%20alt-text">native web alt-text</a> for
culture and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=MLLM-generated%20captions">MLLM-generated captions</a> for linguistics. As a concrete
implementation, we introduce MELLA, a multimodal, multilingual dataset.
Experiment results show that after fine-tuning on MELLA, there is a general
performance improvement for the eight languages on various MLLM backbones, with
models producing "<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=thick%20descriptions">thick descriptions</a>". We verify that the performance gains are
from both cultural knowledge enhancement and linguistic capability enhancement.
Our dataset can be found at https://opendatalab.com/applyMultilingualCorpus.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="MONTHLY" data-date="2025-07-31" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2507.23726" target="_blank" onclick="event.stopPropagation()">Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-07-31</div>
                                        <div class="tags"><span class="tag">MONTHLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2507.23726" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/ByteDance-Seed/Seed-Prover" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 106</div>
                                    <div class="link-item stars-item">‚≠ê 257</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">Seed-Prover, a lemma-style reasoning model using Lean, achieves high performance in formal theorem proving and automated mathematical reasoning through iterative refinement and specialized geometry support.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                LLMs have demonstrated strong mathematical reasoning abilities by leveraging
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=reinforcement%20learning">reinforcement learning</a> with <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=long%20chain-of-thought">long chain-of-thought</a>, yet they continue to
struggle with <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=theorem%20proving">theorem proving</a> due to the lack of clear supervision signals when
solely using natural language. Dedicated domain-specific languages like Lean
provide clear supervision via <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=formal%20verification">formal verification</a> of proofs, enabling effective
training through <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=reinforcement%20learning">reinforcement learning</a>. In this work, we propose
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Seed-Prover">Seed-Prover</a>, a lemma-style whole-proof reasoning model. <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Seed-Prover">Seed-Prover</a>
can iteratively refine its proof based on Lean feedback, proved lemmas, and
self-summarization. To solve IMO-level contest problems, we design three
test-time inference strategies that enable both deep and broad reasoning.
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Seed-Prover">Seed-Prover</a> proves 78.1% of formalized past IMO problems, saturates <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=MiniF2F">MiniF2F</a>,
and achieves over 50\% on <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=PutnamBench">PutnamBench</a>, outperforming the previous
state-of-the-art by a large margin. To address the lack of geometry support in
Lean, we introduce a geometry reasoning engine <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Seed-Geometry">Seed-Geometry</a>, which
outperforms previous formal geometry engines. We use these two systems to
participate in IMO 2025 and fully prove 5 out of 6 problems. This work
represents a significant advancement in <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=automated%20mathematical%20reasoning">automated mathematical reasoning</a>,
demonstrating the effectiveness of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=formal%20verification">formal verification</a> with long
chain-of-thought reasoning.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="TRENDING" data-date="2025-07-30" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2507.22827" target="_blank" onclick="event.stopPropagation()">ScreenCoder: Advancing Visual-to-Code Generation for Front-End
  Automation via Modular Multimodal Agents</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-07-30</div>
                                        <div class="tags"><span class="tag">TRENDING</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2507.22827" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/leigest519/ScreenCoder" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 88</div>
                                    <div class="link-item stars-item">‚≠ê 1.02k</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">A modular multi-agent framework improves UI-to-code generation by integrating vision-language models, hierarchical layout planning, and adaptive prompt-based synthesis, achieving state-of-the-art performance.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Automating the transformation of user interface (UI) designs into front-end
code holds significant promise for accelerating software development and
democratizing design workflows. While recent large language models (LLMs) have
demonstrated progress in text-to-code generation, many existing approaches rely
solely on natural language prompts, limiting their effectiveness in capturing
spatial layout and visual design intent. In contrast, UI development in
practice is inherently <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=multimodal">multimodal</a>, often starting from visual sketches or
mockups. To address this gap, we introduce a modular multi-agent framework that
performs <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=UI-to-code%20generation">UI-to-code generation</a> in three interpretable stages: grounding,
planning, and generation. The <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=grounding%20agent">grounding agent</a> uses a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=vision-language%20model">vision-language model</a> to
detect and label UI components, the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=planning%20agent">planning agent</a> constructs a hierarchical
layout using front-end engineering priors, and the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=generation%20agent">generation agent</a> produces
HTML/CSS code via <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=adaptive%20prompt-based%20synthesis">adaptive prompt-based synthesis</a>. This design improves
robustness, interpretability, and fidelity over end-to-end black-box methods.
Furthermore, we extend the framework into a scalable <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=data%20engine">data engine</a> that
automatically produces large-scale image-code pairs. Using these synthetic
examples, we fine-tune and reinforce an open-source VLM, yielding notable gains
in UI understanding and code quality. Extensive experiments demonstrate that
our approach achieves state-of-the-art performance in layout accuracy,
structural coherence, and code correctness. Our code is made publicly available
at https://github.com/leigest519/ScreenCoder.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="WEEKLY" data-date="2025-08-06" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2508.04482" target="_blank" onclick="event.stopPropagation()">OS Agents: A Survey on MLLM-based Agents for General Computing Devices
  Use</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-08-06</div>
                                        <div class="tags"><span class="tag">WEEKLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2508.04482" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/OS-Agent-Survey/OS-Agent-Survey" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 3</div>
                                    <div class="link-item stars-item">‚≠ê 326</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">(No summary available)</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                The dream to create AI assistants as capable and versatile as the fictional
J.A.R.V.I.S from Iron Man has long captivated imaginations. With the evolution
of (multi-modal) large language models ((M)LLMs), this dream is closer to
reality, as (M)LLM-based Agents using computing devices (e.g., computers and
mobile phones) by operating within the environments and interfaces (e.g.,
Graphical User Interface (GUI)) provided by operating systems (OS) to automate
tasks have significantly advanced. This paper presents a comprehensive survey
of these advanced agents, designated as OS Agents. We begin by elucidating the
fundamentals of OS Agents, exploring their key components including the
environment, observation space, and action space, and outlining essential
capabilities such as understanding, planning, and grounding. We then examine
methodologies for constructing OS Agents, focusing on domain-specific
foundation models and agent frameworks. A detailed review of evaluation
protocols and benchmarks highlights how OS Agents are assessed across diverse
tasks. Finally, we discuss current challenges and identify promising directions
for future research, including safety and privacy, personalization and
self-evolution. This survey aims to consolidate the state of OS Agents
research, providing insights to guide both academic inquiry and industrial
development. An open-source GitHub repository is maintained as a dynamic
resource to foster further innovation in this field. We present a 9-page
version of our work, accepted by ACL 2025, to provide a concise overview to the
domain.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="TRENDING" data-date="2024-03-20" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2403.13372" target="_blank" onclick="event.stopPropagation()">LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2024-03-20</div>
                                        <div class="tags"><span class="tag">TRENDING</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2403.13372" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/hiyouga/LLaMA-Factory" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 116</div>
                                    <div class="link-item stars-item">‚≠ê 56k</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">LlamaFactory is a unified framework enabling efficient fine-tuning of large language models across various tasks using a web-based user interface.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Efficient%20fine-tuning">Efficient fine-tuning</a> is vital for adapting <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=large%20language%20models">large language models</a> (LLMs) to
downstream tasks. However, it requires non-trivial efforts to implement these
methods on different models. We present <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=LlamaFactory">LlamaFactory</a>, a unified framework that
integrates a suite of cutting-edge efficient training methods. It allows users
to flexibly customize the fine-tuning of 100+ LLMs without the need for coding
through the built-in web UI <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=LlamaBoard">LlamaBoard</a>. We empirically validate the efficiency
and effectiveness of our framework on <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=language%20modeling">language modeling</a> and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=text%20generation">text generation</a>
tasks. It has been released at https://github.com/hiyouga/<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=LLaMA">LLaMA</a>-Factory and
already received over 13,000 stars and 1,600 forks.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="WEEKLY" data-date="2025-08-04" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2508.02095" target="_blank" onclick="event.stopPropagation()">VLM4D: Towards Spatiotemporal Awareness in Vision Language Models</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-08-04</div>
                                        <div class="tags"><span class="tag">WEEKLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2508.02095" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <div class="link-item" style="background: #f5f5f5; color: #999;">No GitHub</div>
                                    <div class="link-item upvote-item">üî• 3</div>
                                    <div class="link-item stars-item">‚≠ê 0</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">A benchmark evaluates VLMs' spatiotemporal reasoning, identifying gaps and suggesting improvements like 4D feature field reconstruction and fine-tuning.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Vision language models (<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=VLMs">VLMs</a>) have shown remarkable capabilities in
integrating linguistic and visual reasoning but remain fundamentally limited in
understanding dynamic spatiotemporal interactions. Humans effortlessly track
and reason about object movements, rotations, and perspective shifts-abilities
essential for robust dynamic real-world understanding yet notably lacking in
current <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=VLMs">VLMs</a>. In this paper, we introduce <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=VLM4D">VLM4D</a>, the first benchmark
specifically designed to evaluate the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=spatiotemporal%20reasoning">spatiotemporal reasoning</a> capabilities of
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=VLMs">VLMs</a>. Our benchmark comprises diverse real-world and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=synthetic%20videos">synthetic videos</a>
accompanied by carefully curated <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=question-answer%20pairs">question-answer pairs</a> emphasizing
translational and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=rotational%20motions">rotational motions</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=perspective%20awareness">perspective awareness</a>, and motion
continuity. Through comprehensive evaluations of state-of-the-art open and
closed-source <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=VLMs">VLMs</a>, we identify significant performance gaps compared to human
baselines, highlighting fundamental deficiencies in existing models. Extensive
analysis reveals that <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=VLMs">VLMs</a> struggle particularly with integrating multiple
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=visual%20cues">visual cues</a> and maintaining <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=temporal%20coherence">temporal coherence</a>. We further explore promising
directions, such as leveraging <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=4D%20feature%20field%20reconstruction">4D feature field reconstruction</a> and targeted
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=spatiotemporal%20supervised%20fine-tuning">spatiotemporal supervised fine-tuning</a>, demonstrating their effectiveness in
enhancing spatiotemporal comprehension. Our work aims to encourage deeper
exploration into improving <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=VLMs">VLMs</a>' spatial and temporal grounding, paving the way
towards more capable and reliable visual intelligence for dynamic environments.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="MONTHLY" data-date="2025-08-01" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2508.00414" target="_blank" onclick="event.stopPropagation()">Cognitive Kernel-Pro: A Framework for Deep Research Agents and Agent
  Foundation Models Training</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-08-01</div>
                                        <div class="tags"><span class="tag">MONTHLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2508.00414" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/Tencent/CognitiveKernel-Pro" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 83</div>
                                    <div class="link-item stars-item">‚≠ê 243</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">Cognitive Kernel-Pro is an open-source multi-module agent framework that enhances AI agent robustness and performance through data curation and novel test-time strategies, achieving state-of-the-art results.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                General AI Agents are increasingly recognized as foundational frameworks for
the next generation of artificial intelligence, enabling complex reasoning, web
interaction, coding, and autonomous research capabilities. However, current
agent systems are either closed-source or heavily reliant on a variety of paid
APIs and proprietary tools, limiting accessibility and reproducibility for the
research community. In this work, we present Cognitive Kernel-Pro, a
fully open-source and (to the maximum extent) free multi-module agent framework
designed to democratize the development and evaluation of advanced AI agents.
Within Cognitive Kernel-Pro, we systematically investigate the curation of
high-quality training data for <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Agent%20Foundation%20Models">Agent Foundation Models</a>, focusing on the
construction of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=queries">queries</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=trajectories">trajectories</a>, and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=verifiable%20answers">verifiable answers</a> across four key
domains: web, file, code, and general reasoning. Furthermore, we explore novel
strategies for <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=agent%20test-time%20reflection">agent test-time reflection</a> and voting to enhance agent
robustness and performance. We evaluate Cognitive Kernel-Pro on <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=GAIA">GAIA</a>, achieving
state-of-the-art results among open-source and free agents. Notably, our
8B-parameter open-source model surpasses previous leading systems such as
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=WebDancer">WebDancer</a> and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=WebSailor">WebSailor</a>, establishing a new performance standard for
accessible, high-capability AI agents. Code is available at
https://github.com/Tencent/CognitiveKernel-Pro
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="WEEKLY" data-date="2025-08-02" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2508.01242" target="_blank" onclick="event.stopPropagation()">MeshLLM: Empowering Large Language Models to Progressively Understand
  and Generate 3D Mesh</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-08-02</div>
                                        <div class="tags"><span class="tag">WEEKLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2508.01242" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <div class="link-item" style="background: #f5f5f5; color: #999;">No GitHub</div>
                                    <div class="link-item upvote-item">üî• 3</div>
                                    <div class="link-item stars-item">‚≠ê 0</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">MeshLLM uses large language models to generate and understand text-serialized 3D meshes by decomposing them into meaningful subunits and training with local mesh assembly strategies.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                We present <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=MeshLLM">MeshLLM</a>, a novel framework that leverages <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=large%20language%20models">large language models</a>
(<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=LLMs">LLMs</a>) to understand and generate text-serialized <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=3D%20meshes">3D meshes</a>. Our approach
addresses key limitations in existing methods, including the limited dataset
scale when catering to <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=LLMs">LLMs</a>' token length and the loss of 3D structural
information during mesh serialization. We introduce a Primitive-Mesh
decomposition strategy, which divides <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=3D%20meshes">3D meshes</a> into structurally meaningful
subunits. This enables the creation of a large-scale dataset with 1500k+
samples, almost 50 times larger than previous methods, which aligns better with
the LLM scaling law principles. Furthermore, we propose inferring face
connectivity from vertices and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=local%20mesh%20assembly">local mesh assembly</a> training strategies,
significantly enhancing the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=LLMs">LLMs</a>' ability to capture <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=mesh%20topology">mesh topology</a> and spatial
structures. Experiments show that <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=MeshLLM">MeshLLM</a> outperforms the state-of-the-art
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=LLaMA-Mesh">LLaMA-Mesh</a> in both mesh generation quality and shape understanding,
highlighting its great potential in processing text-serialized <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=3D%20meshes">3D meshes</a>.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="MONTHLY" data-date="2025-07-24" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2508.02694" target="_blank" onclick="event.stopPropagation()">Efficient Agents: Building Effective Agents While Reducing Cost</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-07-24</div>
                                        <div class="tags"><span class="tag">MONTHLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2508.02694" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/OPPO-PersonalAI/OAgents" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 73</div>
                                    <div class="link-item stars-item">‚≠ê 158</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">A study on the efficiency-effectiveness trade-off in LLM-driven agent systems identifies optimal agent framework design to reduce costs while maintaining performance.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                The remarkable capabilities of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Large%20Language%20Model">Large Language Model</a> (<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=LLM">LLM</a>)-driven agents have
enabled sophisticated systems to tackle complex, multi-step tasks, but their
escalating costs threaten scalability and accessibility. This work presents the
first systematic study of the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=efficiency-effectiveness%20trade-off">efficiency-effectiveness trade-off</a> in modern
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=agent%20systems">agent systems</a>, addressing the critical need for cost-effective designs without
sacrificing performance. We investigate three key questions: (1) How much
complexity do <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=agentic%20tasks">agentic tasks</a> inherently require? (2) When do additional modules
yield diminishing returns? (3) How much efficiency can be gained through the
design of efficient <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=agent%20framework">agent framework</a>s? Through an empirical analysis on the GAIA
benchmark, we evaluate the impact of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=LLM%20backbone">LLM backbone</a> selection, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=agent%20framework">agent framework</a>
designs, and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=test-time%20scaling%20strategies">test-time scaling strategies</a>. Using the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=cost-of-pass">cost-of-pass</a> metric, we
quantify the efficiency-performance trade-off across these dimensions. Our
findings inform the development of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Efficient%20Agents">Efficient Agents</a> , a novel <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=agent%20framework">agent framework</a>
that has an optimal complexity to task requirements. <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Efficient%20Agents">Efficient Agents</a> retains
96.7% of the performance of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=OWL">OWL</a>, one leading open-source <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=agent%20framework">agent framework</a>, while
reducing operational costs from 0.398 to 0.228, resulting in a 28.4%
improvement in <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=cost-of-pass">cost-of-pass</a>. Our work provides actionable insights for
designing efficient, high-performing <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=agent%20systems">agent systems</a>, advancing the accessibility
and sustainability of AI-driven solutions.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="WEEKLY" data-date="2025-07-29" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2507.22025" target="_blank" onclick="event.stopPropagation()">UI-AGILE: Advancing GUI Agents with Effective Reinforcement Learning and
  Precise Inference-Time Grounding</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-07-29</div>
                                        <div class="tags"><span class="tag">WEEKLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2507.22025" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <div class="link-item" style="background: #f5f5f5; color: #999;">No GitHub</div>
                                    <div class="link-item upvote-item">üî• 2</div>
                                    <div class="link-item stars-item">‚≠ê 0</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">UI-AGILE enhances GUI agents through improved training with a Continuous Reward function, Simple Thinking reward, and Cropping-based Resampling, and inference with Decomposed Grounding with Selection, achieving state-of-the-art performance on GUI benchmarks.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                The emergence of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Multimodal%20Large%20Language%20Models">Multimodal Large Language Models</a> (MLLMs) has driven
significant advances in Graphical User Interface (GUI) agent capabilities.
Nevertheless, existing GUI agent training and inference techniques still suffer
from a dilemma for reasoning designs, ineffective reward, and visual noise. To
address these issues, we introduce UI-AGILE, a comprehensive framework
enhancing <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=GUI%20agents">GUI agents</a> at both the training and inference stages. For training,
we propose a suite of improvements to the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Supervised%20Fine-Tuning">Supervised Fine-Tuning</a> (SFT) process:
1) a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Continuous%20Reward%20function">Continuous Reward function</a> to incentivize high-precision grounding; 2) a
"Simple Thinking" reward to balance planning with speed and grounding accuracy;
and 3) a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Cropping-based%20Resampling">Cropping-based Resampling</a> strategy to mitigate the sparse reward
problem and improve learning on complex tasks. For inference, we present
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Decomposed%20Grounding%20with%20Selection">Decomposed Grounding with Selection</a>, a novel method that dramatically improves
grounding accuracy on high-resolution displays by breaking the image into
smaller, manageable parts. Experiments show that UI-AGILE achieves the
state-of-the-art performance on two benchmarks <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=ScreenSpot-Pro">ScreenSpot-Pro</a> and
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=ScreenSpot-v2">ScreenSpot-v2</a>. For instance, using both our proposed training and inference
enhancement methods brings 23% grounding accuracy improvement over the best
baseline on <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=ScreenSpot-Pro">ScreenSpot-Pro</a>.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="TRENDING" data-date="2025-07-01" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2507.01006" target="_blank" onclick="event.stopPropagation()">GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable
  Reinforcement Learning</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-07-01</div>
                                        <div class="tags"><span class="tag">TRENDING</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2507.01006" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/THUDM/GLM-4.1V-Thinking" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 212</div>
                                    <div class="link-item stars-item">‚≠ê 1.07k</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">A vision-language model (VLM) named GLM-4.1V-Thinking, developed with a reasoning-centric training framework, achieves state-of-the-art performance across various tasks, including STEM problem solving, video understanding, and long document understanding, outperforming larger models on many benchmarks.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                We present GLM-4.1V-Thinking, a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=vision-language%20model">vision-language model</a> (<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=VLM">VLM</a>) designed to
advance general-purpose multimodal reasoning. In this report, we share our key
findings in the development of the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=reasoning-centric%20training%20framework">reasoning-centric training framework</a>. We
first develop a capable vision foundation model with significant potential
through <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=large-scale%20pre-training">large-scale pre-training</a>, which arguably sets the upper bound for the
final performance. <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Reinforcement%20Learning%20with%20Curriculum%20Sampling">Reinforcement Learning with Curriculum Sampling</a> (<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=RLCS">RLCS</a>) then
unlocks the full potential of the model, leading to comprehensive capability
enhancement across a diverse range of tasks, including <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=STEM%20problem%20solving">STEM problem solving</a>,
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=video%20understanding">video understanding</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=content%20recognition">content recognition</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=coding">coding</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=grounding">grounding</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=GUI-based%20agents">GUI-based agents</a>,
and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=long%20document%20understanding">long document understanding</a>, among others. To facilitate research in this
field, we open-source GLM-4.1V-9B-Thinking, which achieves state-of-the-art
performance among models of comparable size. In a comprehensive evaluation
across 28 <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=public%20benchmarks">public benchmarks</a>, our model outperforms <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Qwen2.5-VL-7B">Qwen2.5-VL-7B</a> on nearly all
tasks and achieves comparable or even superior performance on 18 benchmarks
relative to the significantly larger <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Qwen2.5-VL-72B">Qwen2.5-VL-72B</a>. Notably,
GLM-4.1V-9B-Thinking also demonstrates competitive or superior performance
compared to closed-source models such as <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=GPT-4o">GPT-4o</a> on challenging tasks including
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=long%20document%20understanding">long document understanding</a> and STEM reasoning, further underscoring its strong
capabilities. Code, models and more information are released at
https://github.com/THUDM/GLM-4.1V-Thinking.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="MONTHLY" data-date="2025-08-07" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2508.05635" target="_blank" onclick="event.stopPropagation()">Genie Envisioner: A Unified World Foundation Platform for Robotic
  Manipulation</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-08-07</div>
                                        <div class="tags"><span class="tag">MONTHLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2508.05635" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/AgibotTech/Genie-Envisioner" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 67</div>
                                    <div class="link-item stars-item">‚≠ê 59</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">Genie Envisioner integrates policy learning, evaluation, and simulation using a video diffusion model and neural simulator for instruction-driven robotic manipulation.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                We introduce Genie Envisioner (GE), a unified world foundation platform for
robotic manipulation that integrates policy learning, evaluation, and
simulation within a single video-generative framework. At its core, GE-Base is
a large-scale, instruction-conditioned <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=video%20diffusion%20model">video diffusion model</a> that captures the
spatial, temporal, and semantic dynamics of real-world robotic interactions in
a structured <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=latent%20space">latent space</a>. Built upon this foundation, GE-Act maps latent
representations to executable <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=action%20trajectories">action trajectories</a> through a lightweight,
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=flow-matching%20decoder">flow-matching decoder</a>, enabling precise and generalizable policy inference
across diverse embodiments with minimal supervision. To support scalable
evaluation and training, GE-Sim serves as an action-conditioned neural
simulator, producing <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=high-fidelity%20rollouts">high-fidelity rollouts</a> for closed-loop policy development.
The platform is further equipped with <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=EWMBench">EWMBench</a>, a standardized benchmark suite
measuring <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=visual%20fidelity">visual fidelity</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=physical%20consistency">physical consistency</a>, and instruction-action
alignment. Together, these components establish Genie Envisioner as a scalable
and practical foundation for instruction-driven, general-purpose embodied
intelligence. All code, models, and benchmarks will be released publicly.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="WEEKLY" data-date="2025-08-08" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2508.06494" target="_blank" onclick="event.stopPropagation()">LightSwitch: Multi-view Relighting with Material-guided Diffusion</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-08-08</div>
                                        <div class="tags"><span class="tag">WEEKLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2508.06494" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <div class="link-item" style="background: #f5f5f5; color: #999;">No GitHub</div>
                                    <div class="link-item upvote-item">üî• 1</div>
                                    <div class="link-item stars-item">‚≠ê 0</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">Lightswitch, a material-relighting diffusion framework, enhances 3D relighting by integrating multi-view and material cues, achieving superior quality and efficiency compared to existing methods.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Recent approaches for 3D relighting have shown promise in integrating 2D
image relighting generative priors to alter the appearance of a 3D
representation while preserving the underlying structure. Nevertheless,
generative priors used for 2D relighting that directly relight from an input
image do not take advantage of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=intrinsic%20properties">intrinsic properties</a> of the subject that can be
inferred or cannot consider <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=multi-view%20data">multi-view data</a> at scale, leading to subpar
relighting. In this paper, we propose Lightswitch, a novel finetuned
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=material-relighting%20diffusion%20framework">material-relighting diffusion framework</a> that efficiently relights an arbitrary
number of input images to a target lighting condition while incorporating cues
from inferred <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=intrinsic%20properties">intrinsic properties</a>. By using multi-view and material
information cues together with a scalable <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=denoising%20scheme">denoising scheme</a>, our method
consistently and efficiently relights dense <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=multi-view%20data">multi-view data</a> of objects with
diverse material compositions. We show that our <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=2D%20relighting%20prediction">2D relighting prediction</a>
quality exceeds previous state-of-the-art relighting priors that directly
relight from images. We further demonstrate that LightSwitch matches or
outperforms state-of-the-art <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=diffusion%20inverse%20rendering">diffusion inverse rendering</a> methods in relighting
synthetic and real objects in as little as 2 minutes.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="TRENDING" data-date="2025-08-04" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2508.02038" target="_blank" onclick="event.stopPropagation()">Marco-Voice Technical Report</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-08-04</div>
                                        <div class="tags"><span class="tag">TRENDING</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2508.02038" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/AIDC-AI/Marco-Voice" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 14</div>
                                    <div class="link-item stars-item">‚≠ê 70</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">A multifunctional speech synthesis system integrates voice cloning and emotion control using speaker-emotion disentanglement and rotational emotional embeddings, achieving high expressive and natural speech.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                This paper presents a multifunctional speech synthesis system that integrates
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=voice%20cloning">voice cloning</a> and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=emotion%20control">emotion control</a> speech synthesis within a unified framework.
The goal of this work is to address longstanding challenges in achieving highly
expressive, controllable, and natural speech generation that faithfully
preserves speaker identity across diverse linguistic and emotional contexts.
Our approach introduces an effective <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=speaker-emotion%20disentanglement">speaker-emotion disentanglement</a> mechanism
with <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=in-batch%20contrastive%20learning">in-batch contrastive learning</a>, enabling independent manipulation of
speaker identity and eemotional style, as well as rotational emotional
embedding integration method for smooth <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=emotion%20control">emotion control</a>. To support
comprehensive training and evaluation, we construct CSEMOTIONS, a high-quality
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=emotional%20speech%20dataset">emotional speech dataset</a> containing 10 hours of Mandarin speech from six
professional speakers across seven emotional categories. Extensive experiments
demonstrate that our system, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Marco-Voice">Marco-Voice</a>, achieves substantial improvements in
both objective and subjective metrics. Comprehensive evaluations and analysis
were conducted, results show that MarcoVoice delivers competitive performance
in terms of speech clarity and emotional richness, representing a substantial
advance in the field of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=expressive%20neural%20speech%20synthesis">expressive neural speech synthesis</a>.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="MONTHLY" data-date="2025-08-01" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2508.00819" target="_blank" onclick="event.stopPropagation()">Beyond Fixed: Variable-Length Denoising for Diffusion Large Language
  Models</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-08-01</div>
                                        <div class="tags"><span class="tag">MONTHLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2508.00819" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/Li-Jinsong/DAEDAL" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 61</div>
                                    <div class="link-item stars-item">‚≠ê 111</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">DAEDAL, a novel training-free denoising strategy, enables dynamic length adaptation in Diffusion Large Language Models, improving performance and computational efficiency.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Diffusion%20Large%20Language%20Models">Diffusion Large Language Models</a> (<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=DLLMs">DLLMs</a>) are emerging as a powerful
alternative to the dominant <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Autoregressive%20Large%20Language%20Models">Autoregressive Large Language Models</a>, offering
efficient parallel generation and capable global context modeling. However, the
practical application of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=DLLMs">DLLMs</a> is hindered by a critical architectural
constraint: the need for a statically predefined generation length. This static
length allocation leads to a problematic trade-off: insufficient lengths
cripple performance on complex tasks, while excessive lengths incur significant
computational overhead and sometimes result in performance degradation. While
the inference framework is rigid, we observe that the model itself possesses
internal signals that correlate with the optimal response length for a given
task. To bridge this gap, we leverage these latent signals and introduce
DAEDAL, a novel training-free <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=denoising%20strategy">denoising strategy</a> that enables Dynamic Adaptive
Length Expansion for <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Diffusion%20Large%20Language%20Models">Diffusion Large Language Models</a>. DAEDAL operates in two
phases: 1) Before the denoising process, DAEDAL starts from a short initial
length and iteratively expands it to a coarse task-appropriate length, guided
by a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=sequence%20completion%20metric">sequence completion metric</a>. 2) During the denoising process, DAEDAL
dynamically intervenes by pinpointing and expanding insufficient generation
regions through <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=mask%20token%20insertion">mask token insertion</a>, ensuring the final output is fully
developed. Extensive experiments on <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=DLLMs">DLLMs</a> demonstrate that DAEDAL achieves
performance comparable, and in some cases superior, to meticulously tuned
fixed-length baselines, while simultaneously enhancing computational efficiency
by achieving a higher <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=effective%20token%20ratio">effective token ratio</a>. By resolving the static length
constraint, DAEDAL unlocks new potential for <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=DLLMs">DLLMs</a>, bridging a critical gap
with their Autoregressive counterparts and paving the way for more efficient
and capable generation.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="TRENDING MONTHLY" data-date="2025-08-05" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2508.03320" target="_blank" onclick="event.stopPropagation()">Skywork UniPic: Unified Autoregressive Modeling for Visual Understanding
  and Generation</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-08-05</div>
                                        <div class="tags"><span class="tag">MONTHLY</span><span class="tag">TRENDING</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2508.03320" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/SkyworkAI/UniPic" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 58</div>
                                    <div class="link-item stars-item">‚≠ê 578</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">Skywork UniPic, a 1.5 billion-parameter autoregressive model, unifies image understanding, text-to-image generation, and image editing with state-of-the-art performance on commodity hardware.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                We introduce Skywork UniPic, a 1.5 billion-parameter <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=autoregressive%20model">autoregressive model</a>
that unifies <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=image%20understanding">image understanding</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=text-to-image%20generation">text-to-image generation</a>, and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=image%20editing">image editing</a>
within a single architecture-eliminating the need for task-specific adapters or
inter-module connectors-and demonstrate that compact multimodal systems can
achieve state-of-the-art performance on commodity hardware. Skywork UniPic
achieves a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=GenEval">GenEval</a> score of 0.86, surpassing most existing unified models; sets
a new <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=DPG-Bench">DPG-Bench</a> complex-generation record of 85.5; attains 5.83 on
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=GEditBench-EN">GEditBench-EN</a> and 3.49 on <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=ImgEdit-Bench">ImgEdit-Bench</a> for <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=image%20editing">image editing</a>; and generates 1024 x
1024 images with under 15 GB of GPU memory (e.g., RTX 4090). (1) a decoupled
encoding strategy that leverages a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=masked%20autoregressive%20encoder">masked autoregressive encoder</a> for synthesis
and a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=SigLIP2%20encoder">SigLIP2 encoder</a> for understanding, all feeding a shared autoregressive
decoder; (2) a progressive, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=resolution-aware%20training">resolution-aware training</a> schedule scaling from 256
x 256 to 1024 x 1024 while dynamically unfreezing parameters to balance
capacity and stability; and (3) meticulously curated, 100 million-scale
datasets augmented with task-specific reward models to refine generation and
editing objectives. By demonstrating that <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=high-fidelity%20multimodal%20integration">high-fidelity multimodal integration</a>
need not incur prohibitive resource demands, Skywork UniPic establishes a
practical paradigm for deployable, high-fidelity multimodal AI. Code and
weights are publicly available at
https://huggingface.co/Skywork/Skywork-UniPic-1.5B.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="MONTHLY" data-date="2025-08-07" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2508.05405" target="_blank" onclick="event.stopPropagation()">DeepPHY: Benchmarking Agentic VLMs on Physical Reasoning</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-08-07</div>
                                        <div class="tags"><span class="tag">MONTHLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2508.05405" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/XinrunXu/DeepPHY" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 60</div>
                                    <div class="link-item stars-item">‚≠ê 36</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">DeepPHY evaluates Vision Language Models' physical reasoning and control through simulated environments with varying difficulty levels.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Although <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Vision%20Language%20Models">Vision Language Models</a> (VLMs) exhibit strong perceptual abilities
and impressive visual reasoning, they struggle with attention to detail and
precise action planning in complex, dynamic environments, leading to subpar
performance. Real-world tasks typically require complex interactions, advanced
spatial reasoning, long-term planning, and continuous strategy refinement,
usually necessitating understanding the physics rules of the target scenario.
However, evaluating these capabilities in real-world scenarios is often
prohibitively expensive. To bridge this gap, we introduce <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=DeepPHY">DeepPHY</a>, a novel
benchmark framework designed to systematically evaluate VLMs' understanding and
reasoning about fundamental physical principles through a series of challenging
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=simulated%20environments">simulated environments</a>. <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=DeepPHY">DeepPHY</a> integrates multiple <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=physical%20reasoning">physical reasoning</a>
environments of varying difficulty levels and incorporates fine-grained
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=evaluation%20metrics">evaluation metrics</a>. Our evaluation finds that even state-of-the-art VLMs
struggle to translate descriptive physical knowledge into precise, predictive
control.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="TRENDING" data-date="2025-07-17" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2507.13334" target="_blank" onclick="event.stopPropagation()">A Survey of Context Engineering for Large Language Models</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-07-17</div>
                                        <div class="tags"><span class="tag">TRENDING</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2507.13334" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/Meirtz/Awesome-Context-Engineering" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 236</div>
                                    <div class="link-item stars-item">‚≠ê 1.76k</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">Context Engineering systematically optimizes information payloads for Large Language Models, addressing gaps in generating sophisticated, long-form outputs.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                The performance of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Large%20Language%20Models">Large Language Models</a> (LLMs) is fundamentally determined
by the contextual information provided during inference. This survey introduces
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Context%20Engineering">Context Engineering</a>, a formal discipline that transcends simple prompt design
to encompass the systematic optimization of information payloads for LLMs. We
present a comprehensive taxonomy decomposing <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Context%20Engineering">Context Engineering</a> into its
foundational components and the sophisticated implementations that integrate
them into intelligent systems. We first examine the foundational components:
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=context%20retrieval">context retrieval</a> and generation, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=context%20processing">context processing</a> and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=context%20management">context management</a>. We
then explore how these components are architecturally integrated to create
sophisticated system implementations: <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=retrieval-augmented%20generation">retrieval-augmented generation</a> (RAG),
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=memory%20systems">memory systems</a> and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=tool-integrated%20reasoning">tool-integrated reasoning</a>, and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=multi-agent%20systems">multi-agent systems</a>. Through
this systematic analysis of over 1300 research papers, our survey not only
establishes a technical roadmap for the field but also reveals a critical
research gap: a fundamental asymmetry exists between model capabilities. While
current models, augmented by advanced <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=context%20engineering">context engineering</a>, demonstrate
remarkable proficiency in understanding complex contexts, they exhibit
pronounced limitations in generating equally sophisticated, long-form outputs.
Addressing this gap is a defining priority for future research. Ultimately,
this survey provides a unified framework for both researchers and engineers
advancing context-aware AI.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="MONTHLY" data-date="2025-08-03" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2508.01959" target="_blank" onclick="event.stopPropagation()">SitEmb-v1.5: Improved Context-Aware Dense Retrieval for Semantic
  Association and Long Story Comprehension</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-08-03</div>
                                        <div class="tags"><span class="tag">MONTHLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2508.01959" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <div class="link-item" style="background: #f5f5f5; color: #999;">No GitHub</div>
                                    <div class="link-item upvote-item">üî• 52</div>
                                    <div class="link-item stars-item">‚≠ê 0</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">A new training paradigm and situated embedding models (SitEmb) enhance retrieval performance by conditioning short text chunks on broader context windows, outperforming state-of-the-art models with fewer parameters.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Retrieval-augmented%20generation%20(RAG)">Retrieval-augmented generation (RAG)</a> over long documents typically involves
splitting the text into smaller chunks, which serve as the basic units for
retrieval. However, due to dependencies across the original document,
contextual information is often essential for accurately interpreting each
chunk. To address this, prior work has explored encoding longer <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=context%20window">context window</a>s
to produce embeddings for longer chunks. Despite these efforts, gains in
retrieval and downstream tasks remain limited. This is because (1) longer
chunks strain the capacity of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=embedding%20models">embedding models</a> due to the increased amount of
information they must encode, and (2) many real-world applications still
require returning localized evidence due to constraints on model or human
bandwidth.
  We propose an alternative approach to this challenge by representing short
chunks in a way that is conditioned on a broader <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=context%20window">context window</a> to enhance
retrieval performance -- i.e., situating a chunk's meaning within its context.
We further show that existing <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=embedding%20models">embedding models</a> are not well-equipped to encode
such <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=situated%20context">situated context</a> effectively, and thus introduce a new training paradigm
and develop the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=situated%20embedding%20models%20(SitEmb)">situated embedding models (SitEmb)</a>. To evaluate our method, we
curate a book-plot retrieval dataset specifically designed to assess situated
retrieval capabilities. On this benchmark, our SitEmb-v1 model based on <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=BGE-M3">BGE-M3</a>
substantially outperforms state-of-the-art <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=embedding%20models">embedding models</a>, including several
with up to 7-8B parameters, with only 1B parameters. Our 8B SitEmb-v1.5 model
further improves performance by over 10% and shows strong results across
different languages and several <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=downstream%20applications">downstream applications</a>.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="TRENDING" data-date="2025-05-14" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2505.09388" target="_blank" onclick="event.stopPropagation()">Qwen3 Technical Report</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-05-14</div>
                                        <div class="tags"><span class="tag">TRENDING</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2505.09388" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/QwenLM/Qwen3" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 274</div>
                                    <div class="link-item stars-item">‚≠ê 24k</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">Qwen3, a unified series of large language models, integrates thinking and non-thinking modes, reduces computational resources, and achieves state-of-the-art performance across various tasks and languages.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                In this work, we present Qwen3, the latest version of the Qwen model family.
Qwen3 comprises a series of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=large%20language%20models">large language models</a> (LLMs) designed to advance
performance, efficiency, and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=multilingual%20capabilities">multilingual capabilities</a>. The Qwen3 series
includes models of both dense and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Mixture-of-Expert">Mixture-of-Expert</a> (MoE) architectures, with
parameter scales ranging from 0.6 to 235 billion. A key innovation in Qwen3 is
the integration of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=thinking%20mode">thinking mode</a> (for complex, multi-step reasoning) and
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=non-thinking%20mode">non-thinking mode</a> (for rapid, context-driven responses) into a unified
framework. This eliminates the need to switch between different models--such as
chat-optimized models (e.g., GPT-4o) and dedicated reasoning models (e.g.,
QwQ-32B)--and enables dynamic mode switching based on user queries or chat
templates. Meanwhile, Qwen3 introduces a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=thinking%20budget%20mechanism">thinking budget mechanism</a>, allowing
users to allocate computational resources adaptively during inference, thereby
balancing latency and performance based on task complexity. Moreover, by
leveraging the knowledge from the flagship models, we significantly reduce the
computational resources required to build smaller-scale models, while ensuring
their highly competitive performance. Empirical evaluations demonstrate that
Qwen3 achieves state-of-the-art results across diverse benchmarks, including
tasks in <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=code%20generation">code generation</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=mathematical%20reasoning">mathematical reasoning</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=agent%20tasks">agent tasks</a>, etc.,
competitive against larger MoE models and proprietary models. Compared to its
predecessor Qwen2.5, Qwen3 expands multilingual support from 29 to 119
languages and dialects, enhancing global accessibility through improved
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=cross-lingual%20understanding">cross-lingual understanding</a> and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=generation%20capabilities">generation capabilities</a>. To facilitate
reproducibility and community-driven research and development, all Qwen3 models
are publicly accessible under Apache 2.0.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="TRENDING" data-date="2025-07-03" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2507.02592" target="_blank" onclick="event.stopPropagation()">WebSailor: Navigating Super-human Reasoning for Web Agent</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-07-03</div>
                                        <div class="tags"><span class="tag">TRENDING</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2507.02592" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/Alibaba-NLP/WebAgent/" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 108</div>
                                    <div class="link-item stars-item">‚≠ê 5.94k</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">WebSailor, a post-training methodology, enhances open-source LLMs with sophisticated reasoning to match proprietary systems in complex information-seeking tasks.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Transcending human cognitive limitations represents a critical frontier in
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=LLM">LLM</a> training. Proprietary agentic systems like <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=DeepResearch">DeepResearch</a> have demonstrated
superhuman capabilities on extremely complex information-seeking benchmarks
such as <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=BrowseComp">BrowseComp</a>, a feat previously unattainable. We posit that their success
hinges on a sophisticated <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=reasoning%20pattern">reasoning pattern</a> absent in open-source models: the
ability to systematically reduce extreme uncertainty when navigating vast
information landscapes. Based on this insight, we introduce WebSailor, a
complete post-training methodology designed to instill this crucial capability.
Our approach involves generating novel, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=high-uncertainty%20tasks">high-uncertainty tasks</a> through
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=structured%20sampling">structured sampling</a> and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=information%20obfuscation">information obfuscation</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=RFT%20cold%20start">RFT cold start</a>, and an
efficient <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=agentic%20RL">agentic RL</a> training algorithm, Duplicating Sampling Policy
Optimization (<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=DUPO">DUPO</a>). With this integrated pipeline, WebSailor significantly
outperforms all opensource agents in complex information-seeking tasks,
matching proprietary agents' performance and closing the capability gap.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="TRENDING" data-date="2025-07-20" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2507.15061" target="_blank" onclick="event.stopPropagation()">WebShaper: Agentically Data Synthesizing via Information-Seeking
  Formalization</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-07-20</div>
                                        <div class="tags"><span class="tag">TRENDING</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2507.15061" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/Alibaba-NLP/WebWalker" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 48</div>
                                    <div class="link-item stars-item">‚≠ê 5.94k</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">WebShaper, a formalization-driven framework, synthesizes information-seeking datasets using set theory and Knowledge Projections to enhance reasoning structure and achieve top performance in open-sourced benchmarks.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                The advent of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Large%20Language%20Model%20(LLM)">Large Language Model (LLM)</a>-powered agents has revolutionized
artificial intelligence by enabling solutions to complex, open-ended tasks
through web-based information-seeking (IS) capabilities. The scarcity of
high-quality training data has limited the development of IS agents. Existing
approaches typically adopt an information-driven paradigm that first collects
web data and then generates questions based on the retrieval. However, this may
lead to inconsistency between information structure and reasoning structure,
question and answer. To mitigate, we propose a formalization-driven IS data
synthesis framework WebShaper to construct a dataset. WebShaper systematically
formalizes IS tasks through <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=set%20theory">set theory</a>. Central to the formalization is the
concept of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Knowledge%20Projections%20(KP)">Knowledge Projections (KP)</a>, which enables precise control over
reasoning structure by KP operation compositions. During synthesis, we begin by
creating seed tasks, then use a multi-step expansion process. At each step, an
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=agentic%20Expander">agentic Expander</a> expands the current formal question more complex with
retrieval and validation tools based on our formalization. We train our model
on the synthesized dataset. Experiment results demonstrate that WebShaper
achieves state-of-the-art performance among open-sourced IS agents on <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=GAIA">GAIA</a> and
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=WebWalkerQA%20benchmarks">WebWalkerQA benchmarks</a>.
                            </div>
                        </div>
                    </div>
                </div>
                </div>
                        </div>
                    </section>
                </div>
            </div>
        </main>

        <!-- Footer -->
        <footer class="footer">
            <div class="container">
                <p>&copy; 2025 Daily Digest. Curated with ‚ù§Ô∏è for the curious minds.</p>
            </div>
        </footer>
        <script src="static/script.js"></script>
    </body>
    </html>
    