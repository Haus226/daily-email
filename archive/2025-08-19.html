
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <title>Daily Digest</title>
        <link rel="stylesheet" href="static/style.css">
    </head>
    <body>
        <!-- Header -->
        <header class="header">
            <div class="container">
                <nav class="nav">
                    <div class="logo">üì∞ Daily Digest</div>
                    <ul class="nav-links">
                        <li><a href="#astronomy">Astronomy</a></li>
                        <li><a href="#earth">Earth</a></li>
                        <li><a href="#tarot">Tarot</a></li>
                        <li><a href="#tech">Tech News</a></li>
                        <li><a href="#papers">Papers</a></li>
                    </ul>
                    <div class="date-badge">Tue, Aug 19, 2025</div>
                </nav>
            </div>
        </header>

        <!-- Main Content -->
        <main class="main-content">
            <div class="container">
                <!-- Hero Section -->
                <section class="hero">
                    <h1>Your Daily Tech & Science Digest</h1>
                    <p>Stay updated with the latest in astronomy, earth sciences, technology news, and cutting-edge research papers</p>
                </section>

                <!-- Content Grid -->
                <div class="content-grid">
                    <!-- APOD, Earth Observatory, and Tarot Row -->
                    <div class="three-column">
                        <!-- APOD Section -->
                        <section id="astronomy" class="card apod-card">
                            <div class="card-media" id="apod-media"></div>
                            <div class="card-content-wrapper">
                                <div class="card-header">
                                    <div class="card-icon">‚ú®</div>
                                    <div>
                                        <h2 class="card-title">Astronomy Picture</h2>
                                        <p class="card-subtitle">NASA's daily cosmic wonder</p>
                                    </div>
                                </div>
                                <div class="card-content" id="apod-content">
                                    
        <div style='display: flex; flex-direction: column; gap: 20px;'>
            <div style='display: none;'>
                <img src="https://apod.nasa.gov/apod/image/2508/Spiral1309_HubbleGalbany_960.jpg" alt="APOD" class="apod-media-element">
            </div>
            <div>
                <h3>NGC 1309: A Useful Spiral Galaxy</h3>
                <p><b> Explanation: </b> 
This galaxy is not only pretty -- it's useful. 

A gorgeous spiral some 100 million light-years distant,
<a href="http://en.wikipedia.org/wiki/NGC_1309">NGC 1309</a> lies on the
banks of the constellation of the River
(<a href="http://www.hawastsoc.org/deepsky/eri/index.html">Eridanus</a>).

NGC 1309 spans about 30,000 
<a href="https://spaceplace.nasa.gov/light-year/">light-year</a>s, making it about one third the size of our larger 
<a href="https://science.nasa.gov/resource/the-milky-way-galaxy/">Milky Way galaxy</a>.

Bluish clusters of <a href="https://apod.nasa.gov/apod/ap140925.html">young stars</a> and
dust lanes are seen to trace out NGC 1309's spiral arms as they 
<a href="https://pbs.twimg.com/media/GhF-nwZbkAA2UhU.jpg">wind around</a> an older yellowish star population at its core.

Not just another <a href="https://apod.nasa.gov/apod/ap080824.html">pretty face-on</a> 
spiral galaxy, observations of NGC 1309's two recent supernovas and multiple  
<a href="https://apod.nasa.gov/apod/ap960110.html">Cepheid variable stars</a> contribute to the
<a href="https://lco.global/spacebook/distance/cepheid-variable-stars-supernovae-and-distance-measurement/">calibration</a> of
<a href="https://science.nasa.gov/mission/hubble/science/science-highlights/discovering-a-runaway-universe/">the expansion</a> of the Universe.

Still, after you get over 
this beautiful galaxy's grand design,
<a href="https://esahubble.org/images/potw2530a/zoomable/">check out</a> the array of more distant background 
galaxies also recorded in 
<a href="https://esahubble.org/images/potw2530a/">this sharp image</a> from the  
<a href="https://science.nasa.gov/mission/hubble/">Hubble Space Telescope</a>.


<br><b> Tomorrow's picture: </b>open space</p>
            </div>
        </div>
        
                                </div>
                            </div>
                        </section>

                        <!-- Earth Observatory -->
                        <section id="earth" class="card eo-card">
                            <div class="card-media" id="eo-media"></div>
                            <div class="card-content-wrapper">
                                <div class="card-header">
                                    <div class="card-icon">üåç</div>
                                    <div>
                                        <h2 class="card-title">Earth Observatory</h2>
                                        <p class="card-subtitle">Our planet from above</p>
                                    </div>
                                </div>
                                <div class="card-content" id="eo-content">
                                    
        <div style='display: flex; flex-direction: column; gap: 20px;'>
            <div style='display: none;'>
                <img src='https://eoimages.gsfc.nasa.gov/images/imagerecords/154000/154686/potomaclangley_oli2_20240801_th.jpg' alt='Earth Observatory' class='eo-media-element'>
            </div>
            <div>
                <h3>The Potomac Island Where History Took Flight</h3>
                <p>Samuel Pierpont Langley conducted the first successful flight of an unpiloted, engine-driven, heavier-than-air craft from a houseboat near Chopawamsic Island in the Potomac River. <a href='https://earthobservatory.nasa.gov/images/154686/the-potomac-island-where-history-took-flight' target='_blank'>[Read more]</a></p>
                <div style='background: #f0f8f0; padding: 15px; border-radius: 8px; border-left: 4px solid #2d5016; margin-top: 20px;'>
                    <p>
                        <strong>ü§ñ AI Summary:</strong> On May 6, 1896, Samuel Pierpont Langley, Secretary of the Smithsonian Institution, successfully flew Aerodrome No. 5, an unpiloted, engine-driven, heavier-than-air craft, from a houseboat on the Potomac River near Chopawamsic Island, Virginia. Launched via a spring-loaded catapult at 3:05 p.m., the 25-pound aircraft ascended using southerly headwinds to 70‚Äì100 feet, maintaining remarkable steadiness for over 90 seconds during a spiraling flight covering more than 3,000 horizontal feet. The flight concluded when the steam engine exhausted its fuel, landing the craft undamaged in the river. Witness Alexander Graham Bell described it as gliding "like a great mechanical bird," confirming the practicality of mechanical flight. This marked the world's first successful flight of its kind.
                    </p>
                </div>
            </div>
        </div>
        
                                </div>
                            </div>
                        </section>

                        <!-- Tarot Section -->
                        <section id="tarot" class="card tarot-card">
                            <div class="card-media" id="tarot-media"></div>
                            <div class="card-content-wrapper">
                                <div class="card-header">
                                    <div class="card-icon">üîÆ</div>
                                    <div>
                                        <h2 class="card-title">Daily Tarot</h2>
                                        <p class="card-subtitle">Your mystical guidance</p>
                                    </div>
                                </div>
                                <div class="card-content" id="tarot-content">
                                    
        <div style='display: flex; flex-direction: column; gap: 20px;'>
            <div style='display: none;'>
                <div class='tarot-card-container tarot-media-element'>
                    <div class='tarot-card' onclick='this.style.transform = this.style.transform.includes("rotateY(180deg)") ? "rotateY(0deg)" : "rotateY(180deg)"'>
                        <div>
                            <div style='font-size: 2.5rem; color: #d4af37; text-align: center; line-height: 1.2;'>
                                üîÆ<br>
                                <span style='font-size: 0.8rem; letter-spacing: 2px; font-weight: normal;'>DAILY TAROT</span><br>
                                <span style='font-size: 0.6rem; opacity: 0.8;'>Click to Reveal</span>
                            </div>
                        </div>
                        <div>
                            <img src='https://raw.githubusercontent.com/Haus226/daily-email/refs/heads/main/tarot_cards/Pictorial_Key_to_the_Tarot_Cups_14.jpg' alt='King of Cups' />
                        </div>
                    </div>
                </div>
            </div>
            <div>
                <h3>King of Cups</h3>                
                <div style='background: linear-gradient(135deg, #ffeaa7, #fdcb6e); padding: 5px; border-radius: 12px; border-left: 4px solid #e17055; margin-bottom: 20px; box-shadow: 0 4px 15px rgba(0,0,0,0.1);'>
                    <div style='font-weight: bold; color: #2d3436; margin-bottom: 5px; font-size: 1.2rem; text-transform: uppercase; letter-spacing: 1px;'>‚ú® Core Meaning</div>
                    <p style='margin: 0; color: #2d3436; line-height: 1.5; font-size: 1.0rem;'>Fair man, man of business, law, or divinity; responsible, disposed to oblige the Querent; also equity, art and science, including those who profess science, law and art; creative intelligence.</p>
                </div>
                <div style='background: linear-gradient(135deg, #ddd6fe, #c4b5fd); padding: 5px; border-radius: 12px; border-left: 4px solid #8b5cf6; box-shadow: 0 4px 15px rgba(0,0,0,0.1);'>
                    <div style='font-weight: bold; color: #2d3436; margin-bottom: 5px; font-size: 1.4rem; text-transform: uppercase; letter-spacing: 1px;'>üîç Daily Guidance</div>
                    <p style='margin: 0; color: #2d3436; line-height: 1.6; font-size: 1.3rem; font-weight: 500;'>"The King of Cups whispers wisdom today: Your emotional intelligence is your greatest asset. Trust in your heart's guidance, and let it steer you through life's transformations."</p>
                </div>
    
            </div>
        </div>
        
                                </div>
                            </div>
                        </section>
                        
                        <!-- Navigation Arrows -->
                        <button class="card-nav prev" onclick="prevCard()" aria-label="Previous card">‚Äπ</button>
                        <button class="card-nav next" onclick="nextCard()" aria-label="Next card">‚Ä∫</button>
                    </div>

                    <!-- Hacker News Section -->
                    <section id="tech" class="card hn-card featured-section">
                        <div class="card-header">
                            <div class="card-icon">üî•</div>
                            <div>
                                <h2 class="card-title">Hacker News Top 10</h2>
                                <p class="card-subtitle">What's trending in tech</p>
                            </div>
                        </div>
                        <div class="card-content">
                            <ol class='hn-list'><li><a href='https://nadia.xyz/shameless' target='_blank'>Shamelessness as a strategy (2019)</a></li><li><a href='https://www.popsci.com/technology/tibetan-prayer-scroll-scans/' target='_blank'>X-ray scans reveal Buddhist prayers inside tiny Tibetan scrolls</a></li><li><a href='https://www.smithsonianmag.com/smart-news/lab-grown-salmon-hits-the-menu-at-an-oregon-restaurant-as-the-fda-greenlights-the-cell-cultured-product-180986769/' target='_blank'>Lab-Grown Salmon Hits the Menu at an Oregon Restaurant as the FDA Greenlights</a></li><li><a href='https://divernet.com/scuba-news/freediving/how-croatian-freediver-held-breath-for-29-minutes/' target='_blank'>Croatian freediver held breath for 29 minutes</a></li><li><a href='https://help.obsidian.md/bases' target='_blank'>Obsidian Bases</a></li><li><a href='https://www.binarly.io/blog/persistent-risk-xz-utils-backdoor-still-lurking-in-docker-images' target='_blank'>XZ Utils Backdoor Still Lurking in Docker Images</a></li><li><a href='https://jslegenddev.substack.com/p/how-to-start-making-games-in-javascript' target='_blank'>Starting game development in JavaScript with no experience</a></li><li><a href='https://github.com/tiny-tpu-v2/tiny-tpu' target='_blank'>A minimal tensor processing unit (TPU), inspired by Google's TPU</a></li><li><a href='https://github.com/epicenter-so/epicenter/tree/main/apps/whispering' target='_blank'>Show HN: Whispering ‚Äì Open-source, local-first dictation you can trust</a></li><li><a href='https://graic.net/p/left-to-right-programming' target='_blank'>Left to Right Programming</a></li></ol>
                        </div>
                    </section>

                    <!-- Hugging Face Papers -->
                    <section id="papers" class="card hf-card-container featured-section">
                        <div class="card-header">
                            <div class="card-icon">üìö</div>
                            <div>
                                <h2 class="card-title">Latest Research Papers</h2>
                                <p class="card-subtitle">Cutting-edge AI & ML research</p>
                            </div>
                        </div>
                        
                        <!-- Paper Filters -->
                        <div class="paper-filters" id="hf-filters">
                            <button class="filter-btn active" data-tag="ALL" onclick="filterPapers('ALL')">All Papers</button>
                            <button class="filter-btn" data-tag="DAILY" onclick="filterPapers('DAILY')">Daily</button>
                            <button class="filter-btn" data-tag="WEEKLY" onclick="filterPapers('WEEKLY')">Weekly</button>
                            <button class="filter-btn" data-tag="MONTHLY" onclick="filterPapers('MONTHLY')">Monthly</button>
                            <button class="filter-btn" data-tag="TRENDING" onclick="filterPapers('TRENDING')">Trending</button>
                        </div>

                        <!-- Papers Content -->
                        <div class="card-content">
                            
        <div id="hf-grid" class="papers-grid">
        
                <div class="paper-card hf-card flip-card" data-tags="WEEKLY DAILY" data-date="2025-08-14" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2508.10874" target="_blank" onclick="event.stopPropagation()">SSRL: Self-Search Reinforcement Learning</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-08-14</div>
                                        <div class="tags"><span class="tag">DAILY</span><span class="tag">WEEKLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2508.10874" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/TsinghuaC3I/SSRL" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 66</div>
                                    <div class="link-item stars-item">‚≠ê 41</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">LLMs can serve as efficient simulators for RL tasks by leveraging internal knowledge, reducing reliance on external search engines and improving sim-to-real transfer.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                We investigate the potential of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=large%20language%20models">large language models</a> (<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=LLMs">LLMs</a>) to serve as
efficient simulators for agentic search tasks in <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=reinforcement%20learning">reinforcement learning</a> (<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=RL">RL</a>),
thereby reducing dependence on costly interactions with external search
engines. To this end, we first quantify the intrinsic search capability of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=LLMs">LLMs</a>
via structured prompting and repeated sampling, which we term <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Self-Search">Self-Search</a>. Our
results reveal that <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=LLMs">LLMs</a> exhibit strong scaling behavior with respect to the
inference budget, achieving high <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=pass%40k">pass@k</a> on question-answering benchmarks,
including the challenging <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=BrowseComp">BrowseComp</a> task. Building on these observations, we
introduce <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Self-Search%20RL">Self-Search RL</a> (<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=SSRL">SSRL</a>), which enhances <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=LLMs">LLMs</a>' <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Self-Search">Self-Search</a> capability
through format-based and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=rule-based%20rewards">rule-based rewards</a>. <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=SSRL">SSRL</a> enables models to iteratively
refine their knowledge utilization internally, without requiring access to
external tools. Empirical evaluations demonstrate that <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=SSRL">SSRL</a>-trained policy
models provide a cost-effective and stable environment for search-driven <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=RL">RL</a>
training, reducing reliance on external search engines and facilitating robust
sim-to-real transfer. We draw the following conclusions: 1) <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=LLMs">LLMs</a> possess wo<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=rl">rl</a>d
knowledge that can be effectively elicited to achieve high performance; 2) <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=SSRL">SSRL</a>
demonstrates the potential of leveraging internal knowledge to reduce
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=hallucination">hallucination</a>; 3) <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=SSRL">SSRL</a>-trained models integrate seamlessly with external search
engines without additional effort. Our findings highlight the potential of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=LLMs">LLMs</a>
to support more scalable <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=RL">RL</a> agent training.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="DAILY WEEKLY TRENDING" data-date="2025-08-15" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2508.11630" target="_blank" onclick="event.stopPropagation()">Thyme: Think Beyond Images</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-08-15</div>
                                        <div class="tags"><span class="tag">DAILY</span><span class="tag">TRENDING</span><span class="tag">WEEKLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2508.11630" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/yfzhang114/Thyme" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 58</div>
                                    <div class="link-item stars-item">‚≠ê 141</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">Thyme, a novel paradigm, enables MLLMs to autonomously perform image manipulations and computations, enhancing performance in perception and reasoning tasks through a two-stage training strategy and GRPO-ATS algorithm.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Following OpenAI's introduction of the ``thinking with images'' concept,
recent efforts have explored stimulating the use of visual information in the
reasoning process to enhance model performance in perception and reasoning
tasks. However, to the best of our knowledge, no open-source work currently
offers a feature set as rich as proprietary models (O3), which can perform
diverse image manipulations and simultaneously enhance logical reasoning
capabilities through code. In this paper, we make a preliminary attempt in this
direction by introducing Thyme (Think Beyond Images), a novel paradigm for
enabling <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=MLLMs">MLLMs</a> to transcend existing ``<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=think%20with%20images">think with images</a>'' approaches by
autonomously generating and executing diverse <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=image%20processing">image processing</a> and
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=computational%20operations">computational operations</a> via <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=executable%20code">executable code</a>. This approach not only
facilitates a rich, on-the-fly set of image manipulations (e.g., cropping,
rotation, contrast enhancement) but also allows for mathematical computations,
all while maintaining high autonomy in deciding when and how to apply these
operations. We activate this capability through a two-stage training strategy:
an initial <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=SFT">SFT</a> on a curated dataset of 500K samples to teach code generation,
followed by a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=RL">RL</a> phase to refine decision-making. For the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=RL">RL</a> stage, we manually
collect and design high-resolution question-answer pairs to increase the
learning difficulty, and we propose <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=GRPO-ATS">GRPO-ATS</a> (Group Relative Policy
Optimization with <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Adaptive%20Temperature%20Sampling">Adaptive Temperature Sampling</a>), an algorithm that applies
distinct temperatures to text and code generation to balance reasoning
exploration with code execution precision. We conduct extensive experimental
analysis and ablation studies. Comprehensive evaluations on nea<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=rl">rl</a>y 20
benchmarks show that Thyme yields significant and consistent performance gains,
particula<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=rl">rl</a>y in challenging high-resolution perception and complex reasoning
tasks.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="WEEKLY DAILY TRENDING" data-date="2025-08-13" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2508.10104" target="_blank" onclick="event.stopPropagation()">DINOv3</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-08-13</div>
                                        <div class="tags"><span class="tag">DAILY</span><span class="tag">TRENDING</span><span class="tag">WEEKLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2508.10104" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/facebookresearch/dinov3" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 57</div>
                                    <div class="link-item stars-item">‚≠ê 4.73k</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">DINOv3, a self-supervised learning model, achieves superior performance across various vision tasks by scaling datasets and models, addressing dense feature degradation, and enhancing flexibility with post-hoc strategies.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Self-supervised%20learning">Self-supervised learning</a> holds the promise of eliminating the need for manual
data annotation, enabling models to scale effortlessly to massive datasets and
larger architectures. By not being tailored to specific tasks or domains, this
training paradigm has the potential to learn visual representations from
diverse sources, ranging from natural to aerial images -- using a single
algorithm. This technical report introduces <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=DINOv3">DINOv3</a>, a major milestone toward
realizing this vision by leveraging simple yet effective strategies. First, we
leverage the benefit of scaling both dataset and model size by careful data
preparation, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=design">design</a>, and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=optimization">optimization</a>. Second, we introduce a new method called
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Gram%20anchoring">Gram anchoring</a>, which effectively addresses the known yet unsolved issue of
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=dense%20feature%20maps">dense feature maps</a> degrading during long training schedules. Finally, we apply
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=post-hoc%20strategies">post-hoc strategies</a> that further enhance our models' flexibility with respect
to resolution, model size, and alignment with text. As a result, we present a
versatile <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=vision%20foundation%20model">vision foundation model</a> that outperforms the specialized state of the
art across a broad range of settings, without fine-tuning. <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=DINOv3">DINOv3</a> produces
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=high-quality%20dense%20features">high-quality dense features</a> that achieve outstanding performance on various
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=vision%20tasks">vision tasks</a>, significantly surpassing previous self- and weakly-supervised
foundation models. We also share the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=DINOv3">DINOv3</a> suite of vision models, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=design">design</a>ed to
advance the state of the art on a wide spectrum of tasks and data by providing
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=scalable%20solutions">scalable solutions</a> for diverse resource constraints and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=deployment%20scenarios">deployment scenarios</a>.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="MONTHLY" data-date="2025-08-02" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2508.01191" target="_blank" onclick="event.stopPropagation()">Is Chain-of-Thought Reasoning of LLMs a Mirage? A Data Distribution Lens</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-08-02</div>
                                        <div class="tags"><span class="tag">MONTHLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2508.01191" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/ChengshuaiZhao0/DataAlchemy" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 213</div>
                                    <div class="link-item stars-item">‚≠ê 149</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">CoT reasoning in LLMs is found to be limited by the distribution discrepancy between training and test data, suggesting it is not a robust form of reasoning.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Chain-of-Thought">Chain-of-Thought</a> (CoT) prompting has been shown to improve Large Language
Model (LLM) performance on various tasks. With this approach, LLMs appear to
produce human-like reasoning steps before providing answers (a.k.a., CoT
reasoning), which often leads to the perception that they engage in deliberate
inferential processes. However, some initial findings suggest that CoT
reasoning may be more superficial than it appears, motivating us to explore
further. In this paper, we study <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=CoT%20reasoning">CoT reasoning</a> via a data distribution lens and
investigate if <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=CoT%20reasoning">CoT reasoning</a> reflects a structured <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=inductive%20bias">inductive bias</a> learned from
in-distribution data, allowing the model to conditionally generate reasoning
paths that approximate those seen during training. Thus, its effectiveness is
fundamentally bounded by the degree of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=distribution%20discrepancy">distribution discrepancy</a> between the
training data and the test queries. With this lens, we dissect <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=CoT%20reasoning">CoT reasoning</a>
via three dimensions: task, length, and format. To investigate each dimension,
we design <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=DataAlchemy">DataAlchemy</a>, an isolated and controlled environment to train LLMs
from scratch and systematically probe them under various distribution
conditions. Our results reveal that <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=CoT%20reasoning">CoT reasoning</a> is a brittle mirage that
vanishes when it is pushed beyond training distributions. This work offers a
deeper understanding of why and when <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=CoT%20reasoning">CoT reasoning</a> fails, emphasizing the
ongoing challenge of achieving genuine and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=generalizable%20reasoning">generalizable reasoning</a>.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="DAILY WEEKLY" data-date="2025-08-14" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2508.10975" target="_blank" onclick="event.stopPropagation()">BeyondWeb: Lessons from Scaling Synthetic Data for Trillion-scale
  Pretraining</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-08-14</div>
                                        <div class="tags"><span class="tag">DAILY</span><span class="tag">WEEKLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2508.10975" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <div class="link-item" style="background: #f5f5f5; color: #999;">No GitHub</div>
                                    <div class="link-item upvote-item">üî• 37</div>
                                    <div class="link-item stars-item">‚≠ê 0</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">BeyondWeb, a synthetic data generation framework, outperforms existing datasets and accelerates training for large language models by optimizing multiple factors.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Recent advances in <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=large%20language%20model">large language model</a> (<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=LLM">LLM</a>) <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=pretraining">pretraining</a> have shown that
simply scaling data quantity eventually leads to diminishing returns, hitting a
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=data%20wall">data wall</a>. In response, the use of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=synthetic%20data">synthetic data</a> for <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=pretraining">pretraining</a> has emerged
as a promising paradigm for pushing the frontier of performance. Despite this,
the factors affecting <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=synthetic%20data">synthetic data</a> quality remain poorly understood. In this
work, we introduce <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=BeyondWeb">BeyondWeb</a>, a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=synthetic%20data">synthetic data</a> generation framework that
produces high-quality <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=synthetic%20data">synthetic data</a> for <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=pretraining">pretraining</a>. <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=BeyondWeb">BeyondWeb</a> significantly
extends the capabilities of traditional web-scale datasets, outperforming
state-of-the-art synthetic <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=pretraining">pretraining</a> datasets such as <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Cosmopedia">Cosmopedia</a> and
Nemotron-CC's high-quality synthetic subset (<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Nemotron-Synth">Nemotron-Synth</a>) by up to 5.1
percentage points (pp) and 2.6pp, respectively, when averaged across a suite of
14 <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=benchmark%20evaluations">benchmark evaluations</a>. It delivers up to 7.7x faster training than open web
data and 2.7x faster than <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Nemotron-Synth">Nemotron-Synth</a>. Remarkably, a 3B model trained for
180B tokens on <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=BeyondWeb">BeyondWeb</a> outperforms an 8B model trained for the same token
budget on <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Cosmopedia">Cosmopedia</a>. We also present several insights from <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=BeyondWeb">BeyondWeb</a> on
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=synthetic%20data">synthetic data</a> for <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=pretraining">pretraining</a>: what drives its benefits, which data to
rephrase and how, and the impact of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=model%20size">model size</a> and family on data quality.
Overall, our work shows that there's no silver bullet for generating
high-quality synthetic <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=pretraining">pretraining</a> data. The best outcomes require jointly
optimizing many factors, a challenging task that requires rigorous science and
practical expertise. Naive approaches can yield modest improvements,
potentially at great cost, while well-executed methods can yield transformative
improvements, as exemplified by <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=BeyondWeb">BeyondWeb</a>.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="WEEKLY DAILY" data-date="2025-08-14" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2508.11116" target="_blank" onclick="event.stopPropagation()">PaperRegister: Boosting Flexible-grained Paper Search via Hierarchical
  Register Indexing</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-08-14</div>
                                        <div class="tags"><span class="tag">DAILY</span><span class="tag">WEEKLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2508.11116" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/Li-Z-Q/PaperRegister" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 19</div>
                                    <div class="link-item stars-item">‚≠ê 2</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">PaperRegister enhances paper search by using hierarchical indexing and adaptive retrieval, supporting flexible and fine-grained queries beyond traditional abstract-based systems.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Paper search is an important activity for researchers, typically involving
using a query with description of a topic to find relevant papers. As research
deepens, paper search requirements may become more flexible, sometimes
involving specific details such as module configuration rather than being
limited to coarse-grained topics. However, previous paper search systems are
unable to meet these flexible-grained requirements, as these systems mainly
collect paper abstracts to construct index of corpus, which lack detailed
information to support retrieval by finer-grained queries. In this work, we
propose PaperRegister, consisted of offline <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=hierarchical%20indexing">hierarchical indexing</a> and online
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=adaptive%20retrieval">adaptive retrieval</a>, transforming traditional abstract-based index into
hierarchical index tree for paper search, thereby supporting queries at
flexible granularity. Experiments on paper search tasks across a range of
granularity demonstrate that PaperRegister achieves the state-of-the-art
performance, and particularly excels in fine-grained scenarios, highlighting
the good potential as an effective solution for flexible-grained paper search
in real-world applications. Code for this work is in
https://github.com/Li-Z-Q/PaperRegister.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="DAILY WEEKLY" data-date="2025-08-14" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2508.10395" target="_blank" onclick="event.stopPropagation()">XQuant: Breaking the Memory Wall for LLM Inference with KV Cache
  Rematerialization</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-08-14</div>
                                        <div class="tags"><span class="tag">DAILY</span><span class="tag">WEEKLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2508.10395" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <div class="link-item" style="background: #f5f5f5; color: #999;">No GitHub</div>
                                    <div class="link-item upvote-item">üî• 16</div>
                                    <div class="link-item stars-item">‚≠ê 0</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">XQuant and XQuant-CL reduce memory consumption in LLM inference through low-bit quantization and cross-layer similarity exploitation, achieving significant memory savings with minimal accuracy loss.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Although <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=LLM%20inference">LLM inference</a> has emerged as a critical workload for many downstream
applications, efficiently inferring LLMs is challenging due to the substantial
memory footprint and bandwidth requirements. In parallel, compute capabilities
have steadily outpaced both memory capacity and bandwidth over the last few
decades, a trend that remains evident in modern GPU hardware and exacerbates
the challenge of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=LLM%20inference">LLM inference</a>. As such, new algorithms are emerging that trade
increased computation for reduced memory operations. To that end, we present
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=XQuant">XQuant</a>, which takes advantage of this trend, enabling an order-of-magnitude
reduction in memory consumption through <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=low-bit%20quantization">low-bit quantization</a> with substantial
accuracy benefits relative to state-of-the-art <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=KV%20cache%20quantization">KV cache quantization</a> methods.
We accomplish this by quantizing and caching the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=layer%20input%20activations">layer input activations</a> X,
instead of using standard KV caching, and then rematerializing the Keys and
Values on-the-fly during inference. This results in an immediate 2times
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=memory%20savings">memory savings</a> compared to KV caching. By applying <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=XQuant">XQuant</a>, we achieve up to
sim 7.7times <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=memory%20savings">memory savings</a> with &lt;0.1 <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=perplexity%20degradation">perplexity degradation</a> compared to
the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=FP16%20baseline">FP16 baseline</a>. Furthermore, our approach leverages the fact that X values
are similar across layers. Building on this observation, we introduce
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=XQuant-CL">XQuant-CL</a>, which exploits the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=cross-layer%20similarity">cross-layer similarity</a> in the X embeddings for
extreme compression. Across different models, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=XQuant-CL">XQuant-CL</a> attains up to
10times <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=memory%20savings">memory savings</a> relative to the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=FP16%20baseline">FP16 baseline</a> with only 0.01
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=perplexity%20degradation">perplexity degradation</a>, and 12.5times <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=memory%20savings">memory savings</a> with only 0.1
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=perplexity%20degradation">perplexity degradation</a>. <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=XQuant">XQuant</a> exploits the rapidly increasing compute
capabilities of hardware platforms to eliminate the memory bottleneck, while
surpassing state-of-the-art <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=KV%20cache%20quantization">KV cache quantization</a> methods and achieving
near-FP16 accuracy across a wide range of models.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="TRENDING MONTHLY" data-date="2025-08-04" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2508.02324" target="_blank" onclick="event.stopPropagation()">Qwen-Image Technical Report</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-08-04</div>
                                        <div class="tags"><span class="tag">MONTHLY</span><span class="tag">TRENDING</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2508.02324" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/QwenLM/Qwen-Image" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 190</div>
                                    <div class="link-item stars-item">‚≠ê 3.11k</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">Qwen-Image, an image generation model, advances text rendering and image editing through a comprehensive data pipeline, progressive training, and dual-encoding mechanism.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                We present Qwen-Image, an image generation foundation model in the Qwen
series that achieves significant advances in complex text rendering and precise
image editing. To address the challenges of complex text rendering, we design a
comprehensive <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=data%20pipeline">data pipeline</a> that includes large-scale data collection,
filtering, annotation, synthesis, and balancing. Moreover, we adopt a
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=progressive%20training">progressive training</a> strategy that starts with non-text-to-text rendering,
evolves from simple to complex textual inputs, and gradually scales up to
paragraph-level descriptions. This <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=curriculum%20learning">curriculum learning</a> approach substantially
enhances the model's native text rendering capabilities. As a result,
Qwen-Image not only performs exceptionally well in alphabetic languages such as
English, but also achieves remarkable progress on more challenging logographic
languages like Chinese. To enhance image editing consistency, we introduce an
improved multi-task training paradigm that incorporates not only traditional
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=text-to-image">text-to-image</a> (T2I) and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=text-image-to-image">text-image-to-image</a> (TI2I) tasks but also
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=image-to-image">image-to-image</a> (I2I) reconstruction, effectively aligning the latent
representations between Qwen2.5-VL and MMDiT. Furthermore, we separately feed
the original image into Qwen2.5-VL and the VAE encoder to obtain semantic and
reconstructive representations, respectively. This <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=dual-encoding%20mechanism">dual-encoding mechanism</a>
enables the editing module to strike a balance between preserving semantic
consistency and maintaining <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=visual%20fidelity">visual fidelity</a>. Qwen-Image achieves
state-of-the-art performance, demonstrating its strong capabilities in both
image generation and editing across multiple benchmarks.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="MONTHLY" data-date="2025-08-07" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2508.05629" target="_blank" onclick="event.stopPropagation()">On the Generalization of SFT: A Reinforcement Learning Perspective with
  Reward Rectification</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-08-07</div>
                                        <div class="tags"><span class="tag">MONTHLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2508.05629" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/yongliang-wu/DFT" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 148</div>
                                    <div class="link-item stars-item">‚≠ê 325</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">Dynamic Fine-Tuning (DFT) improves the generalization of Large Language Models (LLMs) by dynamically rescaling gradients, outperforming standard Supervised Fine-Tuning (SFT) and showing competitive results in offline reinforcement learning.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                We present a simple yet theoretically motivated improvement to Supervised
Fine-Tuning (SFT) for the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Large%20Language%20Model">Large Language Model</a> (LLM), addressing its limited
generalization compared to <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=reinforcement%20learning">reinforcement learning</a> (RL). Through mathematical
analysis, we reveal that standard SFT gradients implicitly encode a problematic
reward structure that may severely restrict the generalization capabilities of
model. To rectify this, we propose <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Dynamic%20Fine-Tuning">Dynamic Fine-Tuning</a> (DFT), stabilizing
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=gradient%20updates">gradient updates</a> for each token by dynamically rescaling the objective function
with the probability of this token. Remarkably, this single-line code change
significantly outperforms standard SFT across multiple challenging benchmarks
and base models, demonstrating greatly improved generalization. Additionally,
our approach shows competitive results in <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=offline%20RL">offline RL</a> settings, offering an
effective yet simpler alternative. This work bridges theoretical insight and
practical solutions, substantially advancing SFT performance. The code will be
available at https://github.com/yongliang-wu/DFT.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="WEEKLY DAILY" data-date="2025-08-14" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2508.10868" target="_blank" onclick="event.stopPropagation()">TexVerse: A Universe of 3D Objects with High-Resolution Textures</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-08-14</div>
                                        <div class="tags"><span class="tag">DAILY</span><span class="tag">WEEKLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2508.10868" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/yiboz2001/TexVerse" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 9</div>
                                    <div class="link-item stars-item">‚≠ê 160</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">TexVerse is a large-scale 3D dataset with high-resolution textures, including PBR materials, rigged models, and animated models, suitable for texture synthesis, PBR material development, and 3D vision tasks.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                We introduce TexVerse, a large-scale 3D dataset featuring high-resolution
textures. While recent advances in large-scale 3D datasets have enhanced
high-resolution geometry generation, creating high-resolution textures
end-to-end remains underexplored due to the lack of suitable datasets. TexVerse
fills this gap with a curated collection of over 858K unique high-resolution 3D
models sourced from Sketchfab, including more than 158K models with physically
based rendering (PBR) materials. Each model encompasses all of its
high-resolution variants, bringing the total to 1.6M 3D instances. TexVerse
also includes specialized subsets: TexVerse-Skeleton, with 69K <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=rigged%20models">rigged models</a>,
and TexVerse-Animation, with 54K <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=animated%20models">animated models</a>, both preserving original
skeleton and animation data uploaded by the user. We also provide detailed
model annotations describing overall characteristics, structural components,
and intricate features. TexVerse offers a high-quality data resource with
wide-ranging potential applications in <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=texture%20synthesis">texture synthesis</a>, PBR material
development, animation, and various <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=3D%20vision">3D vision</a> and graphics tasks.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="DAILY WEEKLY" data-date="2025-08-15" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2508.11255" target="_blank" onclick="event.stopPropagation()">FantasyTalking2: Timestep-Layer Adaptive Preference Optimization for
  Audio-Driven Portrait Animation</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-08-15</div>
                                        <div class="tags"><span class="tag">DAILY</span><span class="tag">WEEKLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2508.11255" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/Fantasy-AMAP/fantasy-talking2" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 8</div>
                                    <div class="link-item stars-item">‚≠ê 13</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">A multimodal reward model and adaptive preference optimization framework improve audio-driven portrait animation by aligning with human preferences across multiple dimensions.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Recent advances in audio-driven portrait animation have demonstrated
impressive capabilities. However, existing methods struggle to align with
fine-grained human preferences across multiple dimensions, such as motion
naturalness, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=lip-sync%20accuracy">lip-sync accuracy</a>, and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=visual%20quality">visual quality</a>. This is due to the
difficulty of optimizing among competing preference objectives, which often
conflict with one another, and the scarcity of large-scale, high-quality
datasets with multidimensional preference annotations. To address these, we
first introduce <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Talking-Critic">Talking-Critic</a>, a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=multimodal%20reward%20model">multimodal reward model</a> that learns
human-aligned reward functions to quantify how well generated videos satisfy
multidimensional expectations. Leveraging this model, we curate <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Talking-NSQ">Talking-NSQ</a>, a
large-scale multidimensional human preference dataset containing 410K
preference pairs. Finally, we propose Timestep-Layer adaptive multi-expert
Preference Optimization (TLPO), a novel framework for aligning diffusion-based
portrait animation models with fine-grained, multidimensional preferences. TLPO
decouples preferences into specialized <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=expert%20modules">expert modules</a>, which are then fused
across timesteps and network layers, enabling comprehensive, fine-grained
enhancement across all dimensions without mutual interference. Experiments
demonstrate that <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Talking-Critic">Talking-Critic</a> significantly outperforms existing methods in
aligning with human preference ratings. Meanwhile, TLPO achieves substantial
improvements over baseline models in <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=lip-sync%20accuracy">lip-sync accuracy</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=motion%20naturalness">motion naturalness</a>, and
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=visual%20quality">visual quality</a>, exhibiting superior performance in both qualitative and
quantitative evaluations. Ours project page:
https://fantasy-amap.github.io/fantasy-talking2/
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="TRENDING" data-date="2025-07-30" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2507.22827" target="_blank" onclick="event.stopPropagation()">ScreenCoder: Advancing Visual-to-Code Generation for Front-End
  Automation via Modular Multimodal Agents</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-07-30</div>
                                        <div class="tags"><span class="tag">TRENDING</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2507.22827" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/leigest519/ScreenCoder" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 92</div>
                                    <div class="link-item stars-item">‚≠ê 1.93k</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">A modular multi-agent framework improves UI-to-code generation by integrating vision-language models, hierarchical layout planning, and adaptive prompt-based synthesis, achieving state-of-the-art performance.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Automating the transformation of user interface (UI) designs into front-end
code holds significant promise for accelerating software development and
democratizing design workflows. While recent large language models (LLMs) have
demonstrated progress in text-to-code generation, many existing approaches rely
solely on natural language prompts, limiting their effectiveness in capturing
spatial layout and visual design intent. In contrast, UI development in
practice is inherently <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=multimodal">multimodal</a>, often starting from visual sketches or
mockups. To address this gap, we introduce a modular multi-agent framework that
performs <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=UI-to-code%20generation">UI-to-code generation</a> in three interpretable stages: grounding,
planning, and generation. The <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=grounding%20agent">grounding agent</a> uses a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=vision-language%20model">vision-language model</a> to
detect and label UI components, the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=planning%20agent">planning agent</a> constructs a hierarchical
layout using front-end engineering priors, and the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=generation%20agent">generation agent</a> produces
HTML/CSS code via <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=adaptive%20prompt-based%20synthesis">adaptive prompt-based synthesis</a>. This design improves
robustness, interpretability, and fidelity over end-to-end black-box methods.
Furthermore, we extend the framework into a scalable <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=data%20engine">data engine</a> that
automatically produces large-scale image-code pairs. Using these synthetic
examples, we fine-tune and reinforce an open-source VLM, yielding notable gains
in UI understanding and code quality. Extensive experiments demonstrate that
our approach achieves state-of-the-art performance in layout accuracy,
structural coherence, and code correctness. Our code is made publicly available
at https://github.com/leigest519/ScreenCoder.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="MONTHLY" data-date="2025-08-08" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2508.06471" target="_blank" onclick="event.stopPropagation()">GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-08-08</div>
                                        <div class="tags"><span class="tag">MONTHLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2508.06471" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/zai-org/GLM-4.5" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 144</div>
                                    <div class="link-item stars-item">‚≠ê 2.13k</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">GLM-4.5, a Mixture-of-Experts large language model with 355B parameters, achieves strong performance across agentic, reasoning, and coding tasks using multi-stage training and reinforcement learning.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                We present GLM-4.5, an open-source <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Mixture-of-Experts">Mixture-of-Experts</a> (MoE) large language
model with 355B total parameters and 32B activated parameters, featuring a
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=hybrid%20reasoning%20method">hybrid reasoning method</a> that supports both thinking and direct response modes.
Through <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=multi-stage%20training">multi-stage training</a> on 23T tokens and comprehensive post-training with
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=expert%20model%20iteration">expert model iteration</a> and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=reinforcement%20learning">reinforcement learning</a>, GLM-4.5 achieves strong
performance across agentic, reasoning, and coding (ARC) tasks, scoring 70.1% on
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=TAU-Bench">TAU-Bench</a>, 91.0% on <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=AIME%2024">AIME 24</a>, and 64.2% on <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=SWE-bench%20Verified">SWE-bench Verified</a>. With much fewer
parameters than several competitors, GLM-4.5 ranks 3rd overall among all
evaluated models and 2nd on <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=agentic%20benchmarks">agentic benchmarks</a>. We release both GLM-4.5 (355B
parameters) and a compact version, GLM-4.5-Air (106B parameters), to advance
research in reasoning and agentic AI systems. Code, models, and more
information are available at https://github.com/zai-org/GLM-4.5.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="WEEKLY DAILY" data-date="2025-08-15" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2508.11203" target="_blank" onclick="event.stopPropagation()">StyleMM: Stylized 3D Morphable Face Model via Text-Driven Aligned Image
  Translation</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-08-15</div>
                                        <div class="tags"><span class="tag">DAILY</span><span class="tag">WEEKLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2508.11203" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/kwanyun/EAS" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 8</div>
                                    <div class="link-item stars-item">‚≠ê 3</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">StyleMM constructs stylized 3DMMs from text descriptions using a diffusion model for image-to-image translation while preserving facial attributes.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                We introduce StyleMM, a novel framework that can construct a stylized 3D
Morphable Model (<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=3DMM">3DMM</a>) based on user-defined text descriptions specifying a
target style. Building upon a pre-trained <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=mesh%20deformation%20network">mesh deformation network</a> and a
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=texture%20generator">texture generator</a> for original <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=3DMM">3DMM</a>-based realistic human faces, our approach
fine-tunes these models using stylized facial images generated via text-guided
image-to-image (i2i) translation with a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=diffusion%20model">diffusion model</a>, which serve as
stylization targets for the rendered mesh. To prevent undesired changes in
identity, facial alignment, or expressions during i2i translation, we introduce
a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=stylization%20method">stylization method</a> that explicitly preserves the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=facial%20attributes">facial attributes</a> of the
source image. By maintaining these critical attributes during image
stylization, the proposed approach ensures consistent 3D style transfer across
the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=3DMM">3DMM</a> parameter space through image-based training. Once trained, StyleMM
enables feed-forward generation of stylized face meshes with explicit control
over shape, expression, and texture parameters, producing meshes with
consistent vertex connectivity and animatability. Quantitative and qualitative
evaluations demonstrate that our approach outperforms state-of-the-art methods
in terms of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=identity-level%20facial%20diversity">identity-level facial diversity</a> and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=stylization%20capability">stylization capability</a>. The
code and videos are available at
[kwanyun.github.io/stylemm_page](kwanyun.github.io/stylemm_page).
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="DAILY WEEKLY" data-date="2025-08-14" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2508.10461" target="_blank" onclick="event.stopPropagation()">X-Node: Self-Explanation is All We Need</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-08-14</div>
                                        <div class="tags"><span class="tag">DAILY</span><span class="tag">WEEKLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2508.10461" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/basiralab/X-Node" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 6</div>
                                    <div class="link-item stars-item">‚≠ê 1</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">X-Node is a self-explaining GNN framework that generates per-node explanations by encoding local topology features and integrating them into the message-passing pipeline, maintaining accuracy while enhancing interpretability.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Graph%20neural%20networks">Graph neural networks</a> (<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=GNNs">GNNs</a>) have achieved state-of-the-art results in
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=computer%20vision">computer vision</a> and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=medical%20image%20classification">medical image classification</a> tasks by capturing structural
dependencies across data instances. However, their decision-making remains
largely opaque, limiting their trustworthiness in high-stakes clinical
applications where <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=interpretability">interpretability</a> is essential. Existing <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=explainability">explainability</a>
techniques for <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=GNNs">GNNs</a> are typically post-hoc and global, offering limited insight
into individual node decisions or local reasoning. We introduce <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=X-Node">X-Node</a>, a
self-explaining GNN framework in which each node generates its own explanation
as part of the prediction process. For every node, we construct a structured
context vector encoding interpretable cues such as <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=degree">degree</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=centrality">centrality</a>,
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=clustering">clustering</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=feature%20saliency">feature saliency</a>, and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=label%20agreement">label agreement</a> within its local topology. A
lightweight <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Reasoner%20module">Reasoner module</a> maps this context into a compact explanation
vector, which serves three purposes: (1) reconstructing the node's latent
embedding via a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=decoder">decoder</a> to enforce faithfulness, (2) generating a natural
language explanation using a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=pre-trained%20LLM">pre-trained LLM</a> (e.g., Grok or Gemini), and (3)
guiding the GNN itself via a "text-injection" mechanism that feeds explanations
back into the message-passing pipeline. We evaluate <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=X-Node">X-Node</a> on two graph
datasets derived from <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=MedMNIST">MedMNIST</a> and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=MorphoMNIST">MorphoMNIST</a>, integrating it with <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=GCN">GCN</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=GAT">GAT</a>,
and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=GIN">GIN</a> backbones. Our results show that <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=X-Node">X-Node</a> maintains competitive
classification accuracy while producing faithful, per-node explanations.
Repository: https://github.com/basiralab/<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=X-Node">X-Node</a>.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="WEEKLY DAILY" data-date="2025-08-15" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2508.11616" target="_blank" onclick="event.stopPropagation()">Controlling Multimodal LLMs via Reward-guided Decoding</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-08-15</div>
                                        <div class="tags"><span class="tag">DAILY</span><span class="tag">WEEKLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2508.11616" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <div class="link-item" style="background: #f5f5f5; color: #999;">No GitHub</div>
                                    <div class="link-item upvote-item">üî• 4</div>
                                    <div class="link-item stars-item">‚≠ê 0</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">A reward-guided decoding method for Multimodal Large Language Models (MLLMs) improves visual grounding by controlling object precision and recall, offering dynamic trade-offs between compute and grounding quality.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                As <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Multimodal%20Large%20Language%20Models">Multimodal Large Language Models</a> (<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=MLLMs">MLLMs</a>) gain widespread applicability, it
is becoming increasingly desirable to adapt them for diverse user needs. In
this paper, we study the adaptation of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=MLLMs">MLLMs</a> through controlled decoding. To
achieve this, we introduce the first method for <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=reward-guided%20decoding">reward-guided decoding</a> of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=MLLMs">MLLMs</a>
and demonstrate its application in improving their <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=visual%20grounding">visual grounding</a>. Our method
involves building <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=reward%20models">reward models</a> for <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=visual%20grounding">visual grounding</a> and using them to guide
the MLLM's decoding process. Concretely, we build two separate <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=reward%20models">reward models</a> to
independently control the degree of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=object%20precision">object precision</a> and recall in the model's
output. Our approach enables on-the-fly controllability of an MLLM's inference
process in two ways: first, by giving control over the relative importance of
each reward function during decoding, allowing a user to dynamically trade off
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=object%20precision">object precision</a> for recall in <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=image%20captioning">image captioning</a> tasks; second, by giving
control over the breadth of the search during decoding, allowing the user to
control the trade-off between the amount of test-time compute and the degree of
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=visual%20grounding">visual grounding</a>. We evaluate our method on standard <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=object%20hallucination">object hallucination</a>
benchmarks, showing that it provides significant controllability over MLLM
inference, while consistently outperforming existing <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=hallucination%20mitigation">hallucination mitigation</a>
methods.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="TRENDING" data-date="2025-08-11" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2508.07901" target="_blank" onclick="event.stopPropagation()">Stand-In: A Lightweight and Plug-and-Play Identity Control for Video
  Generation</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-08-11</div>
                                        <div class="tags"><span class="tag">TRENDING</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2508.07901" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/WeChatCV/Stand-In" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 38</div>
                                    <div class="link-item stars-item">‚≠ê 436</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">A lightweight framework for identity preservation in video generation using conditional image branches and restricted self-attentions outperforms full-parameter methods with minimal additional parameters.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Generating high-fidelity human videos that match user-specified identities is
important yet challenging in the field of generative AI. Existing methods often
rely on an excessive number of training parameters and lack compatibility with
other AIGC tools. In this paper, we propose Stand-In, a lightweight and
plug-and-play framework for identity preservation in video generation.
Specifically, we introduce a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=conditional%20image%20branch">conditional image branch</a> into the pre-trained
video generation model. Identity control is achieved through restricted
self-attentions with <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=conditional%20position%20mapping">conditional position mapping</a>, and can be learned quickly
with only 2000 pairs. Despite incorporating and training just sim1\%
additional parameters, our framework achieves excellent results in video
quality and identity preservation, outperforming other full-parameter training
methods. Moreover, our framework can be seamlessly integrated for other tasks,
such as <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=subject-driven%20video%20generation">subject-driven video generation</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=pose-referenced%20video%20generation">pose-referenced video generation</a>,
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=stylization">stylization</a>, and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=face%20swapping">face swapping</a>.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="DAILY WEEKLY" data-date="2025-08-14" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2508.10894" target="_blank" onclick="event.stopPropagation()">MAESTRO: Masked AutoEncoders for Multimodal, Multitemporal, and
  Multispectral Earth Observation Data</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-08-14</div>
                                        <div class="tags"><span class="tag">DAILY</span><span class="tag">WEEKLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2508.10894" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/IGNF/MAESTRO" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 2</div>
                                    <div class="link-item stars-item">‚≠ê 9</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">MAESTRO, an adapted Masked Autoencoder with optimized fusion strategies and spectral prior normalization, achieves state-of-the-art performance on multitemporal Earth observation tasks.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Self-supervised%20learning">Self-supervised learning</a> holds great promise for remote sensing, but standard
self-supervised methods must be adapted to the unique characteristics of Earth
observation data. We take a step in this direction by conducting a
comprehensive benchmark of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=fusion%20strategies">fusion strategies</a> and reconstruction target
normalization schemes for <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=multimodal">multimodal</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=multitemporal">multitemporal</a>, and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=multispectral">multispectral</a> Earth
observation data. Based on our findings, we propose MAESTRO, a novel adaptation
of the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Masked%20Autoencoder">Masked Autoencoder</a>, featuring optimized <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=fusion%20strategies">fusion strategies</a> and a tailored
target normalization scheme that introduces a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=spectral%20prior">spectral prior</a> as a
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=self-supervisory%20signal">self-supervisory signal</a>. Evaluated on four <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Earth%20observation">Earth observation</a> datasets, MAESTRO
sets a new state-of-the-art on tasks that strongly rely on <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=multitemporal">multitemporal</a>
dynamics, while remaining highly competitive on tasks dominated by a single
mono-temporal modality. Code to reproduce all our experiments is available at
https://github.com/ignf/maestro.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="WEEKLY DAILY" data-date="2025-08-08" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2508.06429" target="_blank" onclick="event.stopPropagation()">SPARSE Data, Rich Results: Few-Shot Semi-Supervised Learning via
  Class-Conditioned Image Translation</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-08-08</div>
                                        <div class="tags"><span class="tag">DAILY</span><span class="tag">WEEKLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2508.06429" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/GuidoManni/SPARSE" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 2</div>
                                    <div class="link-item stars-item">‚≠ê 0</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">A GAN-based semi-supervised learning framework improves medical image classification with minimal labeled data by integrating specialized neural networks and ensemble-based pseudo-labeling.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Deep learning has revolutionized medical imaging, but its effectiveness is
severely limited by insufficient labeled training data. This paper introduces a
novel <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=GAN-based%20semi-supervised%20learning">GAN-based semi-supervised learning</a> framework specifically designed for
low labeled-data regimes, evaluated across settings with 5 to 50 labeled
samples per class. Our approach integrates three specialized neural networks --
a generator for <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=class-conditioned%20image%20translation">class-conditioned image translation</a>, a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=discriminator">discriminator</a> for
authenticity assessment and classification, and a dedicated <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=classifier">classifier</a> --
within a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=three-phase%20training%20framework">three-phase training framework</a>. The method alternates between
supervised training on limited labeled data and unsupervised learning that
leverages abundant unlabeled images through image-to-image translation rather
than generation from noise. We employ <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=ensemble-based%20pseudo-labeling">ensemble-based pseudo-labeling</a> that
combines <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=confidence-weighted%20predictions">confidence-weighted predictions</a> from the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=discriminator">discriminator</a> and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=classifier">classifier</a>
with temporal consistency through <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=exponential%20moving%20averaging">exponential moving averaging</a>, enabling
reliable label estimation for unlabeled data. Comprehensive evaluation across
eleven <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=MedMNIST%20datasets">MedMNIST datasets</a> demonstrates that our approach achieves statistically
significant improvements over six state-of-the-art GAN-based semi-supervised
methods, with particularly strong performance in the extreme <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=5-shot%20setting">5-shot setting</a>
where the scarcity of labeled data is most challenging. The framework maintains
its superiority across all evaluated settings (5, 10, 20, and 50 shots per
class). Our approach offers a practical solution for medical imaging
applications where annotation costs are prohibitive, enabling robust
classification performance even with minimal labeled data. Code is available at
https://github.com/GuidoManni/SPARSE.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="TRENDING" data-date="2025-08-14" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2508.10881" target="_blank" onclick="event.stopPropagation()">ToonComposer: Streamlining Cartoon Production with Generative
  Post-Keyframing</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-08-14</div>
                                        <div class="tags"><span class="tag">TRENDING</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2508.10881" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/TencentARC/ToonComposer" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 49</div>
                                    <div class="link-item stars-item">‚≠ê 206</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">ToonComposer is a generative model that unifies inbetweening and colorization in cartoon production, using sparse sketches and a cartoon adaptation method to improve visual quality and efficiency.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Traditional cartoon and anime production involves keyframing, inbetweening,
and colorization stages, which require intensive manual effort. Despite recent
advances in AI, existing methods often handle these stages separately, leading
to error accumulation and artifacts. For instance, inbetweening approaches
struggle with large motions, while colorization methods require dense per-frame
sketches. To address this, we introduce ToonComposer, a generative model that
unifies inbetweening and colorization into a single post-keyframing stage.
ToonComposer employs a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=sparse%20sketch%20injection">sparse sketch injection</a> mechanism to provide precise
control using keyframe sketches. Additionally, it uses a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=cartoon%20adaptation">cartoon adaptation</a>
method with the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=spatial%20low-rank%20adapter">spatial low-rank adapter</a> to tailor a modern video foundation
model to the cartoon domain while keeping its temporal prior intact. Requiring
as few as a single sketch and a colored reference frame, ToonComposer excels
with sparse inputs, while also supporting multiple sketches at any temporal
location for more precise motion control. This dual capability reduces manual
workload and improves flexibility, empowering artists in real-world scenarios.
To evaluate our model, we further created <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=PKBench">PKBench</a>, a benchmark featuring
human-drawn sketches that simulate real-world use cases. Our evaluation
demonstrates that ToonComposer outperforms existing methods in <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=visual%20quality">visual quality</a>,
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=motion%20consistency">motion consistency</a>, and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=production%20efficiency">production efficiency</a>, offering a superior and more
flexible solution for AI-assisted cartoon production.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="TRENDING" data-date="2025-08-13" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2508.09736" target="_blank" onclick="event.stopPropagation()">Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with
  Long-Term Memory</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-08-13</div>
                                        <div class="tags"><span class="tag">TRENDING</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2508.09736" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/ByteDance-Seed/m3-agent" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 46</div>
                                    <div class="link-item stars-item">‚≠ê 341</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">M3-Agent, a multimodal agent with long-term memory, performs multi-turn reasoning and outperforms baselines on a new long-video question answering benchmark.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                We introduce M3-Agent, a novel <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=multimodal%20agent">multimodal agent</a> framework equipped with
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=long-term%20memory">long-term memory</a>. Like humans, M3-Agent can process real-time visual and
auditory inputs to build and update its <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=long-term%20memory">long-term memory</a>. Beyond episodic
memory, it also develops <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=semantic%20memory">semantic memory</a>, enabling it to accumulate world
knowledge over time. Its memory is organized in an entity-centric, multimodal
format, allowing deeper and more consistent understanding of the environment.
Given an instruction, M3-Agent autonomously performs multi-turn, iterative
reasoning and retrieves relevant information from memory to accomplish the
task. To evaluate memory effectiveness and memory-based reasoning in multimodal
agents, we develop <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=M3-Bench">M3-Bench</a>, a new <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=long-video%20question%20answering%20benchmark">long-video question answering benchmark</a>.
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=M3-Bench">M3-Bench</a> comprises 100 newly recorded real-world videos captured from a robot's
perspective (<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=M3-Bench">M3-Bench</a>-robot) and 929 web-sourced videos across diverse
scenarios (<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=M3-Bench">M3-Bench</a>-web). We annotate question-answer pairs designed to test
key capabilities essential for agent applications, such as <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=human%20understanding">human understanding</a>,
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=general%20knowledge%20extraction">general knowledge extraction</a>, and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=cross-modal%20reasoning">cross-modal reasoning</a>. Experimental results
show that M3-Agent, trained via <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=reinforcement%20learning">reinforcement learning</a>, outperforms the
strongest baseline, a prompting agent using <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Gemini-1.5-pro">Gemini-1.5-pro</a> and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=GPT-4o">GPT-4o</a>,
achieving 6.7%, 7.7%, and 5.3% higher accuracy on <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=M3-Bench">M3-Bench</a>-robot, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=M3-Bench">M3-Bench</a>-web
and VideoMME-long, respectively. Our work advances the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=multimodal%20agent">multimodal agent</a>s toward
more human-like <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=long-term%20memory">long-term memory</a> and provides insights into their practical
design. Model, code and data are available at
https://github.com/bytedance-seed/m3-agent
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="TRENDING MONTHLY" data-date="2025-08-14" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2508.10711" target="_blank" onclick="event.stopPropagation()">NextStep-1: Toward Autoregressive Image Generation with Continuous
  Tokens at Scale</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-08-14</div>
                                        <div class="tags"><span class="tag">MONTHLY</span><span class="tag">TRENDING</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2508.10711" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/stepfun-ai/NextStep-1" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 130</div>
                                    <div class="link-item stars-item">‚≠ê 417</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">NextStep-1, a 14B autoregressive model with a 157M flow matching head, achieves state-of-the-art performance in text-to-image generation and image editing by processing discrete text tokens and continuous image tokens.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Prevailing autoregressive (AR) models for text-to-image generation either
rely on heavy, computationally-intensive <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=diffusion%20models">diffusion models</a> to process continuous
image tokens, or employ <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=vector%20quantization">vector quantization</a> (VQ) to obtain discrete tokens with
quantization loss. In this paper, we push the autoregressive paradigm forward
with NextStep-1, a 14B autoregressive model paired with a 157M <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=flow%20matching">flow matching</a>
head, training on discrete text tokens and continuous image tokens with
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=next-token%20prediction">next-token prediction</a> objectives. NextStep-1 achieves state-of-the-art
performance for <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=autoregressive%20models">autoregressive models</a> in text-to-image generation tasks,
exhibiting strong capabilities in <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=high-fidelity%20image%20synthesis">high-fidelity image synthesis</a>. Furthermore,
our method shows strong performance in <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=image%20editing">image editing</a>, highlighting the power
and versatility of our unified approach. To facilitate open research, we will
release our code and models to the community.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="TRENDING" data-date="2025-08-10" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2508.07407" target="_blank" onclick="event.stopPropagation()">A Comprehensive Survey of Self-Evolving AI Agents: A New Paradigm
  Bridging Foundation Models and Lifelong Agentic Systems</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-08-10</div>
                                        <div class="tags"><span class="tag">TRENDING</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2508.07407" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/EvoAgentX/Awesome-Self-Evolving-Agents" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 78</div>
                                    <div class="link-item stars-item">‚≠ê 232</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">A survey of self-evolving AI agents that adapt to dynamic environments through automatic enhancement based on interaction data and feedback.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Recent advances in large language models have sparked growing interest in AI
agents capable of solving complex, real-world tasks. However, most existing
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=agent%20system">agent system</a>s rely on manually crafted configurations that remain static after
deployment, limiting their ability to adapt to dynamic and evolving
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=environment">environment</a>s. To this end, recent research has explored <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=agent%20evolution">agent evolution</a>
techniques that aim to automatically enhance <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=agent%20system">agent system</a>s based on interaction
data and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=environment">environment</a>al feedback. This emerging direction lays the foundation
for self-evolving AI agents, which bridge the static capabilities of foundation
models with the continuous adaptability required by lifelong agentic systems.
In this survey, we provide a comprehensive review of existing techniques for
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=self-evolving%20agentic%20systems">self-evolving agentic systems</a>. Specifically, we first introduce a unified
conceptual framework that abstracts the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=feedback%20loop">feedback loop</a> underlying the design of
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=self-evolving%20agentic%20systems">self-evolving agentic systems</a>. The framework highlights four key components:
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=System%20Inputs">System Inputs</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Agent%20System">Agent System</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Environment">Environment</a>, and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Optimisers">Optimisers</a>, serving as a
foundation for understanding and comparing different strategies. Based on this
framework, we systematically review a wide range of self-evolving techniques
that target different components of the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=agent%20system">agent system</a>. We also investigate
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=domain-specific%20evolution%20strategies">domain-specific evolution strategies</a> developed for specialised fields such as
biomedicine, programming, and finance, where optimisation objectives are
tightly coupled with domain constraints. In addition, we provide a dedicated
discussion on the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=evaluation">evaluation</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=safety">safety</a>, and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=ethical%20considerations">ethical considerations</a> for
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=self-evolving%20agentic%20systems">self-evolving agentic systems</a>, which are critical to ensuring their
effectiveness and reliability. This survey aims to provide researchers and
practitioners with a systematic understanding of self-evolving AI agents,
laying the foundation for the development of more adaptive, autonomous, and
lifelong agentic systems.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="TRENDING" data-date="2025-08-14" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2508.10833" target="_blank" onclick="event.stopPropagation()">UI-Venus Technical Report: Building High-performance UI Agents with RFT</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-08-14</div>
                                        <div class="tags"><span class="tag">TRENDING</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2508.10833" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/inclusionAI/UI-Venus" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 35</div>
                                    <div class="link-item stars-item">‚≠ê 85</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">UI-Venus, a multimodal large language model-based UI agent, achieves state-of-the-art performance in UI grounding and navigation tasks using reinforcement fine-tuning and novel self-evolving frameworks.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                We present UI-Venus, a native UI agent that takes only screenshots as input
based on a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=multimodal%20large%20language%20model">multimodal large language model</a>. UI-Venus achieves SOTA performance
on both <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=UI%20grounding">UI grounding</a> and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=navigation%20tasks">navigation tasks</a> using only several hundred thousand
high-quality training samples through <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=reinforcement%20finetune">reinforcement finetune</a> (RFT) based on
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Qwen2.5-VL">Qwen2.5-VL</a>. Specifically, the 7B and 72B variants of UI-Venus obtain 94.1% /
50.8% and 95.3% / 61.9% on the standard grounding benchmarks, i.e.,
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Screenspot-V2">Screenspot-V2</a> / <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Pro">Pro</a>, surpassing the previous SOTA baselines including
open-source GTA1 and closed-source UI-TARS-1.5.To show UI-Venus's summary and
planing ability, we also evaluate it on the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=AndroidWorld">AndroidWorld</a>, an online UI
navigation arena, on which our 7B and 72B variants achieve 49.1% and 65.9%
success rate, also beating existing models.To achieve this, we introduce
carefully designed <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=reward%20functions">reward functions</a> for both <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=UI%20grounding">UI grounding</a> and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=navigation%20tasks">navigation tasks</a>
and corresponding efficient <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=data%20cleaning%20strategies">data cleaning strategies</a>.To further boost
navigation performance, we <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=pro">pro</a>pose <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Self-Evolving%20Trajectory%20History%20Alignment">Self-Evolving Trajectory History Alignment</a>
\&amp; <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Sparse%20Action%20Enhancement">Sparse Action Enhancement</a> that refine historical reasoning traces and
balances the distribution of sparse but critical actions, leading to more
coherent planning and better generalization in complex UI tasks. Our
contributions include the publish of SOTA open-source UI agents, comprehensive
data cleaning <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=pro">pro</a>tocols and a novel self-evolving framework for im<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=pro">pro</a>ving
navigation performance, which encourage further research and development in the
community. Code is available at https://github.com/antgroup/UI-Venus.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="MONTHLY" data-date="2025-08-14" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2508.10433" target="_blank" onclick="event.stopPropagation()">We-Math 2.0: A Versatile MathBook System for Incentivizing Visual
  Mathematical Reasoning</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-08-14</div>
                                        <div class="tags"><span class="tag">MONTHLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2508.10433" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/We-Math/We-Math2.0" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 139</div>
                                    <div class="link-item stars-item">‚≠ê 138</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">We-Math 2.0 enhances MLLMs' mathematical reasoning through a structured knowledge system, model-centric data space modeling, and reinforcement learning, demonstrating competitive performance on benchmarks.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Multimodal%20Large%20Language%20Models">Multimodal Large Language Models</a> (<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=MLLMs">MLLMs</a>) have demonstrated impressive
capabilities across various tasks, but still struggle with complex mathematical
reasoning. Existing research primarily focuses on dataset construction and
method optimization, often overlooking two critical aspects: comprehensive
knowledge-driven design and model-centric data space modeling. In this paper,
we introduce We-Math 2.0, a unified system that integrates a structured
mathematical knowledge system, model-centric data space modeling, and a
reinforcement learning (RL)-based training paradigm to comprehensively enhance
the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=mathematical%20reasoning">mathematical reasoning</a> abilities of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=MLLMs">MLLMs</a>. The key contributions of We-Math
2.0 are fourfold: (1) <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=MathBook%20Knowledge%20System">MathBook Knowledge System</a>: We construct a five-level
hierarchical system encompassing 491 knowledge points and 1,819 fundamental
principles. (2) <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=MathBook-Standard">MathBook-Standard</a> &amp; Pro: We develop <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=MathBook-Standard">MathBook-Standard</a>, a
dataset that ensures broad conceptual coverage and flexibility through dual
expansion. Additionally, we define a three-dimensional difficulty space and
generate 7 progressive variants per problem to build <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=MathBook-Pro">MathBook-Pro</a>, a
challenging dataset for robust training. (3) <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=MathBook-RL">MathBook-RL</a>: We propose a
two-stage RL framework comprising: (i) <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Cold-Start%20Fine-tuning">Cold-Start Fine-tuning</a>, which aligns the
model with knowledge-oriented <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=chain-of-thought%20reasoning">chain-of-thought reasoning</a>; and (ii) Progressive
Alignment RL, leveraging <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=average-reward%20learning">average-reward learning</a> and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=dynamic%20data%20scheduling">dynamic data scheduling</a> to
achieve progressive alignment across difficulty levels. (4) <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=MathBookEval">MathBookEval</a>: We
introduce a comprehensive benchmark covering all 491 knowledge points with
diverse reasoning step distributions. Experimental results show that
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=MathBook-RL">MathBook-RL</a> performs competitively with existing baselines on four widely-used
benchmarks and achieves strong results on <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=MathBookEval">MathBookEval</a>, suggesting promising
generalization in <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=mathematical%20reasoning">mathematical reasoning</a>.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="MONTHLY" data-date="2025-08-06" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2508.04026" target="_blank" onclick="event.stopPropagation()">VeriGUI: Verifiable Long-Chain GUI Dataset</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-08-06</div>
                                        <div class="tags"><span class="tag">MONTHLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2508.04026" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/VeriGUI-Team/VeriGUI" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 137</div>
                                    <div class="link-item stars-item">‚≠ê 72</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">VeriGUI is a novel dataset for evaluating GUI agents in long-horizon tasks, emphasizing long-chain complexity and subtask-level verifiability.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Recent studies have delved into constructing autonomous agents capable of
performing complex <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Graphical%20User%20Interface%20(GUI)">Graphical User Interface (GUI)</a>-based computer tasks, with
the potential to revolutionize human-computer interaction. Despite encouraging
results, existing efforts mainly focus on short-term interactions and rely on
outcome-only verification, thereby limiting their scalability in real-world GUI
applications that demand long-horizon task decomposition and execution. In this
work, we introduce VeriGUI, a novel verifiable long-chain GUI dataset designed
to facilitate the development and evaluation of generalist <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=GUI%20agents">GUI agents</a> operating
in realistic computer environments. Our dataset emphasizes two critical
dimensions: (1) <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=long-chain%20complexity">long-chain complexity</a>, with tasks decomposed into a sequence of
interdependent subtasks spanning hundreds of steps, explicitly designed to
allow any subtask to serve as a valid starting point; and (2) subtask-level
verifiability, which enables diverse exploration strategies within each
subtask, while ensuring that each subtask-level goal remains verifiable and
consistent. The dataset consists of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=GUI%20task%20trajectories">GUI task trajectories</a> across both <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=desktop">desktop</a>
and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=web">web</a>, annotated by <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=human%20experts">human experts</a>. Extensive experiments on VeriGUI using
various agents with different foundation models reveal significant performance
gaps in handling <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=long-horizon%20tasks">long-horizon tasks</a>, highlighting the need for more robust
planning and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=decision-making%20capabilities">decision-making capabilities</a> in <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=GUI%20agents">GUI agents</a>.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="TRENDING" data-date="2025-08-05" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2508.03680" target="_blank" onclick="event.stopPropagation()">Agent Lightning: Train ANY AI Agents with Reinforcement Learning</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-08-05</div>
                                        <div class="tags"><span class="tag">TRENDING</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2508.03680" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/microsoft/agent-lightning" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 58</div>
                                    <div class="link-item stars-item">‚≠ê 875</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">Agent Lightning is a flexible RL framework for training LLMs in various agents, using a hierarchical RL algorithm and decoupling execution from training to handle complex interactions.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                We present Agent Lightning, a flexible and extensible framework that enables
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Reinforcement%20Learning">Reinforcement Learning</a> (RL)-based training of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Large%20Language%20Models">Large Language Models</a> (LLMs) for
any AI agent. Unlike existing methods that tightly couple RL training with
agent or rely on sequence concatenation with masking, Agent Lightning achieves
complete decoupling between agent execution and training, allowing seamless
integration with existing agents developed via diverse ways (e.g., using
frameworks like LangChain, OpenAI Agents SDK, AutoGen, and building from
scratch) with almost ZERO code modifications. By formulating agent execution as
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Markov%20decision%20process">Markov decision process</a>, we define an unified data interface and propose a
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=hierarchical%20RL%20algorithm">hierarchical RL algorithm</a>, LightningRL, which contains a credit assignment
module, allowing us to decompose trajectories generated by ANY agents into
training transition. This enables RL to handle complex interaction logic, such
as multi-agent scenarios and dynamic workflows. For the system design, we
introduce a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Training-Agent%20Disaggregation%20architecture">Training-Agent Disaggregation architecture</a>, and brings agent
observability frameworks into agent runtime, providing a standardized agent
finetuning interface. Experiments across <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=text-to-SQL">text-to-SQL</a>, retrieval-augmented
generation, and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=math%20tool-use%20tasks">math tool-use tasks</a> demonstrate stable, continuous
improvements, showcasing the framework's potential for real-world agent
training and deployment.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="MONTHLY" data-date="2025-08-04" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2508.02193" target="_blank" onclick="event.stopPropagation()">Seed Diffusion: A Large-Scale Diffusion Language Model with High-Speed
  Inference</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-08-04</div>
                                        <div class="tags"><span class="tag">MONTHLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2508.02193" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <div class="link-item" style="background: #f5f5f5; color: #999;">No GitHub</div>
                                    <div class="link-item upvote-item">üî• 123</div>
                                    <div class="link-item stars-item">‚≠ê 0</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">Seed Diffusion Preview, a discrete-state diffusion language model, achieves fast inference speeds through parallel generation, outperforming Mercury and Gemini Diffusion in speed and quality.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                We present <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Seed%20Diffusion%20Preview">Seed Diffusion Preview</a>, a large-scale language model based on
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=discrete-state%20diffusion">discrete-state diffusion</a>, offering remarkably fast inference speed. Thanks to
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=non-sequential">non-sequential</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=parallel%20generation">parallel generation</a>, discrete diffusion models provide a
notable speedup to mitigate the inherent latency of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=token-by-token%20decoding">token-by-token decoding</a>, as
demonstrated recently (e.g., Mercury Coder, Gemini Diffusion). Seed Diffusion
Preview achieves an inference speed of 2,146 token/s over <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=H20%20GPUs">H20 GPUs</a> while
maintaining competitive performance across a sweep of standard code evaluation
benchmarks, significantly faster than contemporary Mercury and Gemini
Diffusion, establishing new state of the art on the speed-quality Pareto
frontier for code models.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="MONTHLY" data-date="2025-08-07" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2508.05748" target="_blank" onclick="event.stopPropagation()">WebWatcher: Breaking New Frontier of Vision-Language Deep Research Agent</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-08-07</div>
                                        <div class="tags"><span class="tag">MONTHLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2508.05748" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/Alibaba-NLP/WebAgent//" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 112</div>
                                    <div class="link-item stars-item">‚≠ê 6.16k</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">WebWatcher, a multimodal agent with enhanced visual-language reasoning, outperforms existing agents in complex visual and textual information retrieval tasks using synthetic trajectories and reinforcement learning.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Web agents such as Deep Research have demonstrated superhuman cognitive
abilities, capable of solving highly challenging information-seeking problems.
However, most research remains primarily text-centric, overlooking visual
information in the real world. This makes <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=multimodal">multimodal</a> Deep Research highly
challenging, as such agents require much stronger reasoning abilities in
perception, logic, knowledge, and the use of more sophisticated tools compared
to text-based agents. To address this limitation, we introduce WebWatcher, a
multi-modal Agent for Deep Research equipped with enhanced visual-language
reasoning capabilities. It leverages high-quality synthetic <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=multimodal">multimodal</a>
trajectories for efficient cold start training, utilizes various tools for deep
reasoning, and further enhances generalization through <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=reinforcement%20learning">reinforcement learning</a>.
To better evaluate the capabilities of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=multimodal">multimodal</a> agents, we propose
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=BrowseComp-VL">BrowseComp-VL</a>, a benchmark with BrowseComp-style that requires complex
information retrieval involving both visual and textual information.
Experimental results show that WebWatcher significantly outperforms proprietary
baseline, RAG workflow and open-source agents in four challenging VQA
benchmarks, which paves the way for solving complex <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=multimodal">multimodal</a>
information-seeking tasks.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="TRENDING" data-date="2024-03-20" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2403.13372" target="_blank" onclick="event.stopPropagation()">LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2024-03-20</div>
                                        <div class="tags"><span class="tag">TRENDING</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2403.13372" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/hiyouga/LLaMA-Factory" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 122</div>
                                    <div class="link-item stars-item">‚≠ê 56.4k</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">LlamaFactory is a unified framework enabling efficient fine-tuning of large language models across various tasks using a web-based user interface.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Efficient%20fine-tuning">Efficient fine-tuning</a> is vital for adapting <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=large%20language%20models">large language models</a> (LLMs) to
downstream tasks. However, it requires non-trivial efforts to implement these
methods on different models. We present <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=LlamaFactory">LlamaFactory</a>, a unified framework that
integrates a suite of cutting-edge efficient training methods. It allows users
to flexibly customize the fine-tuning of 100+ LLMs without the need for coding
through the built-in web UI <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=LlamaBoard">LlamaBoard</a>. We empirically validate the efficiency
and effectiveness of our framework on <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=language%20modeling">language modeling</a> and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=text%20generation">text generation</a>
tasks. It has been released at https://github.com/hiyouga/<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=LLaMA">LLaMA</a>-Factory and
already received over 13,000 stars and 1,600 forks.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="MONTHLY" data-date="2025-08-09" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2508.07050" target="_blank" onclick="event.stopPropagation()">ReasonRank: Empowering Passage Ranking with Strong Reasoning Ability</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-08-09</div>
                                        <div class="tags"><span class="tag">MONTHLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2508.07050" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/8421BCD/ReasonRank" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 111</div>
                                    <div class="link-item stars-item">‚≠ê 110</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">A reasoning-intensive reranker, ReasonRank, achieves state-of-the-art performance in passage ranking tasks by using synthesized training data and a two-stage post-training approach with reinforcement learning.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Large%20Language%20Model">Large Language Model</a> (LLM) based <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=listwise%20ranking">listwise ranking</a> has shown superior
performance in many passage ranking tasks. With the development of Large
Reasoning Models, many studies have demonstrated that <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=step-by-step%20reasoning">step-by-step reasoning</a>
during test-time helps improve <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=listwise%20ranking">listwise ranking</a> performance. However, due to
the scarcity of reasoning-intensive training data, existing rerankers perform
poorly in many complex ranking scenarios and the ranking ability of
reasoning-intensive rerankers remains largely underdeveloped. In this paper, we
first propose an automated reasoning-intensive training data synthesis
framework, which sources training queries and passages from diverse domains and
applies <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=DeepSeek-R1">DeepSeek-R1</a> to generate high-quality training labels. A
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=self-consistency%20data%20filtering">self-consistency data filtering</a> mechanism is designed to ensure the data
quality. To empower the listwise reranker with strong reasoning ability, we
further propose a two-stage post-training approach, which includes a cold-start
supervised fine-tuning (SFT) stage for reasoning pattern learning and a
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=reinforcement%20learning">reinforcement learning</a> (RL) stage for further ranking ability enhancement.
During the RL stage, based on the nature of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=listwise%20ranking">listwise ranking</a>, we design a
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=multi-view%20ranking%20reward">multi-view ranking reward</a>, which is more effective than a ranking metric-based
reward. Extensive experiments demonstrate that our trained reasoning-intensive
reranker ReasonRank outperforms existing baselines significantly and
also achieves much lower latency than pointwise reranker Rank1. Through
further experiments, our ReasonRank has achieved state-of-the-art (SOTA)
performance 40.6 on the BRIGHT
leaderboard\footnote{https://brightbenchmark.github.io/.} Our codes are
available at https://github.com/8421BCD/ReasonRank.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="TRENDING" data-date="2025-08-11" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2508.08248" target="_blank" onclick="event.stopPropagation()">StableAvatar: Infinite-Length Audio-Driven Avatar Video Generation</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-08-11</div>
                                        <div class="tags"><span class="tag">TRENDING</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2508.08248" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/Francis-Rings/StableAvatar" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 24</div>
                                    <div class="link-item stars-item">‚≠ê 502</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">StableAvatar, an end-to-end video diffusion transformer, synthesizes infinite-length high-quality audio-driven avatar videos with natural synchronization and identity consistency using a Time-step-aware Audio Adapter and Audio Native Guidance Mechanism.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Current <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=diffusion%20models">diffusion models</a> for <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=audio-driven%20avatar%20video%20generation">audio-driven avatar video generation</a> struggle to
synthesize long videos with natural audio synchronization and identity
consistency. This paper presents StableAvatar, the first end-to-end video
diffusion transformer that synthesizes infinite-length high-quality videos
without post-processing. Conditioned on a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=reference%20image">reference image</a> and audio,
StableAvatar integrates tailored training and inference modules to enable
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=infinite-length%20video%20generation">infinite-length video generation</a>. We observe that the main reason preventing
existing models from generating long videos lies in their audio modeling. They
typically rely on third-party off-the-shelf extractors to obtain audio
embeddings, which are then directly injected into the diffusion model via
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=cross-attention">cross-attention</a>. Since current <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=diffusion%20backbones">diffusion backbones</a> lack any audio-related
priors, this approach causes severe <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=latent%20distribution%20error%20accumulation">latent distribution error accumulation</a>
across video clips, leading the latent distribution of subsequent segments to
drift away from the optimal distribution gradually. To address this,
StableAvatar introduces a novel <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Time-step-aware%20Audio%20Adapter">Time-step-aware Audio Adapter</a> that prevents
error accumulation via time-step-aware modulation. During inference, we propose
a novel <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Audio%20Native%20Guidance%20Mechanism">Audio Native Guidance Mechanism</a> to further enhance the audio
synchronization by leveraging the diffusion's own evolving joint audio-latent
prediction as a dynamic guidance signal. To enhance the smoothness of the
infinite-length videos, we introduce a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Dynamic%20Weighted%20Sliding-window%20Strategy">Dynamic Weighted Sliding-window Strategy</a>
that fuses latent over time. Experiments on benchmarks show the effectiveness
of StableAvatar both qualitatively and quantitatively.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="MONTHLY" data-date="2025-08-07" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2508.05004" target="_blank" onclick="event.stopPropagation()">R-Zero: Self-Evolving Reasoning LLM from Zero Data</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-08-07</div>
                                        <div class="tags"><span class="tag">MONTHLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2508.05004" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/Chengsong-Huang/R-Zero" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 110</div>
                                    <div class="link-item stars-item">‚≠ê 475</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">R-Zero is a self-evolving framework that autonomously generates and learns from its own training data, improving reasoning capabilities in LLMs without human-curated tasks.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Self-evolving%20Large%20Language%20Models">Self-evolving Large Language Models</a> (<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=LLMs">LLMs</a>) offer a scalable path toward
super-intelligence by autonomously generating, refining, and learning from
their own experiences. However, existing methods for training such models still
rely heavily on vast human-curated tasks and labels, typically via fine-tuning
or reinforcement learning, which poses a fundamental bottleneck to advancing AI
systems toward capabilities beyond human intelligence. To overcome this
limitation, we introduce <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=R-Zero">R-Zero</a>, a fully autonomous framework that generates
its own training data from scratch. Starting from a single base LLM, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=R-Zero">R-Zero</a>
initializes two independent models with distinct roles, a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Challenger">Challenger</a> and a
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Solver">Solver</a>. These models are optimized separately and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=co-evolve">co-evolve</a> through
interaction: the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Challenger">Challenger</a> is rewarded for proposing tasks near the edge of
the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Solver">Solver</a> capability, and the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Solver">Solver</a> is rewarded for solving increasingly
challenging tasks posed by the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Challenger">Challenger</a>. This process yields a targeted,
self-improving curriculum without any pre-existing tasks and labels.
Empirically, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=R-Zero">R-Zero</a> substantially improves reasoning capability across
different backbone <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=LLMs">LLMs</a>, e.g., boosting the Qwen3-4B-Base by +6.49 on
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=math-reasoning%20benchmarks">math-reasoning benchmarks</a> and +7.54 on <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=general-domain%20reasoning%20benchmarks">general-domain reasoning benchmarks</a>.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="TRENDING" data-date="2025-03-03" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2503.01710" target="_blank" onclick="event.stopPropagation()">Spark-TTS: An Efficient LLM-Based Text-to-Speech Model with
  Single-Stream Decoupled Speech Tokens</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-03-03</div>
                                        <div class="tags"><span class="tag">TRENDING</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2503.01710" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/sparkaudio/spark-tts" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 6</div>
                                    <div class="link-item stars-item">‚≠ê 10.4k</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">Spark-TTS, utilizing BiCodec and Qwen2.5 LLM, achieves superior zero-shot voice cloning and customizable text-to-speech synthesis through a single-stream codec and chain-of-thought approach.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                Recent advancements in large language models (LLMs) have driven significant
progress in <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=zero-shot%20text-to-speech">zero-shot text-to-speech</a> (TTS) synthesis. However, existing
foundation models rely on multi-stage processing or complex architectures for
predicting multiple codebooks, limiting efficiency and integration flexibility.
To overcome these challenges, we introduce Spark-TTS, a novel system powered by
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=BiCodec">BiCodec</a>, a single-stream speech codec that decomposes speech into two
complementary token types: low-bitrate <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=semantic%20tokens">semantic tokens</a> for linguistic content
and fixed-length <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=global%20tokens">global tokens</a> for <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=speaker%20attributes">speaker attributes</a>. This disentangled
representation, combined with the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Qwen2.5">Qwen2.5</a> LLM and a <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=chain-of-thought">chain-of-thought</a> (CoT)
generation approach, enables both coarse-grained control (e.g., gender,
speaking style) and fine-grained adjustments (e.g., precise pitch values,
speaking rate). To facilitate research in <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=controllable%20TTS">controllable TTS</a>, we introduce
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=VoxBox">VoxBox</a>, a meticulously curated 100,000-hour dataset with comprehensive
attribute annotations. Extensive experiments demonstrate that Spark-TTS not
only achieves state-of-the-art zero-shot <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=voice%20cloning">voice cloning</a> but also generates
highly customizable voices that surpass the limitations of reference-based
synthesis. Source code, pre-trained models, and audio samples are available at
https://github.com/SparkAudio/Spark-TTS.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="MONTHLY" data-date="2025-07-31" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2507.23726" target="_blank" onclick="event.stopPropagation()">Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-07-31</div>
                                        <div class="tags"><span class="tag">MONTHLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2507.23726" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/ByteDance-Seed/Seed-Prover" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 106</div>
                                    <div class="link-item stars-item">‚≠ê 267</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">Seed-Prover, a lemma-style reasoning model using Lean, achieves high performance in formal theorem proving and automated mathematical reasoning through iterative refinement and specialized geometry support.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                LLMs have demonstrated strong mathematical reasoning abilities by leveraging
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=reinforcement%20learning">reinforcement learning</a> with <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=long%20chain-of-thought">long chain-of-thought</a>, yet they continue to
struggle with <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=theorem%20proving">theorem proving</a> due to the lack of clear supervision signals when
solely using natural language. Dedicated domain-specific languages like Lean
provide clear supervision via <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=formal%20verification">formal verification</a> of proofs, enabling effective
training through <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=reinforcement%20learning">reinforcement learning</a>. In this work, we propose
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Seed-Prover">Seed-Prover</a>, a lemma-style whole-proof reasoning model. <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Seed-Prover">Seed-Prover</a>
can iteratively refine its proof based on Lean feedback, proved lemmas, and
self-summarization. To solve IMO-level contest problems, we design three
test-time inference strategies that enable both deep and broad reasoning.
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Seed-Prover">Seed-Prover</a> proves 78.1% of formalized past IMO problems, saturates <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=MiniF2F">MiniF2F</a>,
and achieves over 50\% on <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=PutnamBench">PutnamBench</a>, outperforming the previous
state-of-the-art by a large margin. To address the lack of geometry support in
Lean, we introduce a geometry reasoning engine <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Seed-Geometry">Seed-Geometry</a>, which
outperforms previous formal geometry engines. We use these two systems to
participate in IMO 2025 and fully prove 5 out of 6 problems. This work
represents a significant advancement in <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=automated%20mathematical%20reasoning">automated mathematical reasoning</a>,
demonstrating the effectiveness of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=formal%20verification">formal verification</a> with long
chain-of-thought reasoning.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="TRENDING" data-date="2025-08-14" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2508.10893" target="_blank" onclick="event.stopPropagation()">STream3R: Scalable Sequential 3D Reconstruction with Causal Transformer</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-08-14</div>
                                        <div class="tags"><span class="tag">TRENDING</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2508.10893" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/NIRVANALAN/STream3R" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 29</div>
                                    <div class="link-item stars-item">‚≠ê 173</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">STream3R reformulates 3D reconstruction as a decoder-only Transformer problem, using causal attention to efficiently process image sequences and outperform existing methods in both static and dynamic scenes.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                We present STream3R, a novel approach to <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=3D%20reconstruction">3D reconstruction</a> that reformulates
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=pointmap%20prediction">pointmap prediction</a> as a decoder-only <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Transformer">Transformer</a> problem. Existing
state-of-the-art methods for <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=multi-view%20reconstruction">multi-view reconstruction</a> either depend on
expensive <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=global%20optimization">global optimization</a> or rely on simplistic <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=memory%20mechanisms">memory mechanisms</a> that
scale poorly with sequence length. In contrast, STream3R introduces an
streaming framework that processes image sequences efficiently using causal
attention, inspired by advances in modern language modeling. By learning
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=geometric%20priors">geometric priors</a> from large-scale 3D datasets, STream3R generalizes well to
diverse and challenging scenarios, including dynamic scenes where traditional
methods often fail. Extensive experiments show that our method consistently
outperforms prior work across both static and dynamic scene benchmarks.
Moreover, STream3R is inherently compatible with <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=LLM-style%20training">LLM-style training</a>
infrastructure, enabling efficient large-scale <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=pretraining">pretraining</a> and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=fine-tuning">fine-tuning</a> for
various downstream 3D tasks. Our results underscore the potential of causal
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Transformer">Transformer</a> models for <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=online%203D%20perception">online 3D perception</a>, paving the way for real-time 3D
understanding in <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=streaming%20environments">streaming environments</a>. More details can be found in our
project page: https://nirvanalan.github.io/projects/stream3r.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="MONTHLY" data-date="2025-08-11" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2508.07999" target="_blank" onclick="event.stopPropagation()">WideSearch: Benchmarking Agentic Broad Info-Seeking</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-08-11</div>
                                        <div class="tags"><span class="tag">MONTHLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2508.07999" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/ByteDance-Seed/WideSearch" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 101</div>
                                    <div class="link-item stars-item">‚≠ê 67</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">WideSearch is a new benchmark evaluating the reliability of automated search agents in large-scale information collection tasks, revealing significant deficiencies in current systems.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                From professional research to everyday planning, many tasks are bottlenecked
by wide-scale information seeking, which is more repetitive than cognitively
complex. With the rapid development of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Large%20Language%20Models">Large Language Models</a> (LLMs), automated
search agents powered by LLMs offer a promising solution to liberate humans
from this tedious work. However, the capability of these agents to perform such
"wide-context" collection reliably and completely remains largely unevaluated
due to a lack of suitable <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=benchmark">benchmark</a>s. To bridge this gap, we introduce
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=WideSearch">WideSearch</a>, a new <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=benchmark">benchmark</a> engineered to evaluate agent reliability on these
large-scale collection tasks. The <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=benchmark">benchmark</a> features 200 manually curated
questions (100 in English, 100 in Chinese) from over 15 diverse domains,
grounded in real user queries. Each task requires agents to collect large-scale
atomic information, which could be verified one by one objectively, and arrange
it into a well-organized output. A rigorous five-stage <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=quality%20control%20pipeline">quality control pipeline</a>
ensures the difficulty, completeness, and verifiability of the dataset. We
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=benchmark">benchmark</a> over 10 state-of-the-art <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=agentic%20search%20systems">agentic search systems</a>, including
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=single-agent">single-agent</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=multi-agent%20frameworks">multi-agent frameworks</a>, and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=end-to-end%20commercial%20systems">end-to-end commercial systems</a>. Most
systems achieve overall success rates near 0\%, with the best performer
reaching just 5\%. However, given sufficient time, cross-validation by multiple
human testers can achieve a near 100\% success rate. These results demonstrate
that present search agents have critical deficiencies in large-scale
information seeking, underscoring urgent areas for future research and
development in agentic search. Our dataset, evaluation pipeline, and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=benchmark">benchmark</a>
results have been publicly released at https://<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=widesearch">widesearch</a>-seed.github.io/
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="MONTHLY" data-date="2025-08-01" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2508.00414" target="_blank" onclick="event.stopPropagation()">Cognitive Kernel-Pro: A Framework for Deep Research Agents and Agent
  Foundation Models Training</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-08-01</div>
                                        <div class="tags"><span class="tag">MONTHLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2508.00414" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/Tencent/CognitiveKernel-Pro" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 86</div>
                                    <div class="link-item stars-item">‚≠ê 301</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">Cognitive Kernel-Pro is an open-source multi-module agent framework that enhances AI agent robustness and performance through data curation and novel test-time strategies, achieving state-of-the-art results.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                General AI Agents are increasingly recognized as foundational frameworks for
the next generation of artificial intelligence, enabling complex reasoning, web
interaction, coding, and autonomous research capabilities. However, current
agent systems are either closed-source or heavily reliant on a variety of paid
APIs and proprietary tools, limiting accessibility and reproducibility for the
research community. In this work, we present Cognitive Kernel-Pro, a
fully open-source and (to the maximum extent) free multi-module agent framework
designed to democratize the development and evaluation of advanced AI agents.
Within Cognitive Kernel-Pro, we systematically investigate the curation of
high-quality training data for <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Agent%20Foundation%20Models">Agent Foundation Models</a>, focusing on the
construction of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=queries">queries</a>, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=trajectories">trajectories</a>, and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=verifiable%20answers">verifiable answers</a> across four key
domains: web, file, code, and general reasoning. Furthermore, we explore novel
strategies for <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=agent%20test-time%20reflection">agent test-time reflection</a> and voting to enhance agent
robustness and performance. We evaluate Cognitive Kernel-Pro on <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=GAIA">GAIA</a>, achieving
state-of-the-art results among open-source and free agents. Notably, our
8B-parameter open-source model surpasses previous leading systems such as
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=WebDancer">WebDancer</a> and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=WebSailor">WebSailor</a>, establishing a new performance standard for
accessible, high-capability AI agents. Code is available at
https://github.com/Tencent/CognitiveKernel-Pro
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="paper-card hf-card flip-card" data-tags="MONTHLY" data-date="2025-07-24" onclick="flipCard(this)">
                    <div class="flip-card-inner">
                        <!-- Front of card -->
                        <div class="flip-card-front">
                            <div class="paper-header">
                                <div class="paper-title-date">
                                    <h3 class="paper-title">
                                        <a href="https://huggingface.co/papers/2508.02694" target="_blank" onclick="event.stopPropagation()">Efficient Agents: Building Effective Agents While Reducing Cost</a>
                                    </h3>
                                    <div class="paper-date-tags">
                                        <div class="paper-date">Published: 2025-07-24</div>
                                        <div class="tags"><span class="tag">MONTHLY</span></div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="paper-content">
                                <div class="paper-links-grid">
                                    <a href="https://arxiv.org//pdf/2508.02694" target="_blank" class="link-item pdf-link" onclick="event.stopPropagation()">üìÑ PDF</a>
                                    <a href="https://github.com/OPPO-PersonalAI/OAgents" target="_blank" class="link-item github-link" onclick="event.stopPropagation()">üêô GitHub</a>
                                    <div class="link-item upvote-item">üî• 79</div>
                                    <div class="link-item stars-item">‚≠ê 196</div>
                                </div>
                                
                                <div class="llm-summary-box">
                                    <p>
                                        <strong>ü§ñ AI Summary:</strong>
                                        <em style="color: #2d5016;">A study on the efficiency-effectiveness trade-off in LLM-driven agent systems identifies optimal agent framework design to reduce costs while maintaining performance.</em>
                                    </p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Back of card -->
                        <div class="flip-card-back">
                            <h4>üìÑ Abstract</h4>
                            <div class="abstract-text">
                                The remarkable capabilities of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Large%20Language%20Model">Large Language Model</a> (<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=LLM">LLM</a>)-driven agents have
enabled sophisticated systems to tackle complex, multi-step tasks, but their
escalating costs threaten scalability and accessibility. This work presents the
first systematic study of the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=efficiency-effectiveness%20trade-off">efficiency-effectiveness trade-off</a> in modern
<a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=agent%20systems">agent systems</a>, addressing the critical need for cost-effective designs without
sacrificing performance. We investigate three key questions: (1) How much
complexity do <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=agentic%20tasks">agentic tasks</a> inherently require? (2) When do additional modules
yield diminishing returns? (3) How much efficiency can be gained through the
design of efficient <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=agent%20framework">agent framework</a>s? Through an empirical analysis on the GAIA
benchmark, we evaluate the impact of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=LLM%20backbone">LLM backbone</a> selection, <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=agent%20framework">agent framework</a>
designs, and <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=test-time%20scaling%20strategies">test-time scaling strategies</a>. Using the <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=cost-of-pass">cost-of-pass</a> metric, we
quantify the efficiency-performance trade-off across these dimensions. Our
findings inform the development of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Efficient%20Agents">Efficient Agents</a> , a novel <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=agent%20framework">agent framework</a>
that has an optimal complexity to task requirements. <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=Efficient%20Agents">Efficient Agents</a> retains
96.7% of the performance of <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=OWL">OWL</a>, one leading open-source <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=agent%20framework">agent framework</a>, while
reducing operational costs from 0.398 to 0.228, resulting in a 28.4%
improvement in <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=cost-of-pass">cost-of-pass</a>. Our work provides actionable insights for
designing efficient, high-performing <a class="cursor-pointer font-normal text-gray-800 underline decoration-gray-300 decoration-dashed underline-offset-2 hover:decoration-gray-600 dark:text-gray-300 dark:decoration-gray-500 dark:hover:decoration-gray-300" href="https://huggingface.co/papers?q=agent%20systems">agent systems</a>, advancing the accessibility
and sustainability of AI-driven solutions.
                            </div>
                        </div>
                    </div>
                </div>
                </div>
                        </div>
                    </section>
                </div>
            </div>
        </main>

        <!-- Footer -->
        <footer class="footer">
            <div class="container">
                <p>&copy; 2025 Daily Digest. Curated with ‚ù§Ô∏è for the curious minds.</p>
            </div>
        </footer>
        <script src="static/script.js"></script>
    </body>
    </html>
    